{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"style/","title":"Style Guide","text":""},{"location":"style/#markdown","title":"Markdown","text":"<ul> <li>Headings<ul> <li>Use the # sign</li> <li>Leave a space between the # and the text</li> </ul> </li> <li>Unordered Lists<ul> <li>Use a hyphen</li> <li>Indent by four spaces for nesting</li> <li>Indent by four additional spaces to include extra paragraphs attached to a list item</li> </ul> </li> <li>Ordered Lists<ul> <li>Begin with a number followed by a full stop</li> </ul> </li> <li>Code Blocks<ul> <li>Indent by four additional spaces</li> </ul> </li> <li>Code Phrase<ul> <li>Use backticks for code oneliners</li> </ul> </li> <li>Paragraphs<ul> <li>Use a blank line to separate paragraphs</li> </ul> </li> <li>Line Breaks<ul> <li>Use a br element</li> </ul> </li> <li>Bold<ul> <li>Wrap words with two asterisks at start and end</li> </ul> </li> <li>Italic<ul> <li>Wrap words with one asterisk at start and end</li> </ul> </li> <li>Bold and Italic<ul> <li>Wrap words with three asterisks at start and end</li> </ul> </li> <li>Blockquotes<ul> <li>Begin paragraph with &gt; sign</li> <li>Add a &gt; on blank lines between paragraphs</li> <li>Use multiple &gt; to nest blockquotes</li> </ul> </li> <li>Horizontal Rule<ul> <li>Use underscore characters</li> <li>Use a blank line before and after</li> </ul> </li> <li>Links<ul> <li>Square brackets for name followed by round brackets for URL</li> <li>To turn an email or URL into a link enclose it in angle brackets</li> </ul> </li> <li>Images<ul> <li>Begin the link with an exclamation mark immeadiately followed by the square bracket, round bracket combination</li> <li>Round bracket path can be a URL or file path (e.g. <code>/assets/images/image1.png</code>)</li> </ul> </li> </ul>"},{"location":"style/#yaml","title":"YAML","text":"<p>Indent by four spaces</p>"},{"location":"azure/aad/","title":"AAD Features","text":""},{"location":"azure/aad/#key-concepts","title":"Key concepts","text":"<ol> <li>Identity - an object that can be authenticated: users, applications or servers</li> <li>Account - an identity with associated data</li> <li>Azure AD Account - an identity created through AAD</li> <li>Azure tenant - an instance of Azure AD</li> <li>Azure Subscription - used to pay for Azure cloud services. Each suscription is joined to a single tenant</li> </ol> <p>Azure Active Directory is Microsoft's multi-tenant cloud-based directory and identity  management service. AAD is used by Azure, Microsoft 365, Microsoft Intune and Microsoft Dynamics 365. When a company purchases one of these services they are assigned a default directory. The default directory will hold the users and groups with access to the services purchased. The tenant refers to  the organisation and the default directory assigned to it, and therefore the tenant is also referred to as the directory. </p> <p>Subscriptions are both a billing entity and a security boundary.  Each subscription has a single account owner and is associated with a single  AAD, although a single AAD instance may be used for many subscriptions. Management Groups help you manage access, policy and compliance by grouping multiple subscriptions together.</p> <p>AAD provides:</p> <ul> <li>Secure SSO to cloud and on-premises applicatons<ul> <li>Users can sign-in with the same set of credentials to access all of their apps</li> <li>SSO available for M365 and thousands of SaaS apps</li> </ul> </li> <li>Ubiquitous device support<ul> <li>Company portals and web-based access can allow users to connect to apps using existing work credentials</li> </ul> </li> <li>Secure remote access for on-prem web applications. Includes:<ul> <li>MFA</li> <li>Conditional Access Policies</li> <li>Group-based access management</li> </ul> </li> <li>Cloud-extensibility<ul> <li>On-premises directories can be connected to AAD</li> <li>AAD can then be used to manage access across environments</li> </ul> </li> <li>Sensitive-data protection<ul> <li>identity protection available in P2 subscriptions</li> <li>provides advanced security reporting, notifications, recommendations and risk-based policies</li> </ul> </li> <li>Self-service options:<ul> <li>self-service password resets</li> <li>delegation of group management to non-admin users</li> </ul> </li> </ul> <p>Active Directory Domain Services (AD DS) is the deployment of Windows Server-based Active Directory on a physical or virtual server. Windows Active Directory is a suite of technologies. Apart from  Directory Services, it also includes:</p> <ul> <li>Certificate Services (AD CS)</li> <li>Lightweight Directory Services (AD LDS)</li> <li>Federation Services (AD FS)</li> <li>Rights Management Services (AD RMS)</li> </ul> <p>Using AAD is different from deploying an AD domain controller and adding it to your on-premises  domain. AAD is primarily an identity solution and is designed for internet-based applications. AAD  cannot be queried using LDAP: instead AAD provides a REST API over http/https. AAD does not use  Kerberos/NTLM for authentication: instead it uses protocols such as SAML, WS-Federation,  OpenID Connect for authentication and OAuth for authorisation. Federation Services allow use of  third-party authentication services. There are no OUs or GPOs in AAD.</p> <p>Azure Active Directory comes in four editions:</p> <ol> <li>Free: User and Group management for up to 500,000 objects, SSO for Azure and Microsoft 365, Identity and Access Management, B2B Collaboration, on-prem directory synchronisation.</li> <li>Microsoft 365 Apps: adds IAM for Microsoft 365 applications (which includes MFA, group access management and self-service password reset for cloud users)</li> <li>Premium P1: adds Premium features, Hybrid Identities allowing users to access cloud or on-prem resources, Advanced Group Management (dynamic groups, self-service group management), Conditional Access, self-service password resets (SSPR) for on-prem users</li> <li>Premium P2: adds Identity Protection (risk-based conditional access) and Identity Governance</li> </ol> <p>All Azure AD editions support SSPR. SSPR uses a security group to limit which users have SSPR  priviledges. All users accounts in your organisation require a valid licence to use SSPR. Azure Administrators can always reset their passwords. Authentication methods used in SSPR include:</p> <ol> <li>email notification</li> <li>text message</li> <li>security code sent to a phone</li> <li>security questions</li> </ol>"},{"location":"azure/aad/#azure-ad-join","title":"Azure AD Join","text":"<p>Azure AD Join is intended for organisations that do not have an on-prem AD.</p> <p>Azure AD Join allows you to join devices directly to Azure AD without the need to join to an  on-prem AD. It provides SSO, Enterprise state roaming across joined devices, access to an inventory of pre-approved applications (Microsoft Store for Business), access to on-prem resources when the  device also has access to on-prem domain controllers. </p> <p>Devices can be either:</p> <ol> <li>registered <ul> <li>provides the device with an identity used to authenticate the device</li> <li>the device identity can be used to enable or disable the device</li> </ul> </li> <li>joined <ul> <li>provides an identity and allows sign-in to the device using an organisational identity (AAD)</li> <li>changes the local state of the device</li> </ul> </li> </ol>"},{"location":"azure/aad/#users-and-groups","title":"Users and Groups","text":"<p>AAD defines users in three ways:</p> <ul> <li>Cloud Identities: users defined in AAD only</li> <li>Directory-synchronised Identities: defined in an on-prem AD.</li> <li>Guest Users: accounts defined in other cloud providers</li> </ul> <p>The <code>bulk create</code> option in the Azure Portal allows you to download a CSV template to use  to add users in bulk.</p> <p>Security groups can be assigned access to resources, and any members of the group will also  inherit those permissions. Users can be assigned directly to the group or membership rules can be defined so that users are assigned or removed from the group (dynamic user) or devices can be assigned or removed from the group (dynamic device).</p> <p>Administrative units can be defined in AAD and an administrative role can be created with  administrator rights on the adminstrative unit. Individual users within the administrative unit can then be assigned the adminstrative role to manage membership of the unit</p> <p>Administrator roles are used to control who is allowed to do what. Member users are intended for users who are considered internal to the organisation. Guest users are intended for  individuals who are external to your organisation but need to access resources in your tenant as part of a collaboration. Guest users sign-in using their own organisation accounts.  Member users can invite Guest users unless this has been disabled by a User Administrator. </p> <p>If you have multiple tenants, you can use Guest accounts to give users from one tenant  access to resources in another tenant. With Azure B2B collaboration, you don't have to  manage external user identities - this is left to their originating organisation. B2B  collaboration using Guest accounts is simpler than Federation. With Federation you need to establish a trust relationship between different AD servers possibly using on-prem AD FS.  Users authenticating outside the internal network will need to go through a Web Application  proxy to authenticate to the AD FS server. </p> <p>Azure AD-roles are for managing access to AAD resources, like:</p> <ol> <li>Users</li> <li>Groups</li> <li>Billing</li> <li>Licencing</li> <li>Application Registration</li> </ol> <p>Access rights can be assigned directly to a user or group or assigned based on rules (requires a  premium licence). </p>"},{"location":"azure/aad/#rbac","title":"RBAC","text":"<p>Azure role-based access control (RBAC) roles are for managing access to Azure Resources and is built on Azure Resource Manager. Roles can be assigned at the following scopes: </p> <ol> <li>Management Group</li> <li>Subscription</li> <li>Resource Group</li> <li>Resource</li> </ol> <p>RBAC is assigned from the IAM pane for each object scope. RBAC uses an 'allow' model - roles  assigment defines what is allowed for the assignee. The definition of each role contains both an 'Actions' and 'Not Actions' section. </p> <p>You can check role assignment activity in Activity Logs</p>"},{"location":"azure/aml/","title":"Machine Learning","text":"<p>Machine Learning aims to use patterns in data to make estimates. There are four main components to the learning process:</p> <ul> <li>Data: specifically features (inputs) and labels (expected output)</li> <li>Model: makes estimates from features. </li> <li>Objective: what the model is trying to achieve</li> <li>Optimiser: used to improve the performance of the model</li> </ul> <p>Models consist of logic and parameters that are used to transform the  input into an estimate. Models are chosen based on their logic  rather than the parameter values. Parameters are discovered during  the training process. Training is achieved using a cost (objective) function and an optimiser function. The cost function is used to calculate how well a model performed. The optimiser function is used to change the models parameters to improve the cost performance.  A gradient-descent optimiser tracks the decrease in cost as parameter values are changed and identifying when the cost has been brought to it's lowest value. </p> <p>Crucial to the optimiser, is the step-size: how much a parameter value is adjusted for the next iteration. If the step-size is too small,  the optimiser can settle for a local minimal cost. If the step-size is too large, the minimal cost can be missed entirely. Finding the optimal  step-size requires experimentation. </p> <p>Machine Learning training can be either supervised or unsupervised.  Supervised learning assesses a model's performance by comparing its  estimates to the correct answer (labels). Unsupervised training only uses  features. Because unsupervised training does not have access to  expected output, it is often used in circumstances where no single  correct answer exists. </p> <p>Once a Model has been trained, it can be saved and deployed. </p> <p>Model training is dependant on the data used in training. Ideally, this  should be representative, free from errors and missing data points. A  representative dataset contains sufficient values (samples) to represent  the whole population. Errors in measurement values can also skew training  results. Where data points are absent, models can be chosen that  handle missing data, or the missing data can be substituted with  reasonable values, or the records with missing data can be removed. </p>"},{"location":"azure/az104/","title":"AZ104 - Further Reading","text":""},{"location":"azure/az104/#questions","title":"Questions","text":"<ul> <li>How to use Managed Identities to connect to Azure Key Vault</li> <li>How to use Stored Access Policies for Storage Account SAS tokens</li> </ul>"},{"location":"azure/az104/#prerequisites","title":"Prerequisites","text":""},{"location":"azure/az104/#configure-azure-resources-with-tools","title":"Configure Azure Resources with Tools","text":"<ul> <li>Azure Portal Documentation</li> <li>Azure Cloudshell Overview</li> <li>Azure Powershell Documentation</li> <li>Azure CLI Reference</li> <li>Azure Resource Manager</li> <li>Bicep Overview</li> </ul>"},{"location":"azure/az104/#azure-resource-manager","title":"Azure Resource Manager","text":"<ul> <li>Azure Resource Manager Documentation</li> </ul>"},{"location":"azure/az104/#configure-resources-with-arm-templates","title":"Configure Resources with ARM templates","text":"<ul> <li>ARM Template Documentation</li> <li>Azure Quickstart Templates</li> <li>Create Resources using ARM templates</li> </ul>"},{"location":"azure/az104/#azure-automation-with-powershell","title":"Azure Automation with Powershell","text":"<ul> <li>Automate Administrative Tasks with Powershell</li> </ul>"},{"location":"azure/az104/#control-azure-services-with-cli","title":"Control Azure Services with CLI","text":""},{"location":"azure/az104/#deploy-infrastructure-with-json-arm-templates","title":"Deploy Infrastructure with JSON ARM Templates","text":"<ul> <li>ARM Tools for VS Code</li> <li>ARM Template Syntax</li> <li>ARM overview</li> <li>Resource Providers</li> <li>ARM Templates</li> <li>ARM Template Outputs</li> <li>ARM Template Parameters</li> </ul>"},{"location":"azure/az104/#identity-and-governance","title":"Identity and Governance","text":""},{"location":"azure/az104/#configure-aad","title":"Configure AAD","text":"<ul> <li>AAD Documentation</li> <li>AAD Device Identity</li> <li>AAD Self-Service Password Reset</li> <li>AAD Join</li> <li>Hybrid Identity</li> </ul>"},{"location":"azure/az104/#configure-user-and-groups","title":"Configure User and Groups","text":"<ul> <li>Azure Built-In Roles</li> <li>Azure Custom Roles</li> <li>Create User and Groups</li> <li>AAD Fundamentals</li> </ul>"},{"location":"azure/az104/#configure-subscriptions","title":"Configure Subscriptions","text":"<ul> <li>Cost Management and Billing</li> <li>Create Subscriptions</li> <li>Analyse Costs and Create Budgets</li> </ul>"},{"location":"azure/az104/#configure-azure-policy","title":"Configure Azure Policy","text":"<ul> <li>Azure Policy Documentation</li> <li>Built-in Policy Definitions</li> <li>Built-in Policy Initiatives</li> <li>Recommended Policies</li> <li>Pricing Details</li> <li>Policy Samples</li> <li>Create a Custom Policy</li> <li>Create and Assign a Policy Programatically</li> <li>Create a Policy for Non-Compliant Resources</li> </ul>"},{"location":"azure/az104/#configure-rbac","title":"Configure RBAC","text":"<ul> <li>RBAC Documentation</li> <li>RBAC Role Definitions</li> <li>RBAC Built-in Role Definitions</li> <li>RBAC Custom Roles</li> <li>AAD Administrator Roles</li> </ul>"},{"location":"azure/az104/#create-users-and-groups-in-aad","title":"Create Users and Groups in AAD","text":"<ul> <li>AAD Documentation</li> </ul>"},{"location":"azure/az104/#sspr","title":"SSPR","text":"<ul> <li>Quickstart</li> <li>How it Works</li> <li>Deploy SSPR</li> </ul>"},{"location":"azure/az104/#implement-and-manage-storage","title":"Implement and Manage Storage","text":""},{"location":"azure/az104/#configure-storage-accounts","title":"Configure Storage Accounts","text":"<ul> <li>Azure Storage</li> <li>Configure Storage Disaster Recovery</li> </ul>"},{"location":"azure/az104/#configure-blob-storage","title":"Configure Blob Storage","text":"<ul> <li>Blob Storage</li> <li>Blob Storage Access Tiers</li> <li>Blob Replication</li> <li>Blob Lifecycle Management</li> </ul>"},{"location":"azure/az104/#configure-azure-storage-security","title":"Configure Azure Storage Security","text":"<ul> <li>Create an Account SAS</li> <li>Create a Service Level SAS</li> <li>Create a User Delegation SAS</li> <li>Customer-Managed Keys</li> </ul>"},{"location":"azure/az104/#configure-azure-files-and-file-sync","title":"Configure Azure Files and File Sync","text":"<ul> <li>Azure Files Documentation</li> <li>Azure Files Deployment</li> <li>Azure File Sync Deployment</li> <li>Cloud Tiering Overview</li> </ul>"},{"location":"azure/az104/#configure-azure-storage-with-tools","title":"Configure Azure Storage with Tools","text":"<ul> <li>Azure Storage Explorer</li> <li>AzCopy</li> <li>Azure Import/Export Service</li> </ul>"},{"location":"azure/az104/#create-an-azure-storage-account","title":"Create an Azure Storage account","text":""},{"location":"azure/az104/#shared-access-signatures","title":"Shared Access Signatures","text":"<ul> <li>SAS Overview</li> </ul>"},{"location":"azure/az104/#azure-storage-explorer","title":"Azure Storage Explorer","text":"<ul> <li>Storage Explorer with ADLS Gen2</li> </ul>"},{"location":"azure/az104/#deploy-and-manage-compute","title":"Deploy and Manage Compute","text":""},{"location":"azure/az104/#configure-virtual-machines","title":"Configure Virtual Machines","text":"<ul> <li>VM Documentation</li> <li>Disk Storage for VMs</li> <li>Azure Quickstart Templates</li> </ul>"},{"location":"azure/az104/#configure-virtual-machine-availability","title":"Configure Virtual Machine Availability","text":"<ul> <li>VM Availability</li> <li>VMSS Autoscale</li> <li>Deploy Application on VMSS</li> <li>Fault Domains</li> <li>Windows Custom Script Extension</li> <li>Linux Custom Script Extension</li> </ul>"},{"location":"azure/az104/#configure-virtual-machine-extensions","title":"Configure Virtual Machine Extensions","text":"<ul> <li>Azure Automation State Configuration</li> <li>DSC 2.0</li> <li>Deploy Windows apps</li> <li>Azure Automation State Configuration Sandbox</li> </ul>"},{"location":"azure/az104/#configure-azure-app-service-plans","title":"Configure Azure App Service Plans","text":"<ul> <li>App Service Plans</li> <li>App Service Tutorial</li> </ul>"},{"location":"azure/az104/#configure-azure-app-service","title":"Configure Azure App Service","text":"<ul> <li>App Service Docs</li> <li>Web App Sandbox</li> </ul>"},{"location":"azure/az104/#configure-azure-container-instances","title":"Configure Azure Container Instances","text":"<ul> <li>Azure Container Instances Documentation</li> <li>Build a Containerised Web App</li> </ul>"},{"location":"azure/az104/#manage-azure-vms-with-azure-cli","title":"Manage Azure VMs with Azure CLI","text":"<ul> <li>Architect Compute in Azure</li> </ul>"},{"location":"azure/az104/#host-a-web-app-in-azure-app-service","title":"Host a Web App in Azure App Service","text":""},{"location":"azure/az104/#configure-virtual-networks","title":"Configure Virtual Networks","text":""},{"location":"azure/az104/#configure-virtual-networks_1","title":"Configure Virtual Networks","text":"<ul> <li>Virtual Network Documentation</li> <li>Azure Networking Documentation</li> <li>IP Address Sandbox</li> </ul>"},{"location":"azure/az104/#configure-network-security-groups","title":"Configure Network Security Groups","text":"<ul> <li>NSG Documentation</li> <li>ASG Documentation</li> <li>NSG and Service Endpoints Sandbox</li> </ul>"},{"location":"azure/az104/#configure-azure-dns","title":"Configure Azure DNS","text":"<ul> <li>Azure DNS Documentation</li> <li>Azure Private DNS Documentation</li> <li>DNS Zones and Records</li> <li>DNS Sandbox</li> </ul>"},{"location":"azure/az104/#configure-azure-vnet-peering","title":"Configure Azure VNET Peering","text":"<ul> <li>VNet Peering Documentation</li> <li>VPN Gateway Transit</li> <li>Troubleshoot VNet Peering</li> </ul>"},{"location":"azure/az104/#configure-network-routing-and-endpoints","title":"Configure Network Routing and Endpoints","text":"<ul> <li>VNet Routing</li> <li>Private Link Introduction</li> </ul>"},{"location":"azure/az104/#configure-azure-load-balancer","title":"Configure Azure Load Balancer","text":"<ul> <li>Load Balancer Documentation</li> <li>Create a Public Load Balancer</li> <li>Create an Internal Load Balancer</li> <li>Gateway Load Balancer</li> <li>Load Balancer Sandbox</li> </ul>"},{"location":"azure/az104/#configure-azure-application-gateway","title":"Configure Azure Application Gateway","text":"<ul> <li>Azure Application Gateway Documentation</li> <li>WAF on AGW</li> <li>Multi-Site Routing</li> <li>Hands-On HTTPS traffic</li> </ul>"},{"location":"azure/az104/#design-ip-address-schema","title":"Design IP Address Schema","text":""},{"location":"azure/az104/#vnet-peering","title":"VNet Peering","text":""},{"location":"azure/az104/#host-your-domain-with-azure-dns","title":"Host Your Domain with Azure DNS","text":"<ul> <li>DNS Zones and Records</li> </ul>"},{"location":"azure/az104/#custom-routing","title":"Custom Routing","text":"<ul> <li>High-Availability NVAs</li> </ul>"},{"location":"azure/az104/#azure-load-balancer","title":"Azure Load Balancer","text":""},{"location":"azure/az104/#monitor-and-backup-azure-resources","title":"Monitor and Backup Azure Resources","text":""},{"location":"azure/az104/#configure-file-and-folder-backups","title":"Configure File and Folder Backups","text":"<ul> <li>Azure Backup</li> <li>Azure File Share Backup</li> <li>Azure Recovery Services Vault</li> <li>Backup Center</li> <li>MARS Agent</li> </ul>"},{"location":"azure/az104/#configure-virtual-machine-backups","title":"Configure Virtual Machine Backups","text":"<ul> <li>Azure Site Recovery</li> <li>System Center Data Protection Manager</li> <li>Microsoft Azure Backup Server</li> <li>DPM and MABS protection agent</li> </ul>"},{"location":"azure/az104/#configure-azure-monitor","title":"Configure Azure Monitor","text":"<ul> <li>Azure Monitor Documentation</li> <li>Azure Monitor Metrics</li> <li>Azure Monitor Logs</li> <li>Log Queries</li> <li>Azure Monitor Alerts</li> <li>Design a Monitoring Strategy</li> <li>Security Events in Entra ID</li> <li>Monitor Azure Storage</li> </ul>"},{"location":"azure/az104/#configure-azure-alerts","title":"Configure Azure Alerts","text":"<ul> <li>Azure Monitor Alerts</li> <li>Alert Types</li> <li>Alert Rules</li> <li>Azure Monitor Best Practice</li> <li>Alerting Sandbox</li> <li>Monitor VMs with Metrics and Alerts</li> </ul>"},{"location":"azure/az104/#configure-log-analytics","title":"Configure Log Analytics","text":"<ul> <li>Log Analytics Documentation</li> <li>KQL</li> <li>Create a Log Analytics Workspace</li> <li>Analysis Best Practice</li> <li>KQL Reference</li> <li>Azure Monitor Logs Table Reference</li> <li>Log Analytics REST API</li> </ul>"},{"location":"azure/az104/#configure-network-watcher","title":"Configure Network Watcher","text":"<ul> <li>Network Watcher Documentation</li> <li>VM Routing troubleshooting tutorial</li> <li>VPN routing troubleshooting tutorial</li> <li>Network Infrastructure troubleshooting tutorial</li> <li>Configure Monitoring for Virtual Networks</li> </ul>"},{"location":"azure/az104/#incident-response","title":"Incident Response","text":"<ul> <li>Metric Alerts</li> <li>Log Alerts</li> <li>Activity Log Alerts</li> </ul>"},{"location":"azure/az104/#analyse-azure-infrastructure","title":"Analyse Azure Infrastructure","text":"<ul> <li>Log Query Starters</li> <li>Optimise Log Queries</li> <li>Create Dashboards</li> <li>Analysis Best Practice</li> </ul>"},{"location":"azure/az104/#monitor-vm-performance","title":"Monitor VM Performance","text":"<ul> <li>Azure Monitor Documentation</li> <li>Azure Monitor Agents</li> </ul>"},{"location":"azure/cli_commands/","title":"Azure CLI Commands","text":""},{"location":"azure/cli_commands/#references","title":"References","text":"<ul> <li>How to query Azure resources using the Azure CLI</li> <li>JMES Query Tutorial</li> </ul>"},{"location":"azure/cli_commands/#jmes-query-syntax","title":"JMES Query Syntax","text":"<p>By default, Azure CLI queries return JSON output. You can change the output format using the --output switch. Typical values are either 'table' or 'tsv'.</p> <p>The query option can be used to filter the fields returned. Where a command  returns a single row in table output, you can query by field name</p> <pre><code>az account show --query id\n</code></pre> <p>For commands returning more than one row or object, you need to use the 'flatten' operator in the query</p> <pre><code>az account list --query [].id -o table\n</code></pre> <p>To return multiple values in the output, specify these in a comma separated list:</p> <pre><code>az account list --query [].[id,name] -o table\n</code></pre> <p>You can rename the columns by specifying a dictionary instead of an array: note that the query is now in double-quotes:</p> <pre><code>az account list \\\n    --query \"[].{Subscription:id,Name:name}\" \\\n    -o table\n</code></pre> <p>You can also specify an index value to return a specific row: indexes begin at 0 for the first element:</p> <pre><code>az account list \\\n    --query \"[1].{Subscription:id,Name:name}\" \\\n    -o table\n</code></pre> <p>An array slice can also be used:</p> <pre><code>az account list \\\n    --query \"[1:4].{Subscription:id,Name:name}\" \\\n    -o table\n</code></pre> <p>The slice specification equates to: \"return the element at index 1 and all  elements up to, but not including, the element at index 4\". Three elements (4 - 1) are returned </p> <p>In addition to using array indexing, you can filter the results using filter  expressions:</p> <pre><code>az account list \\\n    --query \"[?name == 'mySubscription'].{Subscription:id,Name:name}\" \\\n    -o table`\n\naz account list \\\n    --query \"[?contains(name, 'sub')].{Subscription:id,Name:name}\" \\\n    -o table`\n</code></pre> <p>So far we've been selecting fields that return a scalar value. However some  fields will be dictionaries: use the dot operator to select a single element  from each dictionary:</p> <pre><code>az account list --query \"[].[user.type]\" -o table`\n</code></pre> <p>If the query returns a single row, you can use the tsv output format to  assign the value to a variable or to directly use the value in a command:</p> <pre><code>SUB=$(az account list --query \"[?contains(name, 'research')].[id]\" -o tsv)\naz account set --subscription $SUB\n\naz account set --subscription $(az account list --query \"[?contains(name, 'research')].[id]\" -o tsv)\n</code></pre> <p>For multiple rows of single values, you can pipe the output to xargs, to perform the same  command multiple times:</p> <pre><code>az group list --query \"[?starts_with(name,'az104-03')].[name]\" --output tsv | \\\nxargs -L1 bash -c 'az group delete --name $0 --no-wait --yes'\n</code></pre> <p>You can capture multiple values in an ENV variable and use a foreach loop to iterate:</p> <pre><code>VM_NAMES=$(az vm list --query [].[name] -o tsv)\nfor n in $VM_NAMES; do echo \"on $n now\"; done\n</code></pre> <p>Or you can capture the output in an array, by enclosing the rvalue in round brackets:</p> <pre><code>SUB=($(az account show --query [id, name] -o tsv))\necho \"ID: ${SUB[0]}\"\necho \"Name: ${SUB[1]}\"\n</code></pre> <p>Replace placeholder data in an environment variable:</p> <pre><code>JSON_DATA=$(cat somejsonfile.json)\n\n# replace &lt;name&gt; with 'Example Text'\nJSON_DATA=${JSON_DATA//&lt;name&gt;/Example Text}\n</code></pre>"},{"location":"azure/cli_commands/#rest-api","title":"REST API","text":"<p>The <code>az rest</code> command can be used to send queries directly to ARM:</p> <pre><code>CONF=$(az rest --method GET --url https://management.azure.com/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/xx-xxxxxxxxxx/providers/Microsoft.Network/virtualNetworks/xxx-xxxxxxxxxxx?api-version=2020-07-01)\necho $CONF\naz rest --method PUT --body $CONF --url https://management.azure.com/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/xx-xxxxxxxxxx/providers/Microsoft.Network/virtualNetworks/xxxx-xxxxxxxxxx?api-version=2020-07-01\n</code></pre> <p>Whilst the value for the <code>url</code> parameter should be possible to construct manually, you can also  find the value in the 'Overview' pane for the resource in the Azure Portal, and switching to  'JSON View'. You can also find the value by logging-in to resources.azure.com and browsing to the  resource. </p>"},{"location":"azure/cli_commands/#login","title":"Login","text":"<p>The <code>az login</code> command can be used to authenticate to Azure. Without any additional parameters, the command will attempt to launch a browser to complete an interactive logon. If no browser is available, then the command defaults back to the old-style device-code logon. Use the <code>--use-device-code</code> logon switch to force old-style  device-code logon.</p> <p>The <code>--allow-no-subscriptions</code> parameter can be used for tenant-level access. The  <code>--tenant</code> option allows you to specify the tenant to logon to. </p>"},{"location":"azure/cli_commands/#compute","title":"Compute","text":"<p>The <code>--admin-username</code> parameter of <code>az vm create</code> is used to specify a username for the VM: if  not provided, then the username on your local machine is used. The <code>--generate-ssh-keys</code> will  create <code>id_rsa</code> key pairs on the local and remote machines. If these already exist on the local  machine they will be copied to the remote instead being created.</p> <p><code>az vm image list</code> can be used to get a list of VM images available. By default this will display the most popular images. Use the <code>--all</code> flag to get a complete list. You can also filter the results using <code>--publisher</code>, <code>--sku</code>, <code>--offer</code> flags. Use the <code>--location</code> filter to view offers that are available in the region you wish to deploy to. </p> <p>Use <code>az vm list-sizes --location $LOCATION -o table</code> to list the available machine sizes  for your region. If you don't specify the <code>--size</code> parameter in your <code>az vm create</code> command,  Azure will select a default general-purpose size for you. If you wish to resize an existing VM, you can use the following command to check available sizes for cluster the VM is deployed to: </p> <pre><code>az vm list-vm-resize-options \\\n    --resource-group learn-6e62a4e5-3f38-4461-aa16-2a0149360363 \\\n    --name SampleVM \\\n    --output table\n</code></pre> <p>Once you have identified the desired VM size, run <code>az vm resize</code> to resize the VM.</p> <p>Use <code>az vm list-ip-addresses</code> to list the IP Addresses associated to a VM. </p> <p><code>az vm list</code> will return information about all VMs in the current subscription. Use <code>az vm show</code> to get more detailed information about a specific VM. </p> <p><code>az vm open-port --port $port_number</code> to open a port on a running VM. </p> <p>Check the power status of a VM:</p> <pre><code>az vm get-instance-view \\\n    --name $VM_NAME\\\n    --resource-group $RG_NAME \\\n    --query \"instanceView.statuses[?starts_with(code, 'PowerState/')].displayStatus\" -o tsv\n</code></pre>"},{"location":"azure/cli_commands/#storage","title":"Storage","text":"<p>Create a storage account and container</p> <pre><code>RESOURCE_GROUP_NAME={{ page.resgroup }}\nLOCATION=uksouth\nSTORAGE_ACCOUNT_NAME={{ page.stgname }}\nCONTAINER_NAME={{ page.containername }}\n\naz group create --name $RESOURCE_GROUP_NAME --location $LOCATION\n\naz storage account create \\\n    --resource-group $RESOURCE_GROUP_NAME \\\n    --name $STORAGE_ACCOUNT_NAME --sku Standard_LRS \\\n    --encryption-services blob\n\naz storage container create \\\n    --name $CONTAINER_NAME \\\n    --account-name $STORAGE_ACCOUNT_NAME\n</code></pre> <p>Upload a file to blob storage:</p> <pre><code>az storage blob upload \\\n    --account-name $STORAGE_ACCOUNT_NAME \\\n    --container-name $CONTAINER_NAME \\\n    --file $FILE_NAME --name $CONTAINER_FILE\n</code></pre> <p>Upload a folder to blob storage:</p> <pre><code>az storage blob upload-batch \\\n    --destination $CONTAINER_NAME \\\n    --account-name $STORAGE_ACCOUNT_NAME \\\n    --destination-path $CONTAINER_FOLDER \\\n    --source $LOCAL_FOLDER\n</code></pre> <p>List Storage Account keys:</p> <pre><code>az storage account keys list --account-name $STORAGE_ACCOUNT_NAME`\n</code></pre> <p>Generate and capture SAS keys to access a storage container:</p> <pre><code>ACCOUNT_KEY=$(az storage account keys list \\\n    --resource-group $RESOURCE_GROUP_NAME \\\n    --account-name $STORAGE_ACCOUNT_NAME \n    --query '[0].value' -o tsv)\n\nEND_DATE=date -u -d \"1 year\" '+%Y-%m-%dT%H:%MZ'\n\nSAS_KEY=$(az storage container generate-sas \\\n    -n $CONTAINER_NAME \\\n    --account-key $ACCOUNT_KEY \\\n    --account-name $STORAGE_ACCOUNT_NAME \\\n    --https-only \\\n    --permissions dlrw \\\n    --expiry $END_DATE -o tsv)\n</code></pre> <p>The 'azcopy' command can also be used to move data quickly into and out of Azure  Storage Accounts. Logon to Azure:</p> <pre><code>azcopy login\n</code></pre> <p>Create a container:</p> <pre><code>azcopy make 'https://${STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${CONTAINER_NAME}'\n</code></pre> <p>Copy a folder to the container (creates the data_folder folder in the target container):</p> <pre><code>azcopy copy '/data/data_folder' \\\n    'https://${STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${CONTAINER_NAME}'\n</code></pre>"},{"location":"azure/cli_commands/#deploying-arm-templates","title":"Deploying ARM Templates","text":"<p>Deploy infrastructure from an ARM Template:</p> <pre><code>RESOURCE_GROUP=$RESOURCE_GROUP_NAME\nTEMPLATE_FILE=/path/to/template.json\n\naz group deployment create \\\n    --name blanktemplate \\\n    --resource-group $RESOURCE_GROUP \\\n    --template-file $TEMPLATE_FILE\n</code></pre> <p>Sample Templates are available from the Azure Quickstart Repo</p> <p>You can start with a blank template:</p> <pre><code>{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {},\n    \"functions\": [],\n    \"variables\": {},\n    \"resources\": [],\n    \"outputs\": {}\n}\n</code></pre> <p>You can then add resources as you expand the deployment. For instance to add a storage account,  add a storage account resource to the template:</p> <pre><code>\"resources\": [{\n    \"name\": \"learn132uksstorage\",\n    \"type\": \"Microsoft.Storage/storageAccounts\",\n    \"apiVersion\": \"2021-04-01\",\n    \"tags\": {\n        \"displayName\": \"learn132uksstorage\"\n    },\n    \"location\": \"[resourceGroup().location]\",\n    \"kind\": \"StorageV2\",\n    \"sku\": {\n        \"name\": \"Standard_LRS\",\n        \"tier\": \"Standard\"\n    }\n    \"properties\": {\n        \"supportsHttpsTrafficOnly\": true\n    }\n}],\n</code></pre> <p>Then you can create a new deployment using:</p> <pre><code>templateFile=\"azuredeploy.json\"\ntoday=$(date +\"%d-%b-%Y\")\nDeploymentName=\"addstorage-\"$today\n\naz deployment group create \\\n    --name $DeploymentName \\\n    --resource-group $RESOURCE_GROUP \\\n    --template-file $templateFile\n</code></pre> <p>You can make your template re-useable by using parameters instead of hard-coded values. For instance, to use a parameter to specify the Storage Account type, add the following parameter to the parameters section:</p> <pre><code>\"parameters\":{\n    \"storageAccountType\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"Standard_LRS\",\n        \"allowedValues\": [\n            \"Standard_LRS\",\n            \"Standard_GRS\",\n            \"Standard_ZRS\",\n            \"Premium_LRS\"\n        ],\n        \"metadata\": {\n            \"description\": \"Storage Account type\"\n        }\n    },\n},\n</code></pre> <p>Then replace the hard-coded value for the Storage Account type:</p> <pre><code>\"resources\": [{\n    \"name\": \"learn132uksstorage\",\n    \"type\": \"Microsoft.Storage/storageAccounts\",\n    \"apiVersion\": \"2021-04-01\",\n    \"tags\": {\n        \"displayName\": \"learn132uksstorage\"\n    },\n    \"location\": \"[resourceGroup().location]\",\n    \"kind\": \"StorageV2\",\n    \"sku\": {\n        \"name\": \"[parameters('storageAccountType')]\"\n    }\n    \"properties\": {\n        \"supportsHttpsTrafficOnly\": true\n    }\n}],\n</code></pre> <p>When you deploy the template, you can give a value for the parameter, otherwise the default value is used:</p> <pre><code>templateFile=\"azuredeploy.json\"\ntoday=$(date +\"%d-%b-%Y\")\nDeploymentName=\"use-stg-parameter-\"$today\n\naz deployment group create \\\n    --name $DeploymentName \\\n    --resource-group $RESOURCE_GROUP \\\n    --template-file $templateFile\n    --parameters storageAccountType=Standard_LRS\n</code></pre> <p>Template deployments also allow you to specify outputs that will be returned after a successful  deployment. The reference() function can be used to retrieve the runtime state of the resource:</p> <pre><code>\"outputs\": {\n    \"storageEndpoint\": {\n    \"type\": \"object\",\n    \"value\": \"[reference('learntemplatestorage123').primaryEndpoints]\"\n    }\n}\n</code></pre> <p>We can also add a parameter for the Storage Account Name, and additionally specify allowed values for any of our parameters. A completed ARM template could now look something like this:</p> <pre><code>{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"storageName\": {\n            \"type\": \"string\",\n            \"minLength\": 3,\n            \"maxLength\": 24,\n            \"metadata\": {\n            \"description\": \"The name of the Azure storage resource\"\n            }\n        },\n        // This is the allowed values for an Azure Storage account\n        \"storageSKU\": {\n            \"type\": \"string\",\n            \"defaultValue\": \"Standard_LRS\",\n            \"allowedValues\": [\n                \"Standard_LRS\",\n                \"Standard_GRS\",\n                \"Standard_RAGRS\",\n                \"Standard_ZRS\",\n                \"Premium_LRS\",\n                \"Standard_GZRS\",\n                \"Standard_RAGZRS\"\n            ],\n            \"metadata\": {\n                \"description\": \"Allowed SKUs for Storage Accounts\"\n            }\n        }\n    },\n    \"functions\": [],\n    \"variables\": {},\n    \"resources\": [{\n        \"name\": \"[parameters('storageName')]\",\n        \"type\": \"Microsoft.Storage/storageAccounts\",\n        \"apiVersion\": \"2021-04-01\",\n        \"tags\": {\n            \"displayName\": \"[parameters('storageName')]\"\n        },\n        \"location\": \"[resourceGroup().location]\",\n        \"kind\": \"StorageV2\",\n        \"sku\": {\n            \"name\": \"[parameters('storageSKU')]\",\n            \"tier\": \"Standard\"\n        }\n    }],\n    \"outputs\": {\n        \"storageEndpoint\": {\n            \"type\": \"object\",\n            \"value\": \"[reference(parameters('storageName')).primaryEndpoints]\"\n        }\n    }\n}\n</code></pre>"},{"location":"azure/cli_commands/#aad","title":"AAD","text":"<p>Get a list of users by Job Title:</p> <pre><code>az ad user list \\\n    --query \"[?jobTitle=='Manager'].{ department: department, name: displayName, jobTitle: jobTitle, pname: userPrincipalName }\" \\\n    --output table\n</code></pre> <p>List AD Groups by displayName:</p> <pre><code>az ad group list \\\n    --query \"[?contains(displayName,'Education')].{ name: displayName }\" \\\n    --output tsv\n</code></pre> <p>List members of the Education Department AD Group:</p> <pre><code>az ad group member list \\\n    --group \"Education Department\" \\\n    --query \"[].{ name: displayName }\" \\\n    --output tsv\n</code></pre> <p>Create a service principal with Contributor role on the specified subscription:</p> <pre><code>az ad sp create-for-rbac \\\n--scopes /subscriptions/$SUBSCRIPTION_ID \\\n--role Contributor --name $SP_NAME\n</code></pre> <p>Grant keyvault permissions to a service principal:</p> <pre><code>SP_OBJECT_ID=$(az ad sp list --display-name $SERVICE_PRINCIPAL --query [].objectId -o tsv)\naz keyvault set-policy --name $KEYVAULT_NAME \\\n--secret-permissions get list --object-id $SP_OBJECT_ID\n</code></pre>"},{"location":"azure/cli_commands/#key-vaults","title":"Key Vaults","text":"<p>Create a Key Vault and add a secret:</p> <pre><code>az keyvault create --name $KEYVAULT_NAME \\\n    --resource-group $RESOURCE_GROUP_NAME \\\n    --location $LOCATION --tags $TAG_VALUE\n\naz keyvault secret set \\\n    --vault-name $KEYVAULT_NAME \\\n    --name client-id --value $SECRET_VALUE\n</code></pre>"},{"location":"azure/cli_commands/#azure-load-balancer","title":"Azure Load Balancer","text":"<p>Create a Load Balancer with a static front-end IP:</p> <pre><code>RG=my_resource_group\nPublic_IP=pip-lb-uksouth\n\naz network public-ip create --resource-group $RG \\\n    --allocation-method Static --name $Public_IP\n\naz network lb create --resource-group $RG \\\n    --name myLoadBalancer \\\n    --public-ip-address $Public_IP \\\n    --frontend-ip-name myFrontEndPool \\\n    --backend-pool-name myBackEndPool\n</code></pre> <p>Add a health probe and a distribution rule for http traffic:</p> <pre><code>az network lb probe create \\\n    --resource-group $RG \\\n    --lb-name myLoadBalancer \\\n    --name myHealthProbe \\\n    --protocol tcp \\\n    --port 80\n\naz network lb rule create \\\n    --resource-group $RG \\\n    --lb-name myLoadBalancer \\\n    --name myHTTPRule \\\n    --protocol tcp \\\n    --frontend-port 80 \\\n    --backend-port 80 \\\n    --frontend-ip-name myFrontEndPool \\\n    --backend-pool-name myBackEndPool \\\n    --probe-name myHealthProbe\n</code></pre> <p>Connect two VMs to the Back-End pool:</p> <pre><code>az network nic ip-config update \\\n    --resource-group $RG \\\n    --nic-name webNic1 \\\n    --name ipconfig1 \\\n    --lb-name myLoadBalancer \\\n    --lb-address-pools myBackEndPool\n\naz network nic ip-config update \\\n    --resource-group $RG \\\n    --nic-name webNic2 \\\n    --name ipconfig1 \\\n    --lb-name myLoadBalancer \\\n    --lb-address-pools myBackEndPool\n</code></pre>"},{"location":"azure/cli_commands/#azure-monitor","title":"Azure Monitor","text":"<p>Configure an alert for a VM when the CPU usage exceeds 80%:</p> <pre><code>RG=my_resource_group\nVM=my_vm\n\nVMID=$(az vm show \\\n        --resource-group $RG \\\n        --name $VM \\\n        --query id \\\n        --output tsv)\n\naz monitor metrics alert create -n \"Cpu80PercentAlert\" \\\n    --resource-group $RG \\\n    --scopes $VMID \\\n    --condition \"max percentage CPU &gt; 80\" \\\n    --description \"Virtual machine is running at or greater than 80% CPU utilization\" \\\n    --evaluation-frequency 1m \\\n    --window-size 10m \\\n    --severity 3\n</code></pre>"},{"location":"azure/cli_commands/#configure-defaults","title":"Configure Defaults","text":"<p>The <code>az configure</code> command can be used to set and unset default parameter values. To set a default resource group for each az command use:</p> <p><code>az configure --defaults group=$RESOURCE_GROUP</code></p> <p>To unset the default use:</p> <p><code>az configure --defaults group=''</code></p> <p>To list current defaults use:</p> <p><code>az configure --list-defaults</code></p>"},{"location":"azure/compute/","title":"Compute","text":""},{"location":"azure/compute/#virtual-machines-iaas-service","title":"Virtual Machines - IaaS service","text":"<p>Consider the following when planning VM deployment:</p> <ul> <li>Network<ul> <li>Network addresses and subnets are difficult to re-configure. Consider network topology before deploying any VMs</li> </ul> </li> <li>Name for VM<ul> <li>VM names can be 15 characters long for Windows machines and up to 64 characters for Linux machines. The name should include components that identify:<ul> <li>resouce type (vm)</li> <li>Environment (dev, test, prod)</li> <li>Location</li> <li>Instance</li> <li>Product or Service (Outlook, SQL, etc.)</li> <li>Role (security, web, messaging, etc.)</li> </ul> </li> </ul> </li> <li>Location of VM<ul> <li>VMs should be deployed in a Region close to the customer, to ensure best performance and meet any regulations around data residency. However, not all machine sizes are available in all regions and prices for the same machine size can vary between regions</li> </ul> </li> <li>Size of VM<ul> <li>Choose a machine size to match the expected workload. VMs can be re-sized to provide an agile, elastic approach to machine sizing. Resizing a VM can require a restart or result in a change of IP</li> </ul> </li> <li>Estimate Cost<ul> <li>Two separate costs associated with each VM: <ul> <li>compute <ul> <li>priced per-hour but billed per-minute. No charge for VMs that are stopped and deallocated. Reserved instances can result in reduced costs over pay-as-you-go, if you know the VM needs to run continuously or you need budget-predictability. RI VMs can be exchanged or returned for an early-termination fee. </li> </ul> </li> <li>storage<ul> <li>storage charges still apply even when the VM is stopped or deallocated. </li> </ul> </li> </ul> </li> </ul> </li> <li>Choose Storage<ul> <li>All VMs in Azure have at least two disks: an operating system disk and a temporary disk. Additional data disks can also be added. Premium storage disks provide higher throughput and lower latencies. Use multiple data disks for even higher storage and response times. Manage disks are managed by Azure. All disks are stored as Virtual Hard Disks (VHDs): <ul> <li>OS Disk: contains the pre-installed OS. Registered as a SATA drive. </li> <li>Temporary Disk: On Windows machines this is labelled <code>D:</code> and used to store the <code>pagefile.sys</code>. On Linux machines this is registered as <code>/dev/sdb</code> and mounted on <code>/mnt</code>.</li> <li>Data Disks: the number of data disks and that can be attached to a VM and their type is determined by the VM size. </li> </ul> </li> </ul> </li> <li>Choose OS<ul> <li>Azure bundles the cost of the OS into the price of the VM. Azure Marketplace offers images with various application stacks pre-installed. Custom images can be created and uploaded to Azure Storage. Azure only supports 64-bit operating systems.</li> </ul> </li> </ul> <p>When configuring a VM via the Portal, a number of tabs are available:</p> <ul> <li>Basics: project details, admin account and inbound port rules</li> <li>Disks: select OS disk and data disks</li> <li>Networking: VNets and Load Balancer</li> <li>Management: auto-shutdown and backups</li> <li>Advanced: configure agents, scripts and VM extensions</li> <li>Monitoring</li> <li>Tags</li> </ul> <p>Virtual Machine Extensions provide post-deployment configuration and automation tasks for VMs. VM Extensions can be bundled with the VM deployment or run against existing VMs. VM extensions  can be managed from the CLI, Powershell, ARM templates and the Portal. </p> <p>Custom Scripts Extensions can be added to your VMs. After the Custom Script Extension is added to the VM, you can provide a Powershell script file to execute commands on the VM. The script files can be downloaded from Azure Storage or GitHub. Custom Script Extensions have a default timeout value of 90 minutes.  The script can also be run from the command line:</p> <pre><code>Set-AzVmCustomScriptExtension -FileUri https://scriptstore.blob.core.windows.net/scripts/Install_IIS.ps1 -Run \"PowerShell.exe\" -VmName vmName -ResourceGroupName resourceGroup -Location \"location\"\n</code></pre> <p>Desired State Configuration is a management platform in Windows Powershell. DSC provides a  declarative syntax for specifying machine configuration. </p> <p>Additional Resources:</p> <ul> <li>Linux VM Sandbox</li> <li>Windows VM Sandbox</li> </ul>"},{"location":"azure/compute/#vm-types","title":"VM Types","text":"<ul> <li>General Purpose (A, B, D family): balanced CPU-to-Memory ratio. Good for testing, development, small-to-medium databases, low-to-medium traffic web-servers<ul> <li>A Series: for tesing and development</li> <li>B Series: burstable workloads</li> <li>D Series: general-purpose workloads</li> </ul> </li> <li>Compute Optimised (F family): high CPU-to-Memory. Best suited to workloads where CPU is more likely to be a bottle-neck. For example, web servers, application servers, network appliances and batch processes</li> <li>Memory Optimised (E, G, M family): high Memory-to-CPU ration. Best suited for memory-intensive workloads. For example, relational databases, in-memory anlytics</li> <li>Storage Optimised (L family): have high disk-throughput and I/O. Best suited for data analytics, data warehousing, big-data</li> <li>GPU Optimised (N family): have GPUs. Best suited for compute-intensive workloads or graphics rendering or video editing. Typical examples are model training, inference with deep learning, graphics, gaming, visualisations, video-conferencing, video-streaming workloads</li> <li>High Performance (H family): most powerful CPU VMs available providing high-speed network throughput. Suitable for compute and network workloads. For example SAP Hana</li> </ul> <p>VM types use the following naming convention to indicate their capabilities:</p> <ul> <li>Naming convention: [Family] + [Sub-Family] + [# of vCPUs] + [Constrained vCPUs] + [Additive Features] + [Accelerator Type] + [Version]<ul> <li>Additive Features are:</li> <li>a: AMD-based processor</li> <li>b: Block Storage performance</li> <li>c: confidential</li> <li>d: VM has local temp disk</li> <li>i: isolated size</li> <li>l: low-memory state</li> <li>m: memory-intensive size</li> <li>s: premium storage capable (although newer sizes without 's' can also support premium storage)</li> <li>t: tiny memory size</li> <li>NP: node packing</li> <li>P: ARM CPU</li> </ul> </li> </ul> <p>Examples:</p> <ul> <li>B2ms - B-series, 2 vCPUs, memory-intensive, premium storage</li> <li>D4ds v4 - D-series, 4 vCPUs, local temp disk, premium storage, version 4</li> </ul>"},{"location":"azure/compute/#vm-availability","title":"VM Availability","text":"<p>VM resources need to be configured to provide consistent response times and with high availability.  Azure offers several options to meet these requirements. </p> <p>An availability plan should handle responses to:</p> <ul> <li>Unplanned hardware maintenance - Azure issues an unplanned hardware maintenance event when a  resource component is predeictied to fail. Live Migration is used to migrate a VM from failing  hardware to healthy resources and can result in reduced performance during the migration</li> <li>Unexpected downtime - occurs when hardware fails unexpectedly</li> <li>Planned maintenance - periodic updates to VM Host machines normally have minimal impact on VM availability</li> </ul> <p>An Availability Set is a logical grouping of Virtual Machines performing the same tasks.  Availability sets run your VMs across multiple physical servers, compute racks, storage units  and network switches. If a hardware or software failure occurs, only a subset of your VMs will be affected. By grouping machines into availability sets, you ensure that not all  the machines are upgraded at the same time during a host operating system upgrade. A VM can only  be added to an availability set at create time. To add an existing VM, you would need to delete and recreate it. When using availability sets consider:</p> <ul> <li>Redundancy: place mutliple VMs in the Availability Set</li> <li>Application Tiers: each application tier (front-end, db, controllers, etc) should be placed in its own Availability Set</li> <li>Load Balancing: use Load Balancer to distribute incoming traffic to healthy servers</li> <li>Managed Disks for block level storage</li> </ul> <p>Each VM in an Availability Set is placed in one update domain and one fault domain. An update domain is a group of nodes that are upgraded together during a service upgrade: only one update romain is rebooted at a time. There are 5 non-user-configurable update domains, but you can configure up to 20. A fault domain is a group of nodes that share a common set of hardware that represents  a single point of failure.</p> <p>An Availabilty Zone is a combination of a fault domain and an update domain. By placing your VMs across multiple Availability Zones, you are spreading your VMs over multiple fault domains and multiple update domains. Availability Zones are unique physical locations within an Azure region with independant power, cooling and networking. There is a minimum of three separate zones in all enabled regions. Zonal services such as VMs, disks and Standard IP addresses, pin  each resource to a specific zone. Zone-redundant services such as zone-redundant Storage or SQL  Database are automatically replicated across all zones. </p> <p>Scalability for VMs can be either:</p> <ol> <li>Vertical: (scale up and scale down) involves increasing or decreasing VM size in response to workloads</li> <li>Horizontal: (scale out and scale in) involves increasing or decreasing the number of VMs to support changing workload. </li> </ol> <p>Vertical scaling depends on the availability of larger hardware and usually requires stopping and  restarting a VM. </p> <p>VM Scale Sets allow you to deploy and manage a set of identical VMs. VMSS can automatically  increase or decrease the number of VM instances to provide true auto-scaling. VMSS can be  scaled automatically, manually or both. VMSS supports use of both Azure Load Balancer or Azure  Application Gateway for layer-4 or layer-7 distribution. VMSS can support up to 1000 instances or  600 instances if using custom VM images. </p> <p>When configuring a VMSS, you define:</p> <ul> <li>Scaling Policy: Manual or Auto and minimum and maximum instances allowed</li> <li>Scale Out: CPU threshold, duration and number of instances to increase by</li> <li>Scale In: CPU threshold, duration and number of instances to decrease by</li> <li>Scale-in Policy: determines the order in which VMs are removed. VMs are balanced across availability zones and fault domains, then deleting the VM with the highest instance ID (default) or the newest or the oldest VM.</li> </ul>"},{"location":"azure/compute/#azure-container-instances","title":"Azure Container Instances","text":"<p>Azure Container Instances is a PaaS service to run your container instances. ACI provides:</p> <ul> <li>Fast Startup Times</li> <li>Public IP and DNS connectivity</li> <li>Custom Sizes: nodes can be scaled dynamically</li> <li>Persistent Storage: using Azure Files</li> <li>Virtual Network Deployment: containers can be deployed to a VNET</li> </ul> <p>A Container Group is a collection of containers that get scheduled on the same  host machine and share lifecycle, resources, local network and storage. Container Groups work similarly to a Docker compose project. Container  groups can be deployed via ARM templates or a YAML file. The YAML file approach is only recommended when deploying Container Instances with no additional resources such as file shares. Container groups share an IP Address, port range and FQDN. </p> <p>Container Instances uses Docker and thus ensures that the containers run  in the same platform both locally and in Azure. </p> <p>Container Instances are deployed with the following default FQDN: <code>https://${DNS_NAME_LABEL}.${LOCATION}.azurecontainer.io</code> </p> <p>Azure Kubernetes Service - PaaS service for hosting and orchestrating containers</p> <ul> <li>The master node or control plane is managed by Microsoft</li> <li>The worker nodes containing pods are managed by the customer</li> <li>VMs are defined to run the nodes and pods</li> </ul> <p>Azure Service Fabric</p> <p>Azure Batch - large-scale, high-performance batch jobs</p> <ul> <li>Starts pool of compute VMs</li> <li>Installs applications and data</li> <li>Runs jobs</li> <li>Identifies failures</li> <li>Requeues work</li> <li>Scales down as work completes</li> </ul> <p>Azure Functions - serverless compute</p> <ul> <li>Event-driven execution of code: REST request, timer or message from another application</li> <li>Intended for code that can be executed in seconds</li> <li>Coding support for C#, Python, JavaScript, Typescript, Java and Powershell</li> <li>Functions can be created in the Portal, VS Code or using Azure CLI</li> <li>Automatic scaling to meet demand</li> <li>Stateless environment</li> <li>Durable Functions allow Functions to be chained together whilst maintaining state</li> <li>Functions allow developers to focus on the code, without worrying about the underlying infrastructure</li> </ul> <p>Azure Logic Apps - serverless orchestration</p> <ul> <li>Event-driven execution of workflows</li> <li>Low Code or No Code development platform</li> <li>Used to automate and orchestrate tasks, business processes and workflows</li> <li>Integration solution for apps, data and systems</li> <li>Web-based designer</li> <li>Apps are triggered by other Azure services using connectors</li> <li>Gallery includes over 200 connectors</li> </ul> <p>Azure Virtual Desktop - use cloud-hosted versions of Windows from any location</p> <ul> <li>User profiles containerised using FSLogix</li> <li>Centralised identity management with Azure AD</li> <li>Enable MFA and Conditional Access</li> <li>Multi-session Windows 10 deployment</li> <li>Bring your own licences - Azure Hybrid Benefit</li> <li>Session Host VMs can be shared between multiple users or dedicated to a single user, using AAD group assignments</li> <li>Sessions can be allocated in depth mode (all sessions go to the same host until it reaches its limit) or breadth mode (sessions are automatically connected to the next host in the pool)</li> <li>VMs can be started when users connect to them or left permanently on</li> </ul>"},{"location":"azure/keyvault/","title":"Key Vault","text":""},{"location":"azure/keyvault/#key-concepts","title":"Key Concepts","text":"<p>Key Vault Documentation</p> <p>Azure Key Vault provides secure storage containers called (vaults) for  managing application secrets. Key Vault supports certificate management. </p> <p>Each Key Vault is a collection of cryptographic keys and cryptographically protected data (secrets), representing logical groups that need to be managed together. </p> <p>Secret access and vault management is accomplished via a REST API that is supported by all Azure management tools and client libraries. </p> <p>Keys are cryptographic assets stored in key vaults. Keys are accessed  indirectly by applications calling cryptography methods on the Key Vault and the Key Vault performs the requested operation internally.  Applications do not have direct access to the keys. Key Vault supports assymetric keys RSA 2048. </p> <p>Key Vault supports both Hardware-protected and Software-protected keys:</p> <ol> <li>Hardware-protected keys use HSMs (hardware security modules) for processing and key generation. All actions are confined to the HSM boundary. HSM keys can also be imported from your own HSMs - known as bring your own key  or BYOK. The Microsoft Azure Dedicated HSM service can also be used to  protect HSM-protected applications. </li> <li>Software-protected keys use software-based algorithms: RSA and ECC. Keys  are encrypted at rest using HSMs and remain isolated from the application. </li> </ol> <p>Hardware-protected keys provide FIPS 140-2 Level 2 assurance and are  therefore recommended for production environments. HSM-protected keys require a Premium SKU for the Key Vault.</p> <p>Azure services integrate directly with Azure Key Vault and can decrypt secrets without knowledge of the encryption keys. Secrets are encrypted when added to a Key Vault and decrypts them automatically when you  read them. The encryption key used is unique to each Key Vault.</p> <p>Secrets are small data blobs (less than 10K) protected by HSM-generated key created when the Key Vault is created. Secrets are used to protect sensitive application-data</p> <p>Key Vault also provides certificate services to provision, manage and  deploy public and private SSL/TLS certificates. Certificates can be  requested and renewed through partnerships with Certificate Authorities. </p> <p>Key Vault is designed to store configuration settings for server applications. User data should be stored elsewhere, such as in  Storage Accounts or Azure SQL Database. Secrets used by your application  to access that data can be stored in Key Vault. </p>"},{"location":"azure/keyvault/#best-practice","title":"Best Practice","text":"<p>Use RBAC to grant access to Key Vaults. Access to Key Vaults is controlled at two different levels:</p> <ol> <li>Management Plane - access to Key Vault properties but not Key Vault  data</li> <li>Data Plane - controls access to the Key Vault data but not the Key Vault configuration</li> </ol> <p>Both the Management and Data Plane use AAD for authentication. The  Management Plane uses RBAC for authorisation and the Data Plane uses  Access Policies (or the newly added RBAC) for authorisation. Once a  user or application authenticates, they aquire a token for the  resouce endpoint in the corresponding plane. The token can be used to  access a resource in the Key Vault using a REST request. </p> <p>Users with Contributor access to the Key Vault can grant themselves access to the Data Plane by setting a Key Vault access policy. Contributor access to Key Vaults should be tightly controlled. To restrict access to just the  managment plane, it is recommended that the Key Vault Contributor role is used instead. Additionally, the Key Vault deploy role allows  deployed VMs to access secrets from the Key Vault.</p> <p>Reading and Writing data to the Key Vault is controlled via the Key Vault  Access Policy. The Access Policy assigns a permission-set to an identity. </p> <p>Network access can be restricted to:</p> <ol> <li>Specific Azure Vnets</li> <li>Specific IP Addresses</li> <li>Trusted Microsoft Services</li> </ol> <p>Use separate Key Vaults per application per environment. The names for each  secret can be the same across environments, meaning that developers will only  need to change the vault URL in the application to access secrets across  environments. </p> <p>Turn on logging and alerts for your Key Vaults</p> <p>Apps and Users authenticate to Key Vault using an AAD token. To get a AAD token requires the app or user to hold a secret or certificate. To avoid having to  hold this secret/certificate outside of Key Vault, it is recommended to use a  system-assigned managed-identity to authenticate to Key Vault. When you enable a system-assigend managed-identity on your app, Azure activates a token-granting REST service specifically for the app. The App Service requires a secret to  access the REST service, but this is integrated into the App Service environment.  Managed identities also get registered in AAD when the resource is created,  and are automatically deleted when the resource is deleted or the managed  identity is disabled.</p> <p>Azure Key Vault Developers Guide</p>"},{"location":"azure/keyvault/#manage-certificates","title":"Manage Certificates","text":"<p>Key Vault can be used to manage X.509 certificates from various sources.  Self-signed certificates can be generated directly in the Portal. This  process generates a public/private key pair and signs the Certificate with its own key. </p> <p>Alternatively you can generate a X.509 certificate signing request (CSR).  This creates a public/private key-pair and a CSR. The CSR can be passed  to a CA. The signed Certificate returned by the CA can then be merged with  the stored key-pair to finalise the certificate in the Key Vault. </p> <p>You can also connect your Key Vault with a trusted certificate issuer. You can then request to create a certificate and the Key Vault will interact directly with the CA to fulfill the request. The returned certificate is merged with the Key Vault key-pair. This scenario supports the lifecycle management of the Certificate including renewal and revocation. </p> <p>Existing certificates, in PFX or PEM format, can also be imported with the  private key.</p> <p>Certificates in Azure Key Vault can then be associated to your Web App  in the same subscription. Once the certificate is in place, you can  associate your custom domain with the certificate.</p> <p>When a Key Vault certificate is created, an addressable key and secret  with the same name is also created. The key allows key operations and  the secret allows retrieval of the certificate value as a secret. If the policy used to create the certificate indicates that it is exportable, then  the certificate value will include the private key when retrieved as a  secret. If the certificate is flagged as non-exportable, then the secret will not include the private key when retrieved.</p> <p>A certificate policy indicates how to create and manage the certificate life cycle. If the certificate is imported, then the policy is read from the certificate. If the certificate is created in Key Vault, then  the policy must be supplied. </p> <p>A certificate policy contains the following items: </p> <ol> <li>X509 properties - including subject names and aliases and other properties used to create an x509 certificate request</li> <li>Key properties - key type, key length, exportable and reuse key fields</li> <li>Secret properties - including content type of addressable secret</li> <li>Lifetime Actions - includes both a trigger and action for each lifetime action</li> <li>Issuer - information about the certificate issuer</li> <li>Policy attributes - attributes associated with the policy</li> </ol> <p>Before creating a certificate issuer in a Key Vault, the organisation  administrator must onboard the company with a CA provider and generate credentials used with the provider for requesting new and renewal  certificates.</p>"},{"location":"azure/keyvault/#protecting-key-vault","title":"Protecting Key Vault","text":"<p>If a key vault is deleted, any data that is protected using keys from the  Key Vault become inaccessible. Use soft delete and purge protection settings  to protect Key Vaults from inadvertant or malicious deletion. </p> <p>Each key, secret and certificate stored in Key Vault can be backed-up manually using the 'Download Backup' link on each item. The backup acts as an offline copy of the secret.</p> <p>Backups should be made on each update/delete/create event in the Key Vault</p>"},{"location":"azure/keyvault/#key-rotation","title":"Key Rotation","text":"<p>A rotation strategy should be implemented for the values stored in Key Vault secrets. The strategy can be implemented: </p> <ul> <li>as part of a manual process</li> <li>via REST API calls<ul> <li>an example of this would be using Event Grid in response to an expiry event. The Event Grid can call a Function App generates a new secret and updates the application using the secret</li> </ul> </li> <li>using Azure Automation</li> </ul>"},{"location":"azure/keyvault/#key-vault-cli-commands","title":"Key Vault CLI Commands","text":"<pre><code>az keyvault create --name $KV_NAME --group $RG_NAME --location $LOCATION\naz keyvault secret set --vault-name $KV_NAME --name \"Example Password\" --value $SECRET_VALUE\naz keyvault secret show --vault-name $KV_NAME --name \"Example Password\"\naz keyvault set-policy --secret-permissions get list --name $KV_NAME --object-id $SP_ID\n</code></pre>"},{"location":"azure/kusto/","title":"KQL","text":"<p>Access the demo environment</p>"},{"location":"azure/kusto/#kusto-query-language","title":"Kusto Query Language","text":"<p>KQL is used to run queries in Azure Sentinel. Queries start with the data source followed by a set of data transformation operators bound together by using the pipe delimiter:</p> <pre><code>SecurityEvent \n| where EventID == \"4626\" \n| summarize count() by Account \n| limit 10\n</code></pre>"},{"location":"azure/kusto/#let-statement","title":"'let' Statement","text":"<p>'let' statements are used to bind names to expressions and can be used to define variables, functions and views</p> <pre><code>let timeOffset = 7d;\nlet discardEventId = 4688;\nSecurityEvent\n| where TimeGenerated &amp;gt; ago(timeOffset*2) and TimeGenerated &amp;lt; ago(timeOffset)\n| where EventId != discardEventId\n| where AccountType =~ \"user\"\n| limit 100\n</code></pre> <pre><code>let suspiciousAccounts = datatable(account: string) [\n  @\"\\administrator\",\n  @\"NT AUTHORITY\\SYSTEM\"\n];\nSecurityEvent | where Account in (suspiciousAccounts)\n| limit 100\n</code></pre> <pre><code>let LowActivityAccounts = \n  SecurityEvent\n  | summarize cnt = count() by Account\n  | where cnt &amp;lt; 1000;\nLowActivityAccounts | where Account contains \"SQL\"\n| limit 100\n</code></pre>"},{"location":"azure/kusto/#search-operator","title":"'search' Operator","text":"<p>Use the 'search' operator to search for text in multiple tables and colums. 'search' is not as efficient as 'where' but is useful to trackdown data when  you're unsure which table or column to filter:</p> <pre><code>search \"temp\\\\startup.bat\"\n\nsearch in (Event) \"temp\\\\startup.bat\"\n</code></pre>"},{"location":"azure/kusto/#extend-operator","title":"'extend' Operator","text":"<p>The 'extend' operator will create calculated columns and append the new columns  to the resultset</p> <pre><code>SecurityEvent\n| where ProcessName != \"\" and Process != \"\"\n| extend StartDir =  substring(ProcessName,0, string_size(ProcessName)-string_size(Process))\n| order by StartDir desc, TimeGenerated asc\n| limit 100\n</code></pre>"},{"location":"azure/kusto/#project-operator","title":"'project' Operator","text":"<p>The 'project' operator can be used to control output columns from the query</p> project select columns to include, rename, drop or insert project-away select columns to exclude project-keep select columns to keep project-rename rename columns project-reorder reorder columns <pre><code>SecurityEvent\n| where ProcessName != \"\" and Process != \"\"\n| extend StartDir =  substring(ProcessName,0, string_size(ProcessName)-string_size(Process))\n| order by StartDir desc, TimeGenerated asc\n| project TimeGenerated, StartDir, Computer, SubjectUserName, SubjectMachineName\n| limit 100\n</code></pre>"},{"location":"azure/kusto/#aggregation-operators","title":"Aggregation Operators","text":"<p>'summarize' can be used to group result rows. Aggregate operators can be used  to add columns to the resultset. List of aggregate operators:</p> <ul> <li>count(), countif()</li> <li>dcount(), dcountif()</li> <li>avg(), avgif()</li> <li>max(), maxif()</li> <li>min(), minif()</li> <li>percentile()</li> <li>stdev(), stdevif()</li> <li>sum(), sumif()</li> <li>variance(), varianceif()</li> </ul> <pre><code>SecurityEvent\n| summarize IPAddressCount = dcount(IpAddress)\n\nSecurityEvent\n| where TimeGenerated &gt; ago(1h)\n| where EventID == 4624\n| where AccountType == \"Machine\"\n| summarize howMany = count(), firstTime = min(TimeGenerated), lastTime = max(TimeGenerated) by Computer\n| project Computer, firstTime, lastTime, howMany\n\nlet timeframe = 1d;\nlet threshold = 3;\nSigninLogs\n| where TimeGenerated &gt;= ago(timeframe)\n| summarize applicationCount = dcount(AppDisplayName) by UserPrincipalName, IPAddress\n| where applicationCount &gt;= threshold\n</code></pre> <p>The 'arg_max' and 'arg_min' functions return rows for the max and min values of their first parameter respectively. Subsequent parameters specify the list of columns to return. An  asterix can be used to return all columns:</p> <pre><code>SecurityEvent \n| where Computer == \"SQL12.na.contosohotels.com\"\n| summarize arg_max(TimeGenerated, Account, AccountType, EventID) by Computer\n</code></pre> <p>'make_' functions return a dynamic (JSON) array:</p> <pre><code>SecurityEvent\n| where EventID == \"4624\"\n| summarize make_list(Account) by Computer\n\nSecurityEvent\n| where EventID == \"4624\"\n| summarize make_set(Account) by Computer\n</code></pre> <p>'make_set' will return a list of unique values for the given expression</p>"},{"location":"azure/kusto/#render-operator","title":"'render' Operator","text":"<p>The 'render' operator is used to generate visualisations:</p> <pre><code>SecurityEvent \n| summarize count() by Account\n| render barchart\n\nSecurityEvent \n| summarize count() by bin(TimeGenerated, 1d) \n| render timechart\n</code></pre>"},{"location":"azure/kusto/#union-operator","title":"'union' Operator","text":"<p>The 'union' operator takes two or more tables and returns the rows of all of them. </p> <pre><code>SecurityEvent\n| union SigninLogs\n\nunion Security*\n| summarize count() by Type\n</code></pre>"},{"location":"azure/kusto/#join-operator","title":"'join' Operator","text":"<p>The 'join' operator merges the rows of two tables to form a new table by matching specified column values from each table</p> <pre><code>SecurityEvent \n| where EventID == \"4624\" \n| summarize LogOnCount=count() by EventID, Account \n| project LogOnCount, Account \n| join kind = inner (\n  SecurityEvent \n  | where EventID == \"4634\" \n  | summarize LogOffCount=count() by EventID, Account \n  | project LogOffCount, Account \n) on Account\n</code></pre> <p>Various join types are available:</p> <ul> <li>inner: returns rows that match in both left and right</li> <li>leftsemi: returns all left-side rows that have a match on the right-side</li> <li>leftanti (leftantisemi): returns all left-side rows that don't have a match on the right-side</li> <li>leftouter (rightouter or fullouter): returns a row for every row on the left and right, even if it has no match</li> <li>rightsemi</li> <li>rightanti</li> </ul>"},{"location":"azure/kusto/#unstructured-data","title":"Unstructured Data","text":"<p>The 'extract' operator can be used to get a match for a regular expression from a text string. The arguments for extract are:</p> <ul> <li>regex</li> <li>captureGroup</li> <li>text string</li> <li>type literal (optional)</li> </ul> <pre><code>SecurityEvent\n| where EventID == 4672 and AccountType == 'User'\n| extend Account_Name = extract(@\"^(.*\\\\)?([^@]*)(@.*)?$\", 2, tolower(Account))\n| summarize LoginCount = count() by Account_Name\n| where Account_Name != \"\"\n| where LoginCount &lt; 10\n</code></pre> <p>The 'parse' operator can be used to convert a string column into one or more calculated columns. The computed columns will have nulls for unsuccessfullly parsed strings</p> <pre><code>let Traces = datatable(EventText:string)\n[\n\"Event: NotifySliceRelease (resourceName=PipelineScheduler, totalSlices=27, sliceNumber=23, lockTime=02/17/2016 08:40:01, releaseTime=02/17/2016 08:40:01, previousLockTime=02/17/2016 08:39:01)\",\n\"Event: NotifySliceRelease (resourceName=PipelineScheduler, totalSlices=27, sliceNumber=15, lockTime=02/17/2016 08:40:00, releaseTime=02/17/2016 08:40:00, previousLockTime=02/17/2016 08:39:00)\",\n\"Event: NotifySliceRelease (resourceName=PipelineScheduler, totalSlices=27, sliceNumber=20, lockTime=02/17/2016 08:40:01, releaseTime=02/17/2016 08:40:01, previousLockTime=02/17/2016 08:39:01)\",\n\"Event: NotifySliceRelease (resourceName=PipelineScheduler, totalSlices=27, sliceNumber=22, lockTime=02/17/2016 08:41:01, releaseTime=02/17/2016 08:41:00, previousLockTime=02/17/2016 08:40:01)\",\n\"Event: NotifySliceRelease (resourceName=PipelineScheduler, totalSlices=27, sliceNumber=16, lockTime=02/17/2016 08:41:00, releaseTime=02/17/2016 08:41:00, previousLockTime=02/17/2016 08:40:00)\"\n];\nTraces  \n| parse EventText with * \"resourceName=\" resourceName \", totalSlices=\" totalSlices:long * \"sliceNumber=\" sliceNumber:long * \"lockTime=\" lockTime \", releaseTime=\" releaseTime:date \",\" * \"previousLockTime=\" previousLockTime:date \")\" *  \n| project resourceName, totalSlices, sliceNumber, lockTime, releaseTime, previousLockTime\n</code></pre>"},{"location":"azure/kusto/#structured-data","title":"Structured Data","text":"<p>Dynamic fields in a table contain key-value pairs. Values can be accessed using dot notation</p> <pre><code>SigninLogs \n| extend OS = DeviceDetail.operatingSystem\n</code></pre> <p>Fields that store JSON data can be converted to dynamic fields using the todynamic or parse-json functions</p> <pre><code>SigninLogs\n| extend Location =  todynamic(LocationDetails)\n| extend City =  Location.city\n| extend City2 = Location[\"city\"]\n| project Location, City, City2\n</code></pre> <p>'mv-expand' can be used to duplicate each row for every key-value pair found in the JSON field.</p> <pre><code>SigninLogs\n| mv-expand Location = todynamic(LocationDetails)\n| project Location, LocationDetails\n| limit 20\n</code></pre> <p>'mv-apply' can be used to filter the output by key value</p> <pre><code>SigninLogs\n| mv-apply Location = todynamic(LocationDetails) on ( where Location.city == \"Cannock\")\n| project Location, LocationDetails\n| limit 20\n</code></pre>"},{"location":"azure/kusto/#external-data","title":"External Data","text":"<p>The 'externaldata' operator is used to connect to and query from an external data source:</p> <pre><code>externaldata ( ColumnName : ColumnType [, ...] )\n  [ StorageConnectionString [, ...] ]\n  [with ( PropertyName = PropertyValue [, ...] )]\n</code></pre> <p>For example:</p> <pre><code>Users\n| where UserID in ((externaldata (UserID:string) [\n    @\"https://storageaccount.blob.core.windows.net/storagecontainer/users.txt\" \n      h@\"?...SAS...\" // Secret token needed to access the blob\n    ]))\n</code></pre>"},{"location":"azure/kusto/#virtual-tables","title":"Virtual Tables","text":"<p>Queries created in the query editor can be saved as functions to define a virtual table. The  virtual table can then be access by calling the name of the function</p>"},{"location":"azure/monitor/","title":"Azure Monitor","text":"<p>Azure Monitor is based on a common monitoring data platform that enables you to collect and analyse Logs and Metrics from multiple resources. Logs are useful for complex analysis using log queries. Metrics support real-time monitoring.  Use Monitor &gt; Activity Logs to see all activity for the Tenant. You can add filters for  Resource, Resource Group, Resource Type, Operation and Events. </p> <p>Azure Monitor data is collected and stored in tables based on the resource provider  namespaces (which is part of each resource id) using two data stores: Metrics and Logs. Platform Metrics are automatically collected and sent to Azure Monitor, but Resource logs need to be  configured for collection and storage. Resource Logs can be configured by adding  Diagnostic Settings to the resource. Diagnostic settings define the Source (type of log and  metric data) and Destination (Log Analytics Workspace, Storage Account, Event Hub). The same Source can be routed to one of each destination type per diagnostic setting. To route the same Source to multiple destinations of the same type, you will need to configure a  diagnostic setting per destination. A maximum of 5 diagnostic settings can be configured per resource. Change Analysis monitors alerts you to service issues, outages, component failures or other change data, and can be enabled in Change Analysis in the Azure portal. </p> <p>Azure Monitor includes Insights which are out-of-the-box monitoring and troubleshooting  experiences.</p> <p>Azure Monitor allows you to visualise data in Workbooks and Dashboards. Dashboards allow you to combine different kinds of data (Visualisations) into a single pane in the Azure Portal.  Azure Workbooks is a feature of Azure Monitor that provides a flexible canvas for analysing and visualising collected data. Workbooks allow you to combine data from multiple sources in a single report. Workbooks support data from:</p> <ul> <li>Logs</li> <li>Metrics</li> <li>Azure Resource Graph</li> <li>Alerts</li> <li>Workload Health</li> <li>Azure Resource Health</li> <li>Azure Data Explorer</li> </ul> <p>Workbooks are provided with Insights or can be built using pre-defined templates. </p> <p>Monitor supports alerts and event-driven actions, for example run-books or autoscale. Azure  Monitor can integrate with Event Hubs.  Alerts can be sent over email or text message.  Action groups can be configured for alerts, defining the list of recipients and actions to take. </p> <p>You can access Monitor either from the Monitoring section of a resource blade or using the  Monitor service.</p> <p>Azure Monitor is enabled by default - as soon as you create a subscriptions and start to add resources, Azure Monitor starts collecting data. Activity logs record when resources are created or modified. Metrics tell you about the resource performance. Azure Monitor Metrics  data are retained for 3 months (93 days).</p> <p>Some services require an agent (Azure Monitor Agent) installed to collect metrics and logs:</p> <ul> <li>Azure Cloud Services</li> <li>Azure Virtual Machines</li> <li>Azure Virtual Machine Scale Sets</li> <li>Azure Service Fabric</li> </ul> <p>The Azure Monitor agent has replaced the Log Analytics agent, the diagnostic extension, and the Telegraf agent.</p> <p>Log data can be consolidated in a Log Analytics Workspace and allow you to keep data for up to  2 years. The Workspace must be configured  initially and then the different sources need to be configured to send their data to  the Workspace. </p> <p>Additionally, applications will require the Application Insights SDK or auto-instrumentation  (via an agent) to collect information and write it to the Azure Monitor data platform.</p> <p>Azure Monitor stores log data in Azure Monitor Logs (aka Log Analytics) workspace. A workspace is an Azure resource that serves as an administrative boundary or geographic location for  data storage. The workspace is also a container where you collect and aggregate data. In a  workspace, you can isolate data by granting different access rights to users. Log data is organised into tables. Each workspace allows configuration of settings such as: </p> <ul> <li>pricing tier</li> <li>retention</li> <li>data capping</li> </ul> <p>RBAC roles are used to restrict user access to data in the logs workspace. Workspaces are hosted on dedicated clusters which are automatically deployed and managed by Azure. Custom clusters can be used if you require greater control or ingestion rates greater then 500GB. </p> <p>Planning Considerations:</p> <ul> <li>Do you need logs stored in specific regions for data sovereignty</li> <li>Avoid outbound data transfers by having a workspace in the same region as the resources it manages</li> <li>Does the system support multiple departments or business groups or do you need a consolidated view</li> </ul> <p>Deployment Models</p> <ul> <li>Centralised: All logs stored in a central workspace and administered by a single team. Administrative overhead to maintain access control per user. Also known as the hub-and-spoke model</li> <li>Decentralised: Each team has their own workspace created in a resource group they own and manage. Log data is segregated per resource. Difficult to analyse data across groups</li> <li>Hybrid: implements both centralised and decentralised models. Can become complicated to manage and still results in problems accessing data across groups</li> </ul> <p>Access Modes</p> <ul> <li>Workspace Context: users can view all logs in their workspace</li> <li>Resource Context: users access logs for specific resources, resource groups or subscriptions in their workspace. Queries are scoped to only data associated with the resources they have access to. The resouce-context model is a new feature, where log data preserves user access roles from the resource.</li> </ul> <p>Effective monitoring combines Azure Insights with Azure Workbooks. </p>"},{"location":"azure/monitor/#metrics-explorer","title":"Metrics Explorer","text":"<p>The Metrics item in the menu for any Azure resource, allows you to access charts of the metrics collected for the resource. Selecting a graph opens the Metrics Explorer. From the Metrics Explorer, you can chart values from multiple metrics and pin them to a Dashboard.</p>"},{"location":"azure/monitor/#data-explorer","title":"Data Explorer","text":"<p>Azure Data Explorer is a fast and highly scalable data exploration service for log and telemetry data. It can handle multiple data streams from diverse sources. Data Explorer is designed for big data analytics providing support for near-real-time analytics, pattern recognition, time series analysis, anomaly detection, forecasting and machine learning. ADE is integrated with  ML Services such as Databricks and AML to allow the building and deployment of models. Supports cost effective storage of long-term data. </p>"},{"location":"azure/monitor/#further-reading","title":"Further Reading","text":"<ul> <li>sources of data in Azure Monitor</li> <li>Infrastructure metrics and logs</li> <li>Performance Efficiency</li> <li>Azure Data Explorer</li> <li>Holistic Monitoring Strategy</li> <li>Incident Response</li> <li>Intro to Data Explorer</li> <li>Hands-On Storage Troubleshooter</li> <li>Design a Log Analytics Architecture</li> </ul> <p>Both Defender for Cloud and Microsoft Sentinel use Azure Monitor Logs as their underlying  logging data platform. </p>"},{"location":"azure/monitor/#defender-for-cloud","title":"Defender for Cloud","text":"<p>Defender for Cloud can be used to monitor the security of your workloads. Defender for Cloud integrates well with Azure PaaS services and can be enabled for autoprovisioning on IaaS  service (via an agent) such as VMs.</p> <p>Use Defender for Cloud to:</p> <ul> <li>Understand the Security Posture of your Architecture</li> <li>Indentify and address risks and threats</li> <li>Secure a complex infrastructure</li> <li>Secure Hybrid infrastructure</li> </ul> <p>Defender for Cloud uses Azure Monitor Logs to collect data. Data is collected using the  Log Analytics agent. </p> <p>Defender for Cloud gives detailed analyses of data security, network security, identity and access, and application security allowing you to build better infrastructure through  recommendations. </p> <p>Defender for Cloud supports Just-In-Time access controls for VMs. Adaptive Application Controls allow you to control which applications are allowed to run on your VMs.</p> <p>Defender for Cloud provides security alerts for your resources. These can be drilled into and allows actions to be taken based on events that are grouped into incidents. </p>"},{"location":"azure/monitor/#application-insights","title":"Application Insights","text":"<p>Distributed traces are a series of related events that follow a user request through a  distributed system. Distributed tracing in Azure Monitor is enabled through the Application  Insights SDK or installing the correct agent or library. </p> <p>Application Insights can be used to:</p> <ul> <li>Analyse issues with your applications</li> <li>Improve application development lifecycle</li> <li>Measure user experience and analyse user behaviour</li> </ul> <p>Instrumentation refers to the collection of monitoring data. You can enable instrumentation  for Application Insights in your apps by using an agent or SDK. For some App Services,  Application Insights can be enabled from a toggle in the Application Insights blade for  the App Service. Application Insights collects instrumentation and displays  interactive visualisations. Application Insights also detects app dependancies to support distributed tracing and application topology views (Application Map). The Performance pane allows you to track slow requests. By adding instrumentation to your web pages via JavaScript, you can collect usage information such as number of users, sessions, events, OS versions and user location. The Availability pane allows you to create availability tests to continuously monitor the health of your app. Alerts can be configured with alert rules. </p> <p>Application Insights can be used to monitor resource usage and performance for VMs and  Kubernetes using Azure Monitor VM Insights and Azure Monitor Container Insights. Enabling VM Insights adds the required extensions and configuration to your VMs and VM Scale Sets.  Similarly you can monitor the health of controllers, nodes and containers in your  Kubernetes clusters using Container Insights.</p> <p>Application Insights Hub provides guided troubleshooting to triage and isolate issues by  service category. </p>"},{"location":"azure/monitor/#microsoft-sentinel","title":"Microsoft Sentinel","text":"<p>Microsoft Sentinel can be used to collect data across the enterprise and allow you to perform threat and anomaly hunting. Incidents allow you to group related alerts. Playbooks can  be used to automate your response to alerts. Hunting queries can be built either from  scratch or from template queries developed by Microsoft security researchers. Azure Notebooks for Microsoft Sentinel are playbooks consisting of investigation or hunting steps. </p> <p>Use Microsoft Sentinel to:</p> <ul> <li>Get a detailed overview of your Enterprise</li> <li>Avoid having to use disparate tools</li> <li>Use enterprise-grade AI to identify and handle threats</li> </ul> <p>Deploy Sentinel in the Azure Portal by creating a Log Analytics Workspace and adding it to Sentinel. Use Connectors to link data sources to Sentinel. Connectors exist for both Microsoft and Non-Microsoft data sources. Sentinel also provides a REST API to connect other sources.  When you connect a data source, your logs will be synched to Sentinel. </p> <p>Sentinel allows you to add alert rules and specify their severity. Alerts are grouped into  incidents, to ease investigation. When you want to investigate an incident, you can change its status from New to In Progress. Clicking the 'Investigate' link, will display an Investigation Map. The Investigation Map will also show a timeline for related events. </p> <p>Automate response to incidents using Playbooks. Playbooks use Logic Apps connected via triggers to Sentinel alerts. Configure the Logic App trigger to be 'Sentinel Alert triggered' and add the desired actions (e.g. block user account or ip address). When the playbook is configured,  it can be added to the Sentinel Alert rule, in the Automated Response section. </p>"},{"location":"azure/monitor/#cli-commands-for-azure-monitor","title":"CLI commands for Azure Monitor","text":"<ul> <li> <p>Create a workspace for Monitor Logs</p> <pre><code>az monitor log-analytics create --resource-group $RG_NAME --workspace-name $WS_NAME\n</code></pre> </li> <li> <p>List tables in your workspace</p> <pre><code>az monitor log-analytics table list --resource-group $RG_NAME --workspace-name $WS_NAME --output table\n</code></pre> </li> <li> <p>Change the retention time for a table</p> <pre><code>az monitor log-analytics table update --resource-group $RG_NAME --workspace-name $WS_name --name $TABLE_NAME --retention-time 45\n</code></pre> </li> <li> <p>Create regular exports of Log Analytics data to a Storage Account with az monitor log-analytics data-export create</p> </li> <li> <p>Create links between workspaces and Azure resources with az monitor log-analytics workspace linked-service create</p> </li> <li> <p>To create and manage your own Storage Account linked to Azure monitor use az monitor log-analytics linked-storage</p> </li> <li> <p>To manage intelligence packs use az monitor log-analytics workspace pack</p> </li> <li> <p>To manage saved searches in log analytics use az monitor log-analytics workspace saved-search</p> </li> <li> <p>Workspaces can be deleted with az monitor log-analytics workspace delete. Use the --force option for a hard delete, otherwise the workspace will be recoverable for 2 weeks after deletion</p> </li> <li> <p>Create a Diagnostic Setting</p> <pre><code>az monitor diagnostic-settings create  \\\n--name KeyVault-Diagnostics \\\n--resource /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/myresourcegroup/providers/Microsoft.KeyVault/vaults/mykeyvault \\\n--logs    '[{\"category\": \"AuditEvent\",\"enabled\": true}]' \\\n--metrics '[{\"category\": \"AllMetrics\",\"enabled\": true}]' \\\n--storage-account /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/myresourcegroup/providers/Microsoft.Storage/storageAccounts/mystorageaccount \\\n--workspace /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourcegroups/myresourcegroup/providers/microsoft.operationalinsights/workspaces/myworkspace \\\n--event-hub-rule /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/myresourcegroup/providers/Microsoft.EventHub/namespaces/myeventhub/authorizationrules/RootManageSharedAccessKey\n</code></pre> </li> </ul>"},{"location":"azure/monitor/#terraform-scripts-for-azure-monitor","title":"Terraform Scripts for Azure Monitor","text":"<p>Create a Log Analytics Workspace</p> <pre><code>resource \"azurerm_log_analytics_workspace\" \"log-central\" {\n  name                = \"log-central\"\n  resource_group_name = \"rg-log-analytics\"\n  location            = \"uk_south\"\n  sku                 = \"PerGB2018\"\n  retention_in_days   = 90\n\n  tags = var.tags\n\n}\n</code></pre> <p>Create a Key Vault Diagnostic Setting</p> <pre><code>resource \"azurerm_monitor_diagnostic_setting\" \"diag-key-vault-01\" {\n    name                       = \"diag-key-vault-01\"\n    target_resource_id         = azurerm_key_vault.key-vault-01.id\n    log_analytics_workspace_id = azurerm_log_analytics_workspace.log-central.id\n\n    log {\n        category       = \"\"\n        category_group = \"audit\"\n        enabled        = true\n\n        retention_policy {\n        enabled = true\n        days    = 0\n        }\n    }\n\n    log {\n        category       = \"\"\n        category_group = \"allLogs\"\n        enabled        = false\n\n        retention_policy {\n        days    = 0\n        enabled = false\n        }\n    }\n\n    metric {\n        category = \"AllMetrics\"\n\n        retention_policy {\n        enabled = false\n        }\n    }\n}\n</code></pre>"},{"location":"azure/monitor/#platform-logs","title":"Platform Logs","text":"<p>Platform Logs are available at different layers of Azure:</p> <ol> <li>Resource Logs are for Azure Resources and provide insight into operations that were performed inside an Azure Resource (e.g. retrieving data from a database or access a key vault secret). This layer is referred to as the data plane</li> <li>Activity Logs are Azure Subscriptions and provide insight into operations performed on each resource in the subscription from the outside (e.g. configuration changes). This layer is  referred to as the management plane.</li> <li>AAD Logs provides information on sign-in and changes to AAD for the tenant.</li> </ol>"},{"location":"azure/monitor/#log-analytics-charging","title":"Log Analytics Charging","text":"<p>Log Analytics and Application Insights charge for the data they ingest. The Pay-As-You-Go tier is charged at \u00a32.48 per GB per day and allows up to 5GB of data per billing account. Commitment tiers provide savings if the volume of data matches the commitment tier. For example the 100GB per day commitment is charged at \u00a3211.56 per day and works out as \u00a32.12 per GB per day assuming that you consume 100GB per day. </p> <p>Additional ingestion charges are made for:</p> <ol> <li>Basic Log ingestion: for logs that can be managed without full analytics capabilities. Charged at \u00a30.54 per GB of data ingested and \u00a30.006 per GB of data searched</li> <li>Log Data Retention: for logs that are stored for archive purposes. Charged at \u00a30.022 per GB per month and \u00a30.006 per GB of data searched. Archived logs can be restored to enable full log analytics capabilities and this is charged at \u00a30.108 per GB per day</li> <li>Log Data Exports: Log Analytics data that is exported is charged at \u00a30.108 per GB </li> <li> <p>Web Tests: Application Insights provides tests for service health which are charged at:</p> <ul> <li>\u00a30.0006 per scheduled web test execution</li> <li>Free for ping testing</li> <li>\u00a38.635 for test execution for multi-stage web tests</li> <li>Platform logs sent to Log Analytics attract no additional cost over the ingestion cost. When the logs are sent to Event Hub or Storage or MarketPlace Partners there is a charge of \u00a30.270 per GB. </li> <li>Log alerts and Notifications also attract additional charges. See the Log Analytics Pricing page for full details</li> </ul> </li> </ol>"},{"location":"azure/networking/","title":"Azure Networking","text":""},{"location":"azure/networking/#virtual-networks","title":"Virtual Networks","text":"<p>Microsoft VNet Docs</p> <p>Azure Virtual Network (VNet) is a logical isolation of the Azure Cloud dedicated to your  subscription. Azure VNet is an IaaS resource, providing a software-defined private network. </p> <ul> <li>VNets belong to a resource group and can only be part of one Region</li> <li>Traffic entering VNet and region is not billed: only traffic leaving the VNet and Region is billed</li> <li>External VNet routing is either:<ul> <li>Hot-Potato routing: packet is passed to the internet as soon as possible</li> <li>Cold-Potato routing: packets stay on Microsoft network as long as possible. This means packet is handed to an Edge Point of Presence to be delivered to its final destination. Cold-Potato routing ensures the packets spends the least time on the internet, and results in fast delivery and lower latency</li> </ul> </li> <li>Internet communication - VMs can connect to the internet by default. To connect to the VM from the internet you can assign a Public IP address or put the VM behind a public load balancer</li> <li>Communication between Azure resources - some services (VMs, App Services, AKS, VMSS) can communicate via the Virtual Network. Other services (CosmosDB, Service Bus, Key Vault, Storage Accounts, Azure SQL Db) require Service Endpoints</li> <li>Address Space - the address space for each VNet needs to be unique in your subscription and for any other network that you wish to connect to</li> <li>NAT Gateway - you can configure a subnet to use a static outbound IP address when accessing the internet</li> <li>Bastion Host - can be enabled for the VNet</li> <li>DDoS Protection - can be enabled for the VNet</li> <li>Subnet Delegation - designate a subnet to be used for a dedicated service</li> </ul> <p>Each VNet has its own CIDR address block and can be either isoloted or linked to other  VNets and on-prem networks as long as the CIDR blocks do not overlap. </p> <p>Control traffic into a VNet using a VPN Gateway, an NSG or a firewall</p> <ul> <li>A firewall can control access to multiple VNets across multiple subscriptions</li> <li>A VPN Gateway can only be applied to a single VNet</li> <li>NSGs are applied at the Subnet or Device level</li> </ul> <p>The virtual network can be segmented into subnets  (use ipcalc to help with subnet addressing). Azure reserves the first 4 and the last IP  address from each subnet. For the 192.168.1.0/24 subnet, the reservations are:</p> <ol> <li>192.168.1.0 - network address</li> <li>192.168.1.1 - default gateway</li> <li>192.168.1.2 and 192.168.1.3 - azure DNS</li> <li>192.168.1.255 - broadcast address</li> </ol> <p>Address can be either private (for use within the VNet) or public (for communication over the internet and with public-facing Azure resources). Public IP addresses can be asssigned to:</p> <ul> <li>Virtual Machines</li> <li>Load Balancers</li> <li>VPN Gateways</li> <li>Application Gateways</li> </ul> <p>Public IPs come in two SKUs. The Standard SKU only supports static assignment, is only available for network interfaces, load balancers, application gateways and VPN gateways. Standard SKU provides  zone-redundancy and is closed to inbound traffic by default (an NSG must be configured to allow inbound traffic). </p> <p>Public IP Address prefixes are a reserved, static range of public IP addresses. Address prefixes are region-dependant and can be associated to availability zones within the region. </p> <p>Private IPs are assigned from the subnet range that the resource is deployed to. Private IPs support static or dynamic assignment to:</p> <ul> <li>Virtual Machines</li> <li>Load Balancers (front-end config)</li> <li>Application Gateways (front-end config)</li> </ul>"},{"location":"azure/networking/#network-security-groups","title":"Network Security Groups","text":"<p>NSGs are used to limit traffic to resources in a virtual network. An NSG contains rules that allow or deny inbound or outbound traffic. NSGs can be assigned to one or more subnets or NICs.  Inbound traffic is evaluated at the subnet first, then at the NIC. This is reversed for outbound traffic. For the traffic to be allowed, the NSGs must contain an 'allow' rule at both levels. </p> <p>The default rules in an NSG:</p> <ul> <li>Deny all inbound traffic except from the VNet and Azure Load Balancers</li> <li>Allow outbound traffic to the VNet and the Internet</li> </ul> <p>NSGs filter traffic to and from Azure resources within a VNet. Rules are defined by specifying:</p> <ul> <li>Source of traffic</li> <li>Source port of traffic</li> <li>Destination of traffic</li> <li>Destination port of traffic</li> <li>Protocol used</li> </ul> <p>Rules are assigned an action (either allow or deny) and a priority to determine the order in  which they are processed. Higher priority rules override the lower priority rules. NSGs can be  associated with multiple Subnets and NICs, but a Subnet and NIC can only have one NSG associated. NSGs can only be associated to resources in the same region and subscription. Firewalls are  required to filter traffic across multiple subscriptions</p> <ul> <li>Use <code>az network nsg list</code> command to list NSGs</li> <li>Use <code>az network nsg rule list</code> to inspect NSG rules</li> </ul> <p>Application Security Groups logically group Virtual Machines by workload: NICs for  specific VMs can be assigned to the appropriate ASG. The ASGs can then  be used as source or destination in an NSG. This allows you  to control traffic to the servers without needing to specify specific IP addresses. For example,  an NSG can allow internet inbound to the Web Servers ASG, allow the Web Servers to communicate  with the Database servers, and deny other traffic to reach the Database servers.  Members of an ASG must be on the same VNet. When NSG rules refer to two ASGs, they must  both be on the same VNet. Only 3,000 ASGs allowed per subscription</p>"},{"location":"azure/networking/#azure-firewall","title":"Azure Firewall","text":"<ul> <li>Uses a static, public IP for your virtual network resources</li> <li>Integrated with Azure Monitor to enable logging and analytics</li> <li>User-Defined Routing used to control traffic</li> <li>Inbound and Outbound Filtering rules</li> <li>Application rules - allowed FQDNs that can be accessed from a subnet</li> <li>Inbound Destination Network Address Translation (DNAT)</li> <li>Outbound Source Network Address Translation (SNAT)</li> <li>Normally Deployed to a central VNet to control general network access</li> <li> <p>Premium SKU adds:</p> <ul> <li>TLS inspection</li> <li>Intrusion Detection Systems</li> <li>Intrusion Prevention Systems</li> <li>URL Filtering</li> <li>Web Categories</li> </ul> </li> <li> <p>Third-Party NVAs are available via the Azure Marketplace as an alternative to Azure Firewall</p> </li> </ul> <p>Azure Firewall is a stateful firewall (analyses connections not just individual packets)  with high availability and unrestricted scalability. Used  to protect your VNets across subscriptions. Availability zones are configured during deployment. </p> <p>Application rules allow you to restrict traffic using FQDNs. Filtering rules can be specified with IP addresses, ports and protocols. Threat-intelligence feed from Microsoft can be used to filter traffic from known malicious sources. By default, Azure Firewall blocks all traffic unless you enable it. </p> <p>A hub-and-spoke model is recommended for implementing Azure Firewall. The Firewall should be  placed in the hub VNet to filter traffic to and from peers and the on-prem network. </p> <ol> <li>DNAT Rules - DNAT (Destination Network Address Translation) rules are used to translate your Firewall Public IP and port to a private IP and port. NAT rules must be accompanied by a matching network rule that allows the traffic. NAT rules are useful for publishing SSH, RDP or non-HTTP/S applications to the internet. </li> <li>Network Rules - Specifies the ip addresses, ports and protocols to allow for non-HTTP/S traffic (although http(s) traffic can be targetted using port numbers 80 and 443) </li> <li>Application Rules - Specify FQDNs or FQDN tags that can be accessed from a subnet. </li> </ol> <p>DNAT rules are examined first, then Network rules and finally Application rules irrespective  of the priority assigned to the rules. Rules are organised into Rule Collections and Rule  Collection Groups. Rules in a rule Collection must be of the same type. Rule Collection Groups can contain Rule Collections of different type. Rule Collections in a Rule Collection Group and Rules in a Rule Collection are processed by priority with 100 being the highest priority  and 65,000 being the lowest. However, the DNAT-Network-Application processing is order is also  preserved, so the processing order will be:</p> <ol> <li>DNAT Rule Collection Group</li> <li>DNAT Rule Collection</li> <li>Network Rule Collection Group</li> <li>Network Rule Collection</li> <li>Application Rule Collection Group</li> <li>Application Rule Collection</li> </ol> <p>Network rules and application rules are terminating: if a matching rule is found, no subsequent rules are processed. </p> <p>If Threat Intelligence filtering is enabled, these rules are highest priority and processed  before network and application rules. If IDPS is configured in Alert and Deny mode, these  rules are applied after the network and application rules have been processed. </p> <p>You can temporarily stop a deployed firewall by executing: </p> <pre><code>az network firewall ip-config delete -f $FW_NAME -g $RG_NAME -n $CONFIG_NAME\n</code></pre> <p>This will reduce the costs of running the firewall. To re-enable the firewall, you can re-apply  your terraform code or manually run: </p> <pre><code>az network firewall ip-config create -f $FW_NAME -g $RG_NAME -n $CONFIG_NAME --public-ip-address $PIP_NAME --vnet-name $VNET_NAME\n</code></pre>"},{"location":"azure/networking/#vnet-routing","title":"VNet Routing","text":"<p>Azure automatically creates system routes and assigns the routes to each subnet in a VNet. System routes cannot be created or deleted manually, but the can be over-ridden with  Custom Routes. Each route consists of an address prefix and a next hop. The following default routes are added to each subnet in a VNet</p> Source Address Prefix Next Hop Default VNet address space Virtual Network Default 0.0.0.0/0 Internet Default 10.0.0.0/8 None Default 172.16.0.0/12 None Default 192.168.0.0/16 None Default 100.64.0.0/10 None <p>The Virtual Network next-hop rule for the VNet address spaces, ensures that all traffic is  routed within the VNet. Azure creates a route for each address range defined in the  VNet address space. Subnets are within the VNet address space and therefore subnet routing  is also handled by the VNet routes.</p> <p>The Internet next-hop rule, means that all traffic not covered by the Virtual Network rule,  is routed via the internet. If the traffic is to an Azure service then this is routed over  the Azure backbone. </p> <p>The None next-hop rules means that all traffic to these addresses are dropped, unless you  have used these addresses in a VNet, in which case the Virtual Network rule will be used to  replace the None rule. </p> <p>Additional Routes are added when specific Azure features are used:</p> <ol> <li>VNet Peering: a route is added for each address space within the VNets of the peering</li> <li> <p>Virtual Network Gateway: when using a Virtual Network Gateway, routes are added to the VNet with a next-hop type of Virtual Network Gateway. The source is also 'Virtual Network Gateway' because the gateway adds the routes to the subnet. The address prefixes are either:</p> <ul> <li>propogated from on-prem via BGP or</li> <li>configured locally on the local network gateway</li> </ul> </li> <li> <p>Virtual Network Service Endpoint: the public IP addresses for certain services are added to the route table when you enable a service endpoint to a service. Service endpoints are enabled for individual subnets within a virtual network, so the route is only added to the subnet the service endpoint belongs to. The public IP addresses of Azure services change periodically and these are automatically updated in the route tables. </p> </li> </ol> <p>Custom routes can be created to change the default behaviour provided by System Routes. For instance, when you want to direct traffic to a Virtual Network Applicance such as a Firewall.  Custom routes can be created in a route table or by exchanging BGP routes with on-prem gateways. Route Tables can be associated to zero or more VNet subnets. Each subnet can have no more than  one route table associated. User-defined routes are combined with the default routes, and the  user-defined routes take precedence. The next-hop types for custom routes are: </p> <ol> <li> <p>Virtual Appliance: a virtual machine running a network applicance, for example a firewall. For Virtual Appliance next-hop rules, you also need to specify an next-hop IP address which can be one of:</p> <ul> <li>a private IP address for the appliance</li> <li>a private IP address for an Azure internal load balancer</li> </ul> </li> <li> <p>Virtual Network Gateway. The Virtual Network Gateway must be created with type VPN. You can not use a Virtual Network Gateway of type ExpressRoute in a custom route, as ExpressRoute gateways must use BGP</p> </li> <li>None: drops all traffic for the specified address prefix</li> <li>Virtual Network: when you need to override the routing within a virtual network.</li> <li>Internet: used to explicitly route traffic over the internet, or to route traffic to Azure services with a Public IP via the Azure backbone</li> </ol> <p>You can not use VNet-Peering or VirtualNetworkServiceEndpoint as the next-hop type in  user-defined routes, as these are created automatically by Azure when VNet-Peerings or Service  Endpoints are created.</p> <p>Service Tags can be used to specify address-prefixes in custom routes. Service Tags are groupings of IP addresses for specific Azure services. Service Tags are automatically updated when the IP  address prefixes change. Routes with explicit IP-prefixes are given precedence over routes using service tags. Where more than one service tag has matching IP prefixes, precedence is decided in this order:</p> <ol> <li>Regional Tags</li> <li>Top Level Tags (i.e. service type)</li> <li>AzureCloud Regional Tags</li> <li>AzureCloud Tags</li> </ol> <p>Azure uses the most-specific route if two matching routes existing in the routing table. Otherwise custom routes are used before BGP routes and BGP routes are chosen before system routes. </p> <p>The default system-route for 0.0.0.0/0 routes all traffic over the internet (or the Azure Backbone for Azure Services) if no other matching rule is encountered. You can override this with a  custom route that sets the next-hop as either a Virtual Appliance or a Virtual Network Gateway. This means that traffic to Azure Services will also be routed to the Virtual Appliance or  Network Gateway unless you enable Service Endpoints for the service. Service Endpoints will  create routes for the service which will take precedence over the default route. </p> <p>Also, if you override the default 0.0.0.0/0 route, then you will no longer be able to directly  access resources on that subnet from the Internet. Indirect access to the resource is still  possible if inbound traffic passed through the device specified as the next-hop type for the rule. If the next-hop type is Virtual Appliance then this must:</p> <ul> <li>Be accessible from the internet</li> <li>Have a public IP address</li> <li>Not have any NSG rule associated to it that denies connection to the target</li> <li>Not deny the connection</li> <li>Be able to NAT and forward the traffic or be able to proxy the connection</li> </ul> <p>If the next-hop type is Express Route, then your on-prem gateway can NAT-and-forward or proxy  the connection using ExpressRoute private peering. </p> <p>If using a VPN-gateway, don't create a 0.0.0.0/0 custom-route in the Gateway subnet as this  will prevent the VPN-gateway from working. </p> <p>Routing Example</p>"},{"location":"azure/networking/#service-endpoints","title":"Service Endpoints","text":"<p>Service Endpoints allow you to secure Azure service resources to only your VNets. This is a two-step process of: </p> <ol> <li>Enable a service endpoint in a subnet</li> <li>Configure the service to only allow connections from the specified VNets</li> </ol> <pre><code># Enable a service endpoint in a subnet\naz network vnet subnet update \\\n  --resource-group $RG_NAME --vnet-name $VNET_NAME --name subnet-1 \\\n  --service-endpoints \"Microsoft.Storage.Global\"\n\n# Set default Deny rule on Service\naz storage account update \\\n  --resource-group $RG_NAME --name $SA_NAME \\\n  --default-action Deny\n\n# Add network rule to allow access from the subnet\nsubnetid=$(az network vnet subnet show --resource-group $RG_NAME --vnet-name $VNET_NAME --name subnet-1 --query id --output tsv)\naz storage account network-rule add \\\n  --resource-group $RG_NAME --account-name $SA_NAME --subnet $subnetid\n</code></pre> <p>The endpoint extends the VNet identity to the service resource and allows resources in  the VNet to access the service using private IP addresses. Because the endpoint uses the  VNet identity, rather than its address range, endpoints for a service can be added to  multiple VNets even where their addresses overlap.</p> <p>If you want to inspect or filter the traffic sent to  the Azure service, you can apply the service endpoint to the VNA subnet, and then route traffic from the VNet through the VNA. </p> <p>Service Endpoints are available for:</p> <ul> <li>Storage</li> <li>SQL</li> <li>CosmosDB</li> <li>Synapse</li> <li>KeyVault</li> <li>Cognitive Services</li> <li>Container Registry</li> <li>EventHub</li> <li>ServiceBus</li> <li>App Service</li> </ul> <p>Endpoints are enabled on subnets configured in the VNet. For Azure SQL and Data Lake Storage,  the endpoint only applies to traffic within the same region. Service Endpoints ensure that  traffic from the VNet to the service travels on the Azure Backbone only.</p> <p>By default, Azure services secured by service endpoints will not be reachable from on-premises devices. You can allow access to the service from Public IPs using the firewall configuration  for the service.</p> <p>With Service Endpoints, DNS entries for Azure services continue to the Public IP Addresses  assigned to the service. </p> <p>There is no additional charge for using service endpoints and there is no limit on the number of service endpoints in a virtual network.</p>"},{"location":"azure/networking/#private-link","title":"Private Link","text":"<p>Private Link provides a method to access Azure PAAS, customer-owned or partner services privately  without using the public internet. Private Link allows private access to services in other Regions. Private endpoints are accessible using private peering or VPN tunnels from on-prem or peered  VNets. </p> <p>Private Link is used to map the private endpoint to the PAAS service, thus bringing the  service into your private virtual network. </p> <p>Azure PaaS services are normally deployed with a Public IP address. When you access these  services from an Azure VNet via a Public IP, traffic is routed over the internet.</p> <p>Private Link allows you to replace the Public IP address for the service with a private  network interface. The private network interface is a private endpoint which is assigned an  IP address from the range assigned to your subnet. </p> <pre><code>az network private-link-service create \\\n  --resource-group test-rg --name private-link-service \\\n  --vnet-name vnet-1 --subnet subnet-1 \\\n  --lb-name load-balancer --lb-frontend-ip-configs frontend \\\n  --location uksouth\n\nexport resourceid=$(az network private-link-service show --name private-link-service --resource-group test-rg --query id --output tsv)\n\naz network private-endpoint create \\\n  --connection-name connection-1 --name private-endpoint \\\n  --private-connection-resource-id $resourceid \\\n  --resource-group test-rg \\\n  --vnet-name vnet-pe --subnet subnet-pe \\\n  --manual-request false \n</code></pre> <p>Private Link Services also allows you to create private endpoints for your own custom Azure services.  The custom service must be placed behind an Azure Standard Load Balancer and the private endpoint is  attached to the Load Balancer's front-end IP configuration. When you create your own  Private Link Service it is assigned a unique name (prefix.guid.suffix). This name can then be  used by consumers of the service to establish a private endpoint connection to the service  in their own VNets. </p> <p>Private endpoints are mapped to your Azure virtual networks, so that the services appear to be part of the VNet. Private Link allows private access even across Regions. All traffic is  routed using the Azure Backbone and therefore does not pass over the Internet. Once Private Link is enabled for the service, the public endpoint for the service can be disabled to prevent any public access. </p> <p>Peered virtual networks can also access Private Link resources and on-premise networks can also  use the Private Link connections via ExpressRoute or VPN Gateway. </p> <p>If you configure the private endpoints to integrate with your private DNS zone, then the FQDN is automatically assigned to the private endpoint. </p> <p>Private endpoints are charged per hour and per GB of inbound and outbound traffic. </p>"},{"location":"azure/networking/#azure-dns","title":"Azure DNS","text":"<p>When a tenant is created, an initial domain name is assigned in the '.onmicrosoft.com' space.  You can add a domain name with Azure DNS to a subscription and resource group. Use  GoDaddy to search for un-used domain name. You can then delegate the zone to Azure DNS by setting up NS records to point to Azure DNS servers. You can then add A records for the public IPs of  your VMs and use the name servers listed in the NS records to resolve the VM IP addresses. </p> <p>To use your own DNS servers, the custom domain name  must be added to your directory and verified. Verification is performed by adding a  TXT or MX record to the domain.  Azure will then  query the domain for this record. If successful, the domain name is then added to the  subscription and resources can be configured to use this domain. </p> <p>Azure DNS provides a secure DNS service to manage and resolve domain names in a virtual  network. A DNS zone hosts DNS records for a domain: to host your domain in Azure you need  to create a DNS zone for that domain name. DNS zones are added to resource groups. The DNS  zone name must be unique in the resource group, but can be used in other resource groups and  subscriptions. The root/parent domain is registered at the registrar and pointed to Azure DNS. Child Domains are registered in Azure DNS directly.</p> <p>You do not need to own the domain name to host the domain in Azure: but you do need to own the domain to configure the domain.</p> <p>To delegate your domain to Azure DNS, you need to update the parent domain with the NS records from your Azure DNS zone. </p> <p>Private DNS zones enable you to use your own custom domain names (e.g. contoso.org)  and provides name resolution for VMs within a virtual network and between virtual networks.  The Private DNS zone needs to be linked to the VNet(s). When the VNet link is created,  you can enable 'auto registration' for connected devices. DNS records for the private zone are not viewable or retrievable, but the records are registered and will resolve. </p> <p>For a single VNet, when you create a Private Zone and link it to the VNet, VMs in the VNet can  carry out dns forward (name to IP) and reverse (IP to name) resolution for the private IP addresses of the VMs. </p> <p>For multiple VNets linked to a common private DNS zone, forward DNS lookups are resolved, but reverse DNS lookups are only resolved for addresses in the same VNet.</p> <p>Alias Records can be added to Azure DNS using Azure Resource Names (e.g. the resource  name of a Load Balancer public IP address). This way, when the resource IP address updates, the DNS record will also dynamically update. </p>"},{"location":"azure/networking/#vnet-peering","title":"VNet Peering","text":"<p>VNet Peering allows you to connect two or more VNets so that they appear as a single network from a connectivity perspective. There are two types of peering: regional for VNets in the same region and; global for VNets in different regions.</p> <p>VNet Peering keeps traffic on the Azure Backbone and provides a low-latency,  high-bandwidth connection between resources. </p> <p>A VPN Gateway can be used in one of the peered VNets to enable the other peers to access resources outside the VNet. This would typically be used in the Hub VNet of a Hub-and-Spoke model to allow the spokes to communicate with on-prem networks via the Hub VPN Gateway. This setup requires 'Allow Gateway Transit' enabled in the hub VNet and  'Allow remote gateways' in the spoke VNets.</p> <p>VNet Peering is non-transitive, but you can configure user-defined routes and service chaining to provide transitivity. </p>"},{"location":"azure/networking/#vpn-gateway","title":"VPN Gateway","text":"<p>See also Create a Site-to-Site VPN connection</p> <p>A VPN Gateway is used to connect two trusted private networks using an encrypted  tunnel over a public, untrusted network. A VPN Gateway can be used to connect  Azure VNets and On-prem networks and typically provides up to 1 Gbps bandwidth. </p> <p>Only one VPN Gateway can be deployed per VNet, but multiple connections can be made to the same VPN Gateway. Connections can be: </p> <ul> <li>Site to Site </li> <li>VNet to VNet</li> <li>Point to Site (connects an individual device)</li> </ul> <p>For a Site-to-Site connection, the following steps are required to configure a VPN connection:</p> <ol> <li> <p>Create VNets and Subnets</p> <p>requires a reservation from the on-site network address range. Ensure that there is no overlap between on-prem and cloud networks/subnets.</p> </li> <li> <p>Specify the DNS server (optional)</p> <p>required for name resolution for resources deployed in the VNet</p> </li> <li> <p>Create the Gateway Subnet</p> <p>requires a /28 or /27 CIDR block. The Gateway subnet must be called 'GatewaySubnet'</p> </li> <li> <p>Create the VPN Gateway</p> <p>when creating the virtual network gateway you can choose between VPN or ExpressRoute as the gateway type, and route-based or policy-based as the VPN type. Policy-based only supports IKEv1, comes with a Basic SKU only, and provides only one tunnel. Policy-based VPNs direct traffic using IPsec policies. Route-based is the usual choice and uses route table to direct traffic. The SKU will determine the number of tunnels available and the aggregate throughput of the connection. The different SKUs provide between 30-100 S2S connections, 250 to 10,000 P2S connections and 650Mbps to 10.0 Gbps aggregate throughput. During this step you will also be able to choose between active-active or active-standby mode. terraform resource: <code>azurerm_virtual_network_gateway</code></p> </li> <li> <p>Create the Local Network Gateway</p> <p>A reference to the On-Prem location. Contains a name for the connection, the IP address or FQDN of the on-prem VPN device, and one or more IP address ranges that define the address range for the on-prem network</p> </li> <li> <p>Configure the On-Prem VPN Device</p> <p>Check the list of validated devices</p> </li> <li> <p>Create the VPN Connection</p> <p>Uses the Local Network Gateway, Virtual Network Gateway and a Pre-Shared Key (PSK). The Shared Key value must match the value used when configuring the on-prem VPN Device in Step 6.</p> </li> </ol> <p>A VPN Gateway consists of two or more VMs deployed to a dedicated Gateway Subnet in the VNet. VPN Gateways support Availability Zones. When connecting to a single  on-site VPN, the VPN Gateway can be configured in Active/Standby mode, where only one instance connects to the on-site VPN device. Failover is automatic and  should occur in 10 to 15 seconds for planned maintainence or 60 to 90 seconds  for unplanned outtages. If two on-site VPNs are available then the Azure VPN  Gateway can be configured in Active/Active mode. </p> <ul> <li>A VNet can only have one VPN Gateway and the VPN Gateway can only be associated with one VNet.</li> <li>VNet address spaces can not overlap with on-prem network address spaces. </li> <li>Use a pre-shared key for authentication</li> <li>Internet Key Exchange (IKE) is used to set up a security association (an agreement of the encryption) between two endpoints</li> <li>IPSec suite used to encrypt/decrypt data based on the security association</li> <li>VPN is either:<ol> <li>Policy-based<ul> <li>Specifies static IP address for encryption through the tunnel</li> <li>supports IKEv1 only</li> <li>relies on static route definitions</li> <li>primarily used where legacy on-prem VPN devices require them</li> </ul> </li> <li>Route-based<ul> <li>IPSec tunnel modelled as a network interface or vitual tunnel interface</li> <li>IP routing decides whether to use IPSec tunnel</li> <li>supports IKEv2</li> <li>Can use dynamic routing protocols</li> <li>more resilient to creation of new subnets</li> </ul> </li> </ol> </li> <li>Deploy VPN Gateway<ul> <li>GatewaySubnet - add a subnet called GatewaySubnet to the VNet with a netmask of at least /27</li> <li>Public IP address - Basic SKU Public IP needed as target for on-prem VPN device</li> <li>Local Network Gateway: defines the on-prem network address spaces that will be connecting to Azure and the on-prem VPN used. The on-prem VPN appliance is defined either by IP Address or FQDN</li> <li>Virtual Network Gateway - routes traffic between the VNet and on-prem network</li> <li>Connection: creates a logical connection between the Local Network Gateway and the VPN Gateway</li> </ul> </li> <li>VPN gateways can also be used for ExpressRoute failover or in Zone-redundant configuration.</li> </ul> <p>Terraform Sample Code:</p> <pre><code>resource \"azurerm_resource_group\" \"example\" {\nname     = \"test\"\nlocation = \"West US\"\n}\n\nresource \"azurerm_virtual_network\" \"example\" {\nname                = \"test\"\nlocation            = azurerm_resource_group.example.location\nresource_group_name = azurerm_resource_group.example.name\naddress_space       = [\"10.0.0.0/16\"]\n}\n\nresource \"azurerm_subnet\" \"example\" {\nname                 = \"GatewaySubnet\"\nresource_group_name  = azurerm_resource_group.example.name\nvirtual_network_name = azurerm_virtual_network.example.name\naddress_prefixes     = [\"10.0.1.0/24\"]\n}\n\nresource \"azurerm_local_network_gateway\" \"onpremise\" {\nname                = \"onpremise\"\nlocation            = azurerm_resource_group.example.location\nresource_group_name = azurerm_resource_group.example.name\ngateway_address     = \"168.62.225.23\"\naddress_space       = [\"10.1.1.0/24\"]\n}\n\nresource \"azurerm_public_ip\" \"example\" {\nname                = \"test\"\nlocation            = azurerm_resource_group.example.location\nresource_group_name = azurerm_resource_group.example.name\nallocation_method   = \"Dynamic\"\n}\n\nresource \"azurerm_virtual_network_gateway\" \"example\" {\nname                = \"test\"\nlocation            = azurerm_resource_group.example.location\nresource_group_name = azurerm_resource_group.example.name\n\ntype     = \"Vpn\"\nvpn_type = \"RouteBased\"\n\nactive_active = false\nenable_bgp    = false\nsku           = \"Basic\"\n\nip_configuration {\n    public_ip_address_id          = azurerm_public_ip.example.id\n    private_ip_address_allocation = \"Dynamic\"\n    subnet_id                     = azurerm_subnet.example.id\n}\n}\n\nresource \"azurerm_virtual_network_gateway_connection\" \"onpremise\" {\nname                = \"onpremise\"\nlocation            = azurerm_resource_group.example.location\nresource_group_name = azurerm_resource_group.example.name\n\ntype                       = \"IPsec\"\nvirtual_network_gateway_id = azurerm_virtual_network_gateway.example.id\nlocal_network_gateway_id   = azurerm_local_network_gateway.onpremise.id\n\nshared_key = \"4-v3ry-53cr37-1p53c-5h4r3d-k3y\"\n}\n</code></pre>"},{"location":"azure/networking/#expressroute-and-vwan","title":"ExpressRoute and VWAN","text":"<p>Use Azure ExpressRoute to  create private connections between Azure Data Centres and your on-prem network: ExpressRoute connections do not go over the public internet. Connection is  private but not encrypted.</p> <p>ExpressRoute connections can be made through an approved connectivity provider using:</p> <ol> <li>Colocation: using an Exchange with Azure co-location</li> <li>Point-to-Point Ethernet connection</li> <li>IPVPN Connection: connect your WAN directly using Multi-Protocol Label Switching (MPLS) VPN</li> </ol> <p>ExpressRoute provides bandwidth up to 100 Gbps and low latency and is a good option for  heavy-duty data transfers such as storage, backups and recovery. ExpressRoute can also be used for Hybrid applications such as a Web App in Azure that uses on-prem AD for authentication and authorisation. </p> <p>ExpressRoute Features:</p> <ol> <li>Layer 3 connectivity - uses BGP to exchange routes with on-prem routers</li> <li>Redundancy - each ExpressRoute connection consists of two BGP connections to two  Microsoft Enterprise Edge (MSEE) routers. </li> <li>Regional Connectivity - the peering location for the ExpressRoute connection grants access to resources in the same geopolitical region. </li> <li>Global Connectivity - with ExpressRoute Premium add-on, the connection provides access to  resources from all regions, except national clouds</li> <li>On-Prem connectivity - on-prem datacenters in different locations can be connected together using ExpressRoute circuits if ExpressRoute Global Reach is enable. This way traffic between the two datacenters does not need to traverse the public internet.</li> </ol> <p>ExpressRoute can be used in addition to Site-to-Site VPN: you just need to configure two  VPN Gateways: one of type VPN and another of type ExpressRoute. </p> <p>Azure Virtual WAN  provides a cloud-hosted network hub that provides transitive network  connectivity to sites that are connected using combinations of Point-to-Site, Site-to-Site and ExpressRoute connections (spokes). Azure VWAN comes in two SKUs: Basic only supports the use of Site-to-Site VPN connections; Standard supports all types of VPN connections.</p>"},{"location":"azure/networking/#network-watcher","title":"Network Watcher","text":"<p>Add the network watcher agent to a VM:</p> <pre><code>Set-AzVMExtension `\n-ResourceGroupName $rgName `\n-Location $location `\n-VMName $vmName `\n-Name 'networkWatcherAgent' `\n-Publisher 'Microsoft.Azure.NetworkWatcher' `\n-Type 'NetworkWatcherAgentWindows' `\n-TypeHandlerVersion '1.4'\n</code></pre>"},{"location":"azure/networking/#load-balancer","title":"Load Balancer","text":"<p>Load balancing refers to evenly distrubuting load across a group of backend resources. Azure Load Balancer works at Layer 4 (Transport), and therefore uses IP address and Port to route traffic.  The load balancer distributes inbound flows that arrive  at the front-end to the backend resources according to load-balancing rules and health probes.  The backend resources can be either VMs or instances in a VMSS. </p> <p>Public load balancers are used to load balance internet traffic to your VMs.  The Load Balancer maps public IP address and port of incoming traffic to private IP address and port of the VM. Mapping is also provided for the response traffic from the VM (SNAT).</p> <p>The default distribution mode for Azure Load Balancer is a five-tuple hash (source ip, source  port, destination IP, destination port and protocol). An alternate distribution mode,  source IP affinity, is available which uses a two-tuple hash (source IP and destination IP)  or a three-tuple hash (source IP, destination IP and protocol). </p> <p>Internal load balancers are used to load balance traffic inside a VNet or to load balance traffic to Azure resources via a VPN connection. </p> <p>Load Balancers aren't physical instances: Load Balancer objects are used to express how  Azure configures its infrastructure to meet your requirements.</p> <p>Availability Sets protect from hardware failures within a datacentre and provide a 99.95% SLA. Availability Zones protect from entire datacentre failure and provide a 99.99% SLA. Availability Zones provide redundancy across datacentres in the same Region. </p> <p>Load Balancers come in two main SKUs:</p> <ul> <li>Basic: supports 300 instances in backend pools, HTTP and TCP health probes, no zone redundancy, NSGs are optional, no SLA</li> <li>Standard: supports 1000 instances in the backend pool, HTTPS, HTTP and TCP health probes, zone-redundant and zonal frontends for inbound and outbound traffic, inbound flows closed by default unless allowed via an NSG, 99.99% SLA</li> </ul> <p>For the Basic SKU, backend pool devices are limited to machines in a single availability set or VMSS. Standard SKU can use any VM in a single VNet, availability set or VMSS.</p> <p>Load balancer rules map a given frontend IP:Port to a set of backend IP:Port combinations: before configuring the rule, you should create the frontend, backend and health probe. Session persistence can be based on:</p> <ul> <li>None: (default) any VM can handle the request</li> <li>Client IP: successive requests from the same IP are handled by the same VM</li> <li>Client IP and Protocol: successive requests from the same IP:Port combination are handled by the same VM</li> </ul> <p>HTTP health probes rely on a 200 Status response from the backend within 31 seconds. TCP probes expect a successful connection on a specified port: you can configure Port, Interval and Unhealthy Threshold. Guest agent probes can also be used, but only when HTTP/TCP probes are not possible.</p> <p>Traffic Manager is a DNS-based load balancer, that provides load-balancing at the Global  (cross-regional) level. </p>"},{"location":"azure/networking/#application-gateway","title":"Application Gateway","text":"<p>Azure Application Gateway implements load-balancing at Layer 7 (Application), and makes  decisions based on attributes of the HTTP request such as the URI path (path-based routing)  or address (multi-site routing). The backend pool can include VMs, VMSS, App Service and even on-prem servers. Application  Gateway uses round-robin to load balance requests and provides the following features:</p> <ul> <li>Support for HTTP, HTTP/2, HTTPS, and Websockets</li> <li>Application firewall: checks requests for common security threats</li> <li>End-to-end request encryption</li> <li>Autoscaling</li> <li>Redirection</li> <li>Rewrite HTTP Headers</li> <li>Custom Error Pages</li> </ul> <p>Application Gateway health probes can accept HTTP responses between 200 and 399 to indicate OK. If no health probe is configured, App Gateway configures a default the expects a response within 30 seconds. </p> <p>Application Gateway can load balance within a Region: for Global Load Balancing at the  Application Layer use Azure Front Door. </p>"},{"location":"azure/networking/#azure-content-delivery-network","title":"Azure Content Delivery Network","text":"<ul> <li>Includes Web Application Firewall (WAF) service</li> </ul>"},{"location":"azure/networking/#azure-ddos-protection","title":"Azure DDoS Protection","text":"<ul> <li>Discards DDoS traffic at the Azure network edge  </li> <li>Protects against over-consumption of resources</li> <li>A singel DDoS protection plan is enabled at the tenant level and used across multiple subscriptions</li> <li>Default Basic plan is free</li> <li>Standard SKU adds protection from:<ul> <li>volumetric attacks</li> <li>protocol attacks</li> <li>resource layer (application) attacks</li> </ul> </li> </ul>"},{"location":"azure/networking/#azure-traffic-manager","title":"Azure Traffic Manager","text":"<ul> <li>a DNS-Based traffic load balancer</li> <li>lets you distribute traffic across global Azure regions</li> </ul>"},{"location":"azure/networking/#cli-commands","title":"CLI Commands","text":"<pre><code>az network route-table list -o table\naz network route-table list --query \"[].[name,resourceGroup,routes[].[addressPrefix,nextHopType, nextHopIpAddress]]\n\naz network nic show-effective-route-table -n $NIC -g $RESOURCE_GROUP\n</code></pre>"},{"location":"azure/networking/#resticting-access-to-paas-resource-with-service-endpoints","title":"Resticting Access to PaaS Resource with Service Endpoints","text":"<pre><code>RG_NAME=rg-play-tut001\nVNET_NAME=vnet-play-tut001\nLOCATION=uksouth\nPUBLIC_SNET=snet-public\nPRIVATE_SNET=snet-private\nPRIVATE_NSG=nsg-private\nSTORAGE_NAME=stgplaytut001\nFILE_SHARE=fs-play-tut001\n</code></pre> <ol> <li> <p>Create the VNET with Public Subnet</p> <pre><code>az group create -n $RG_NAME --location $LOCATION\n\naz network vnet create \\\n--name $VNET_NAME \\\n--resource-group $RG_NAME \\\n--address-prefix 10.0.0.0/16 \\\n--subnet-name $PUBLIC_SNET \\\n--subnet-prefix 10.0.0.0/24\n</code></pre> </li> <li> <p>List the Services supporting Service Endpoints</p> <pre><code>az network vnet list-endpoint-services --location $LOCATION -o table\n</code></pre> </li> <li> <p>Create an Additional Subnet with a Storage Service Endpoint</p> <pre><code>az network vnet subnet create \\\n--vnet-name $VNET_NAME \\\n--resource-group $RG_NAME \\\n--name $PRIVATE_SNET \\\n--address-prefix 10.0.1.0/24 \\\n--service-endpoints Microsoft.Storage\n</code></pre> </li> <li> <p>Restrict Access for the Private Subnet</p> <pre><code>az network nsg create \\\n--resource-group $RG_NAME \\\n--name $PRIVATE_NSG\n\n# add an NSG to the Private Subnet\naz network vnet subnet update \\\n--vnet-name $VNET_NAME \\\n--name $PRIVATE_SNET \\\n--resource-group $RG_NAME \\\n--network-security-group $PRIVATE_NSG\n\n# allow outbound access to Storage Public IPs\naz network nsg rule create \\\n--resource-group $RG_NAME \\\n--nsg-name $PRIVATE_NSG \\\n--name Allow-Storage-All \\\n--access Allow \\\n--protocol \"*\" \\\n--direction Outbound \\\n--priority 100 \\\n--source-address-prefix \"VirtualNetwork\" \\\n--source-port-range \"*\" \\\n--destination-address-prefix \"Storage\" \\\n--destination-port-range \"*\"\n\n# deny outbound to all public IP Addresses\naz network nsg rule create \\\n--resource-group $RG_NAME \\\n--nsg-name $PRIVATE_NSG \\\n--name Deny-Internet-All \\\n--access Deny \\\n--protocol \"*\" \\\n--direction Outbound \\\n--priority 110 \\\n--source-address-prefix \"VirtualNetwork\" \\\n--source-port-range \"*\" \\\n--destination-address-prefix \"Internet\" \\\n--destination-port-range \"*\"\n\n# allow SSH inbound\naz network nsg rule create \\\n--resource-group $RG_NAME \\\n--nsg-name $PRIVATE_NSG \\\n--name Allow-SSH-All \\\n--access Allow \\\n--protocol Tcp \\\n--direction Inbound \\\n--priority 120 \\\n--source-address-prefix \"*\" \\\n--source-port-range \"*\" \\\n--destination-address-prefix \"VirtualNetwork\" \\\n--destination-port-range \"22\"\n</code></pre> </li> <li> <p>Create a Storage Account</p> <pre><code>az storage account create \\\n--name $STORAGE_NAME \\\n--resource-group $RG_NAME \\\n--sku Standard_LRS \\\n--kind StorageV2\n\nsaConnectionString=$(az storage account show-connection-string \\\n--name $STORAGE_NAME \\\n--resource-group $RG_NAME \\\n--query 'connectionString' \\\n--out tsv)\n</code></pre> </li> <li> <p>Create a File Share in the Storage</p> <pre><code>az storage share create \\\n--name $FILE_SHARE \\\n--quota 2048 \\\n--connection-string $saConnectionString &gt; /dev/null\n</code></pre> </li> <li> <p>Deny all access to the Storage Account</p> <pre><code>az storage account update \\\n--name $STORAGE_NAME \\\n--resource-group $RG_NAME \\\n--default-action Deny\n</code></pre> </li> <li> <p>Allow access to the Storage from the Private Network</p> <pre><code>az storage account network-rule add \\\n--resource-group $RG_NAME \\\n--account-name $STORAGE_NAME \\\n--vnet-name $VNET_NAME \\\n--subnet $PRIVATE_SNET\n</code></pre> </li> <li> <p>Create VMs on both Subnets</p> <pre><code>az vm create \\\n--resource-group $RG_NAME \\\n--name myVmPublic \\\n--image UbuntuLTS \\\n--vnet-name $VNET_NAME \\\n--subnet $PUBLIC_SNET \\\n--generate-ssh-keys\n\naz vm create \\\n--resource-group $RG_NAME \\\n--name myVmPrivate \\\n--image UbuntuLTS \\\n--vnet-name $VNET_NAME \\\n--subnet $PRIVATE_SNET \\\n--generate-ssh-keys\n</code></pre> </li> <li> <p>SSH to VMs and mount the Share (only succeeds on privateVM)</p> <pre><code>ssh &lt;publicIpAddress&gt;\n\nsudo mkdir /mnt/MyAzureFileShare\nsudo mount --types cifs //$STORAGE_NAME.file.core.windows.net/$FILE_SHARE /mnt/MyAzureFileShare --options vers=3.0,username=$STORAGE_NAME,password=$AccountKey,dir_mode=0777,file_mode=0777,serverino\n</code></pre> </li> <li> <p>Try to list share from own device (fails)</p> <pre><code>az storage share list \\\n--account-name $STORAGE_NAME \\\n--account-key $AccountKey\n</code></pre> </li> </ol>"},{"location":"azure/storage/","title":"Azure Storage","text":"<p>Azure Storage is a cloud storage solution supporting blobs, files, disks, messages, tables and other types of data. Combining data services into a single account allows you to manage them as a  group - the settings applied to the Storage Account apply to all services within the  account. Deleting a Storage Account also deletes all the data in it. </p> <p>For each combination of settings that you require, you will need a corresponding Storage Account. An Azure Storage Account defines the following settings:</p> <ul> <li>Subscription (for billing)</li> <li>Location (where the data is stored)</li> <li>Performance (Standard or Premium). Premium is intended for low-latency  data access and uses SSDs. You cannot convert between Standard and  Premium storage: instead you will need to create a new Storage  Account and migrate the data from the original Account. Standard is  generally sufficient for most new Storage Accounts. Premium offers three specialist flavours: Page Blobs, Block Blobs or File Shares. </li> <li>Replication (LRS, ZRS, GRS)</li> <li>Access Tier (Hot, Cold, Archive)</li> <li>Secure Transfer Required</li> <li>Virtual Networks. Use the 'Firewalls and virtual networks' settings on your storage accounts to restrict access to specifici virtual networks or public IPs. Subnets and Virtual networks must exist in the same Azure region or region pair as the storage account. </li> </ul> <p>The number of Storage Accounts that you will need depends on your data  diversity (where it is consumed, how sensitive it is, who pays the bill, how business critical is the data, etc). Before creating each Storage  you should first analyse the data and understand the appropriate storage settings. </p> <p>The following limits apply to Storage Accounts:</p> <ul> <li>Number of storage accounts per region per subscription: 250</li> <li>Maximum storage account capacity: 5 PiB</li> <li>No limit to number of containers, blobs, files, tables, etc</li> <li>Maximum request rate per storage account: 20,000 requests per second</li> <li>Maximum ingress per storage account (US/Europe): 10 Gbps</li> <li>Maximum ingress per storage account (outside US/Europe): 5 Gbps</li> <li>Maximum egress: 50 Gbps</li> </ul> <p>Ingress rates may be increased by raising a support request with Microsoft. Data is durable, secure, scalable, managed, accessible. </p> <p>Azure storage encryption is enabled by default and cannot be disabled.  Data is encrypted using 256-bit AES (advanced encryption standard).  Encryption keys can be either Microsoft-managed or Customer-managed.  Customer-managed keys are stored in Key Vault and can be generated by Key Vault or independantly. To manage a Storage Account the Key Vault keys need to be in the same region as the SA, but can be in a  different subscription. </p> <p>Azure storage provides a unique namespace for your data, accessible  over HTTP or HTTPS.  Namespaces are formatted: <code>https://$STORAGE_ACCOUNT_NAME.$SERVICE_NAME.core.windows.net</code>. For example:</p> <ul> <li>Blobs: https://$STORAGE_ACCOUNT_NAME.blob.core.windows.net</li> <li>Files: https://$STORAGE_ACCOUNT_NAME.file.core.windows.net</li> <li>Queues: https://$STORAGE_ACCOUNT_NAME.queue.core.windows.net</li> <li>Table: https://$STORAGE_ACCOUNT_NAME.table.core.windows.net</li> <li>Data Lake: https://$STORAGE_ACCOUNT_NAME.dfs.core.windows.net</li> </ul> <p>You can also configure a custom domain to access blob data in a Storage Account. However,  there is no native support for HTTPS access to blobs with custom domains. Custom domains can be configured with direct mapping or intermediary mapping.</p> <p>Four options are available for accessing Blob Storage:</p> <ul> <li>AAD. A security principal authenticates to AAD and receives an OAuth 2.0  token, which is then used to authenticate to the storage. Works well for  apps using managed identities or security principals.</li> <li>Shared Keys for the Storage Account. Two 512-bit keys are created with every Storage Account. These keys can be used to grant full access to the Storage Account. It is recommended that these keys are regularly rotated.</li> <li>Shared Access Signature: SAS keys grant granular-access for a specified period of time. Three types of SAS key:<ul> <li>User delegation SAS - secured with AAD credentials - only available for Blob Storage access</li> <li>Service SAS - secured with the storage account keys - grants access to one of the four storage services in an account. </li> <li>Account SAS - secured with the storage account keys - grants access to the services in the SA and also to service-level operations</li> </ul> </li> <li>Public: read-only anonymous access. To enable public access to containers or blobs, public access must also be enabled for the Storage Account. </li> </ul> <p>SAS URI parameters:</p> <ul> <li>restype: indicates service or account-level operations</li> <li>sv: storage version</li> <li>ss: storage service</li> <li>st: start time</li> <li>se: expiry time</li> <li>sr: resource</li> <li>sp: permissions</li> <li>sip: ip address ranges</li> <li>spr: ip protocols</li> <li>sig: signature </li> </ul> <p>User Delegation SAS tokens are recommended for applications, as you  don't have to use the storage account keys in code. </p> <p>Stored access policies can be associated to Service SAS keys, to give you  the option to revoke permissions without having to regenerate the  storage account keys. The only way to revoke permissions on ad-hoc  SAS keys is to regenerate the SA Shared Keys.</p>"},{"location":"azure/storage/#storage-emulators","title":"Storage Emulators","text":"<p>You can use Storage Emulators during development, to avoid incurring  costs associated with using Azure Storage Accounts:</p> <ol> <li>Azure Storage Emulator - uses MSSQL 2012 Express LocalDB to emulate Azure Table, Queue and Blob Storage</li> <li>Azurite - Node.js open-source emulator</li> </ol>"},{"location":"azure/storage/#replication-strategies","title":"Replication Strategies","text":"<ol> <li>LRS - Locally Redundant Storage. Data is replicated to a second copy in the same data centre</li> <li>ZRS - Zone Redundant Storage. Data is replicated across three availability zones in the same region.</li> <li>GRS - Geo Redundant Storage. Data is replicated to a secondary region. Regional data is also replicated locally with LRS.</li> <li>RA-GRS - Read-Access GRS. Provides read-access to data in the secondary region. </li> <li>GZRS - Geo-Zone Redundant Storage. Data is replicated across three availability zones in the primary region, and also replicated to a secondary region across three availability zones. </li> <li>RA-GZRS - Read-Access GZRS. Provides read-access to data in the secondary region. </li> </ol>"},{"location":"azure/storage/#azure-blob-storage","title":"Azure Blob Storage","text":"<p>Blob Storage is optimised for massive amounts of unstructured data such as text or binary data. Blobs are stored in Containers, which act like folders to organise the blob storage.</p> <ul> <li>Ideal for:<ul> <li>serving images/files to a browser</li> <li>storing files for distributed access (eg. to support an installation)</li> <li>streaming video/audio</li> <li>storing backups</li> <li>storing data for analysis</li> <li>storing up to 8TB of data for VMs</li> </ul> </li> <li>Access tier can be set at blob level, during or after upload. Lifecycle Management on the Storage Account allows you to configure rules to transition blobs between tiers.</li> <li>Blob storage comes in three forms:<ul> <li>Page Blob: used to hold random access (unordered) files, such as VM disks (managed disks are stored in Microsoft storage accounts)</li> <li>Block Blob: used to hold objects that are ordered, consecutive and non-random, such as backups</li> <li>Append Blob: used for information that is added in consecutive order, such as log files</li> </ul> </li> </ul> <p>Blob Storage setup has the following configuration:</p> <ul> <li>Container options<ul> <li>Name: must be unique within the Storage Account, 3-63 characters (a-z0-9-) long</li> <li>Access Level: <ul> <li>Private (default): deny anonymous access to the container and blobs</li> <li>Blob: allow anonymous read access to blobs only</li> <li>Container: allow anonymous read access to container and blobs</li> </ul> </li> </ul> </li> <li>Blob Types and Upload options<ul> <li>Block Blob: consists of blocks of data assembled to make a blob</li> <li>Append Blob: optimised for append operations. Suitable for log files</li> <li>Page Blob: up to 8 TB in size, optimised for frequent read/write operations. Suitable for VM disks. </li> <li>Block Blobs are the default. Once a blob is created, you cannot change its type.</li> </ul> </li> <li>Access Tier<ul> <li>Hot - optimised for data that is accessed frequently: highest storage costs, lowest access costs</li> <li>Cool - accessed infrequently and stored for at least 30 days: higher access costs and lower storage costs than hot tier. Suitable for data that is accessed infrequently but needs to be available immeadiately, e.g. backup and DR files</li> <li>Archive - rarely accessed, stored for at least 180 days: lowest storage cost and highest access cost. Data is stored off-line and will take several hours before the first byte is avaiable. If data is removed before 180 days, it will incur early deletion charges. </li> <li>Premium Blob Storage: uses SSDs for I/O intensive workloads</li> </ul> </li> <li>Lifecycle rules<ul> <li>transition blobs to cooler storage and/or deletion</li> <li>define daily rule-based conditions to run at the Storage Account level</li> <li>apply rule-based conditions to containers or sets of blobs</li> </ul> </li> <li>Replication<ul> <li>define a replication policy from one storage account to another</li> <li>both source and destination must be in the Hot or Cool tier - but they don't need to be in the same tier</li> <li>requires blob versioning enabled on source and destination</li> <li>doesn't support replication for blob snapshots</li> <li>replication can be used to reduce access latency, by keeping copies closer to end-users</li> <li>can support compute workloads by allowing the same set of blobs to be accessed in different regions</li> </ul> </li> </ul> <p>There are several options for uploading Blob Data:</p> <ul> <li>Storage Explorer</li> <li>AzCopy</li> <li>Azure Data Factory</li> <li>Azure Storage Data Movement library (.Net)</li> <li>blobfuse (linux filesytem support)</li> <li>Azure Data Box Disk</li> <li>Azure Import/Export (using WAImport Tool to copy files to disk)</li> </ul>"},{"location":"azure/storage/#azure-disks","title":"Azure Disks","text":"<p>Persistent block storage for Azure VMs: data disks have a maximum capacity of around 32 TB</p>"},{"location":"azure/storage/#azure-data-lake","title":"Azure Data Lake","text":"<p>Azure Data Lake is the Hadoop Distributed Flie System (HDFS) as a service</p>"},{"location":"azure/storage/#azure-file-storage","title":"Azure File Storage","text":"<p>Azure File Storage provides cloud-hosted file shares. Files can be accessed using SMB or NFS from  on-prem or VMs in Azure. Scripts to connect a file share to a local machine or VM can be downloaded using the 'Connect' link on the file share blade in the Azure Portal. </p> <p>Files are encrypted at rest, and SMB ensures encryption in transit.  Files can be access using a URL or SAS URL token. </p> <ul> <li>Ideal for:<ul> <li>Applications that use file shares</li> <li>Sharing files between VMs or Users</li> </ul> </li> </ul> <p>File Shares are accessed on port 445 - so this needs to be enabled in your firewall. When creating the file share, the 'enable secure transfer' enforces encryption during data  transfers.</p> <p>Azure Files provides capablility to take snapshots of file shares to create a point-in-time,  read-only copy of the file share. Share snapshots are incremental, recording only changes that were made since the last snapshot. Although the snapshots are incremental, you only need the save the most recent snapshot to recover a file or share. To delete a share that has  snapshots, you must first delete all snapshots. </p> <p>Azure File Sync enables you to cache several Azure Files shares on an on-prem Windows server or  cloud VM. You can then access the files via the server cache using the protocols supported on  the server. Azure File Sync allows you to cache the shares on an many servers as required.  Cloud tiering is a feature that allows you to define policies to control which files are stored locally and which are kept in Azure Files. Files that are not stored locally are  replaced with a pointer (URL to the file in Azure). Tiered files are retrieved on-demand from Azure as needed. Azure File Sync can be used to:</p> <ul> <li>Provide write access to the same data across Windows Servers and Azure Files</li> <li>Store backup files</li> <li>Provide DR storage</li> <li>Archiving files to Azure with Cloud Sync</li> </ul> <p>Azure File Sync comprises six main components:</p> <ol> <li>Storage Sync Service: forms sync relationships with Storage Accounts</li> <li>Sync Group: defines the endpoints (local:Azure) for the sync</li> <li>Registered Server: a local server that is trusted by the Sync Service</li> <li>Azure File Sync Agent: Package installed on the local server to enable synchronisation with Azure</li> <li>Server Endpoint: represents a particular location on the local server that is to be synchronised with Azure Files</li> <li>Cloud Endpoint: an Azure Files share that the server endpoint is to be synchronised to. </li> </ol> <p>The Azure Files share can be a member of only one Sync Group and only one Cloud Endpoint. A server can be registered with only one Cloud Sync Service resource at a time.</p> <p>Deploy Azure File Sync</p> <ol> <li>Deploy the Storage Sync Service on Azure</li> <li>Install the Azure File Sync Agent on your local Windows Server</li> <li>Register the local server to the Storage Sync Service (establishes the trust relationship)</li> </ol>"},{"location":"azure/storage/#azure-queue-storage","title":"Azure Queue Storage","text":"<p>Azure Queue Storage is used to store and retrieve messages. Queue messages can be up to 64KB and each queue can hold millions of messages. Queues can be used to store lists of messages  to be processed asynchronously.</p> <p>You can use a queue in conjuction with Azure Functions to respond to a user action. For example,  every time a new file is added to a storage container, you can write a message to the queue.  An Azure Function can then retrieve the message and run a custom action. </p>"},{"location":"azure/storage/#azure-table-storage","title":"Azure Table Storage","text":"<p>Azure Table Storage is now part of Azure Cosmos DB, a fully-managed, NoSQL database service. In  addition to the Azure Table Storage service, there's an Azure Cosmos DB Table API offering  throughput-optimised tables, global distribution and automatic secondary indexes. </p> <p>Table Storage is ideal for storing structured or relational data. </p>"},{"location":"azure/storage/#azure-importexport-service","title":"Azure Import/Export Service","text":"<p>Used to securely export data from on-prem to Azure Storage by sending  disks to an Azure Data Centre or to import data from Azure Storage to  disks which are then sent to on-prem. </p>"},{"location":"azure/storage/#create-an-import-job","title":"Create an Import Job:","text":"<ul> <li>Create a Storage Account in Azure for the Import Job</li> <li>Attach the disks to be used to an on-prem server</li> <li>Install the WAImportExport tool on the server</li> <li>Run WAImportExport tool to copy the data to the disks</li> <li>Create an Import Job in Azure Portal </li> <li>Ship the disks to the Azure Data Centre</li> </ul>"},{"location":"azure/storage/#create-an-export-job","title":"Create an Export Job:","text":"<ul> <li>Determine number of disks required to the Export Job</li> <li>Create an Export Job in Azure Portal</li> <li>Ship the disks to the Data Centre</li> <li>Disks are returned with the required data </li> </ul>"},{"location":"azure/storage/#data-box","title":"Data Box","text":"<ul> <li>Data Box is a physical storage device with a maximum capacity of 80TB</li> <li>Use device to copy data to or from Azure Storage</li> <li>Data Box can be used for: <ul> <li>One-time migration - for large (&gt;40TB) datasets</li> <li>Initial bulk transfer - followed by incremental loads over the network or with Data Box Gateway</li> <li>Periodic Uploads - where network connectivity is limited</li> </ul> </li> </ul> <p>Data Box Gateway is a service that allows you to securely transfer large amounts of data to and from Data Box</p> <ul> <li>Data Box Gateway consists of: <ul> <li>a virtual device hosted on-prem</li> <li>a gateway resource on Azure</li> <li>a local Web UI for management and support</li> </ul> </li> <li>Files can be copied to the gateway using SMB or NFS</li> <li>Files written to the Gateway are transferred to Azure Storage</li> <li>Data Box Gateway can be used to:<ul> <li>Cloud Archival - copy 100s TBs of data to Azure Storage</li> <li>Continuous Data Ingestion</li> <li>Incremental Transfers following a Bulk Transfer - incremental loads after an initial Data Box load</li> </ul> </li> </ul>"},{"location":"azure/afc/core_concepts/","title":"Core Concepts","text":"<p>Cloud Computing: Delivery of computing services over the internet</p> <p>Cloud Models</p> <ul> <li>Public Cloud: (Multi-Tenant) services available over the internet and available to anyone who wants to purchase them</li> <li>Private Cloud: (Single-Tenant) computing resources available exclusively to one business or organisation. Resources can be on-prem or hosted by a service provider</li> <li>Hybrid Cloud: combines Public and Private Cloud by allowing resources to be shared across both</li> </ul> <p>Cloud Benefits</p> <ul> <li>High Availability. Availability is the percentage of time a resource is available to service a request. High Availability can be met by failing-over to a replica in the same Region. </li> <li>Scalability: vertically or horizontally to manually handle additional workloads:<ul> <li>vertically: add RAM or CPU to existing resource</li> <li>horizontally: adding more instances of the resource</li> </ul> </li> <li>Elasticity: autoscaling, so that apps always have resources to meet changing demand of current workload</li> <li>Agility: rapid deployment as needs change</li> <li>Geo-Distribution: deploy apps to regions around the globe, to gain best performance for the region</li> <li>Disaster Recovery: backup, data replication and geo-distribution protects data from disasters. DR strategy relies heavily on Recovery Time Objective (RTO) and Recovery Point Objective (RPO). Disaster Recovery can be met by failing-over to a replica in another region. Replication provides for shorter RTOs and RPOs than can be achieved with backups.</li> </ul> <p>Pay-As-You-Go pricing model</p> <ul> <li>lower operating costs</li> <li>run infrastructure more efficiently</li> <li>scale as need changes</li> <li>PAYG is considered Operational Expenditure (OpEx) as opposed to Capital Expenditure (CapEx) for Private Clouds</li> </ul> <p>Cloud Service Models - Shared Responsibility Model</p> <ul> <li>IaaS: provider configures hardware, but customer is responsible for OS and Networking</li> <li>PaaS: provider configures platform, but customer is responsible for deploying applications to the environment. Development Tools, Databases and Business Analytics are PaaS offerings</li> <li>SaaS: provider manages application environment, but customer is responsible for their data, devices and accounts.</li> <li>Serverless: provider manages everything up to and including language and runtime. Customer responsible for code and logic. Serverless model is targetted at event-based solutions</li> </ul> <p>Azure Portal</p> <ul> <li>designed for resilience and availability</li> <li>available in every Azure data centre</li> <li>Continuously updated</li> </ul> <p>Regions and DataCentres</p> <ul> <li>All hardware used in Azure is contained in data centres distriubuted across the globe</li> <li>Datacentres are grouped into Regions</li> <li>Resources are deployed to Regions - no direct access to individual data centres</li> <li>A Region is an area containing at least one, but usually more data centres</li> <li>Datacentres within a region are nearby and connected via a low-latency network</li> <li> <p>Availability Sets are physically separate hardware components within a data centre</p> <ul> <li> <p>Availability sets are used to deploy virtual machines to different fault domains and update domains      <code>az availability-set create -g $RESOURCE_GROUP -n $AV_SET_NAME --platform-fault-domain-count 2 --platform-update-domain-count 2</code></p> </li> <li> <p>Fault domains are physical groupings of resources that share the same power and networking.</p> </li> <li>Update domains are logical groupings used as an update/patching boundary: updates are only applied to one update domain at a time.</li> </ul> </li> <li> <p>Availability Zones are physically separate data centres within an Azure Region</p> <ul> <li>Minimum of three zones per region</li> <li>Each Availability Zone consists of one or more data centres with independant networking, power and cooling</li> <li>Availability Zones within a Region are connected through high-speed, fibre-optic connections</li> <li>Availability Zones act as isolation boundaries</li> <li>Use availability zones to replicate applications and protect against outtages on a particular Availability Zone (local-redundancy or regional-redundancy)</li> <li>Replication across availability zones increases costs by duplicating resources</li> </ul> </li> <li>Zone-redundant services (SQL Database, zone-redundant storage) are automatically deployed across zones</li> <li>Zonal services (VMs, managed disks, IP Address) are deployed to a specific zone, but can be replicated to other zones</li> <li>Non-zonal services are globally available</li> <li>Region Pairs<ul> <li>Each Azure Region is automatically paired with another region in the same geography (UK, Europe, US), at least 300 miles away</li> <li>A Geography can have multiple Regions and defines a data residency and compliance boundary</li> <li>Region pairs are physically connected</li> <li>Allows for failover between regions (geo-redundancy), and protects from regional disasters</li> <li>Azure Site Recovery - protects against Region failures and ensures data residency boundaries</li> <li>Replication across regions increases costs by duplicating resources and data transfers between zones. Data transfers within a Region are not billed: data transfers leaving a Region are billed.</li> </ul> </li> <li>Storage Redundancy<ul> <li>LRS - locally-redundant storage: three copies of your data, replicated synchronously within a single physical location in the primary region</li> <li>ZRS - Zone-redundant storage: three copies of your data, replicated synchronously across three availability zones in the primary region</li> <li>GRS - geo-redundant storage: three copies of your data, replicated synchronously within a single physical location in the primary region. Also provides three copies of your data replicated asynchronously to a single physical location in the secondary region</li> <li>GRS - geo-redundant storage maintains three copies of the data in both regions, but data is only accessible from the primary copy</li> <li>RA-GRS allows read-access from both regions simultaneously</li> </ul> </li> <li>Edge Zones - allows data processing and code to be executed closer to the end user. Not located in Azure Region data centres<ul> <li>Azure Edge Zones - Azure public cloud resources within Microsofts Point-Of-Presence edge-locations. Part of Microsoft's global network</li> <li>Azure Edge Zones with carrier - Azure public cloud resources within Carriers data centre locations. Part of the Carriers global network</li> <li>Azure Private Edge Zones - Azure Stack (private cloud) resources within the customers location or 3rd-party private network. Not part of the Microsoft or Carrier network. Can be connected to Microsoft data centre regions using VPN or ExpressRoute</li> </ul> </li> <li>A proximity placement group is a logical grouping used to make sure that Azure compute resources are physically located close to each other<ul> <li>Useful for workloads where low latency is a requirement</li> <li>Do not provide high availability</li> <li>Need to be created before the resource is added to it: <code>az ppg create</code></li> </ul> </li> </ul> <p>Azure Resource Manager</p> <ul> <li>Azure Resource Manager is the deployment and management service for Azure</li> <li>Resource Manager recieves requests from Azure tools (Portal, PowerShell, CLI), APIs and SDKs</li> <li>Resource Manager authenticates and authorises the request, and then sends the request to the Azure service which takes the requested action</li> <li>ARM templates are JSON files that allow you to manage resources declaratively</li> <li>ARM templates allow you to deploy, manage and monitor resources as a group. Templates can be re-deployed through the development lifecycle</li> <li>ARM templates can define dependancies between resources and apply access control through RBAC</li> </ul> <p>Subscriptions</p> <ul> <li>A subscription is required to use Azure</li> <li>Subscriptions are linked to an account. An account may have one or more subscriptions</li> <li> <p>A subscription can be used to define either a billing boundary or access-control boundary</p> <ul> <li>Azure generates billing at the subscription level</li> <li>Access-management policies are assigned at the subscription level</li> </ul> </li> <li> <p>Subscriptions can be used to separate</p> <ul> <li>Environments</li> <li>Departments</li> <li>Billing</li> </ul> </li> <li> <p>Additional subscriptions can be used to overcome subscriptions limits: e.g. 10 ExpressRoute connections per subscription</p> </li> <li>Billing Profiles can be used to create invoice sections within the same billing account</li> </ul> <p>Management Groups</p> <ul> <li>Used to manage access, policies and compliance for multiple subscriptions</li> <li>Subscriptions within a management group automatically inherit the governance conditions of the parent Management Groups. These conditions can not be overwritten at lower levels in the hierarchy</li> <li>Subscriptions within a management group must trust the same Azure AD tenant</li> <li>10,000 Management Groups limit in a single directory</li> <li>A Management Group tree supports up to six levels of depth</li> <li>Management Groups and Subscriptions can have only one direct parent</li> <li>Management Groups and Subscriptions exist in a single hierarchy in each directory</li> </ul> <p>Azure Marketplace</p> <ul> <li>searchable catalogue of services optimised and certified to run on Azure</li> <li>includes access to solutions from independant providers and MS partners</li> </ul> <p>Azure Free Account</p> <ul> <li>free access for 12 months</li> <li>credit to spend for first 30 days</li> <li>25 always-free products</li> <li>requires credit card and MS or Github account</li> </ul> <p>Azure Free Student Account</p> <ul> <li>free access for 12 months</li> <li>$100 credit to spend for first 12 monts</li> <li>free access to certain software developer tools</li> </ul>"},{"location":"azure/afc/core_management_tools/","title":"Core Management Tools","text":""},{"location":"azure/afc/core_management_tools/#resource-management","title":"Resource Management","text":""},{"location":"azure/afc/core_management_tools/#resource-groups","title":"Resource Groups","text":"<ul> <li>Put resources with the same lifecycle into a single resource group. Deleting a resource group deletes all the resources within it</li> <li>A resource group is logical collection of Azure resources. Use tags to further organise resources</li> <li>Once created, a resource group can not be renamed</li> <li>Resources can be moved from one resource group to another. When moving resource between resource groups the resource groups are locked for write and delete operations until the move completes</li> <li>Not all resources support being moved</li> <li>The resources in a resource group can reside in different regions</li> <li>The location for a resource group determines where the resource group metadata resides</li> <li>Resources can interact with resources in other resource groups</li> <li>Resource groups can be used as a scope for access permissions</li> </ul>"},{"location":"azure/afc/core_management_tools/#azure-resource-manager","title":"Azure Resource Manager","text":"<ul> <li>Provides a consistent management layer for actions from PowerShell, CLI, Portal, REST clients and SDKs</li> <li>The Azure Resource Manager API passes requests to the Azure Resource Manager service which authenticates and authorises the request</li> <li>The Azure Resource Manager service then passes the request to the appropriate resource providers</li> <li>Azure Resource Manager locks can be used to prevent changes or deletion of resources and groups</li> </ul> <p>Resource providers offer a set of resources and operations for working with Azure services. The  resource provider defines a set of REST operations. Common Resource providers include:</p> <ol> <li>Microsoft.Compute</li> <li>Microsoft.Storage</li> <li>Microsoft.Web</li> <li>Microsoft.KeyVault</li> </ol> <p>Before using a resource provider it must be registered in your Azure subscription. Some  resource providers are registered by default for each subscription. When you create a resource through the Portal, the resource provider is automatically registered. Resources defined  in ARM or Bicep templates also result in automatic registration of the corresponding  resource provider. If the template results in the creation of services from that are not  explicitly defined in the template, then the resource providers may need to be registered  manually beforehand. </p> <p>Registering a resource provider requires the <code>/register/action</code> permission,  which is included in the 'Contributor' and 'Owner' roles. You can view the current  resource provider registrations in the 'Settings' menu for a subscription. </p> <p>Use Resource Explorer in the Portal to see the list of available Providers. Each provider will have attributes detailing the resource types the service provides and the locations, api versions and capabilities associated to the resource type. </p> <p>PowerShell provides the <code>Get-AzResourceProvider</code> and <code>Register-AzResourceProvider</code> cmdlets for working with resource providers:</p> <pre><code>Get-AzResourceProvider -ListAvailable | Select-Object ProviderNamespace, RegistrationState\n\nGet-AzResourceProvider -ListAvailable | Where-Object RegistrationState -eq \"Registered\" | Select-Object ProviderNamespace, RegistrationState | Sort-Object ProviderNamespace\n\nRegister-AzResourceProvider -ProviderNamespace Microsoft.Batch\n\nGet-AzResourceProvider -ProviderNamespace Microsoft.Batch\n\n(Get-AzResourceProvider -ProviderNamespace Microsoft.Batch).ResourceTypes.ResourceTypeName\n</code></pre> <p>Azure CLI provides this functionality in the <code>az provider</code> command:</p> <pre><code>az provider list --query \"[].{Provider:namespace, Status:registrationState}\" --out table\n\naz provider list --query \"sort_by([?registrationState=='Registered'].{Provider:namespace, Status:registrationState}, &amp;Provider)\" --out table\n\naz provider register --namespace Microsoft.Batch\n\naz provider show --namespace Microsoft.Batch\n\naz provider show --namespace Microsoft.Batch --query \"resourceTypes[*].resourceType\" --out table\n</code></pre>"},{"location":"azure/afc/core_management_tools/#azure-portal","title":"Azure Portal","text":"<ul> <li>Azure Portal Documentation</li> <li>Azure Portal Sandbox</li> </ul> <p>Azure Mobile App</p> <p>Azure PowerShell</p> <ul> <li>Available on Windows, Linux and Mac</li> <li>Comes as a module to add to Windows Powershell or Powershell Core </li> <li>Az module is open-source and available on github</li> <li>Azure Powershell Reference</li> <li>PowerShell Sandbox</li> </ul> <p>Azure CLI</p> <ul> <li>Available on Windows, Linux and Mac</li> <li>Good for one-off management, administrative or reporting tasks</li> <li>Use <code>az find &lt;search_string&gt;</code> to get help</li> <li>Azure CLI Reference</li> <li>Azure CLI Sandbox</li> </ul> <p>Cloud Shell</p> <ul> <li>Use PowerShell or Azure CLI from a Web browser or mobile phone</li> <li>Run either Bash or Powershell environment on an Ubuntu Linux container</li> <li>Terraform also available from CloudShell</li> <li>Runs on a temporary host on a per-session, per-user basis</li> <li>Times out after 20 minutes of inactivity</li> <li>Persists $HOME in a 5GB image held in the file share created the first time you run Cloud Shell</li> </ul> <p>ARM Templates</p> <ul> <li>Define infrastructure for repeatable deployments using declarative templates</li> <li>You can deploy, update or delete resources using a template</li> <li>Templates can be re-used in different environments</li> <li>ARM templates allow you to work with resources in your solution as a group</li> <li>Resource deployments are created in parallel</li> <li>Validation stage checks for dependancies and allows for easier rollback of deployments</li> <li>Use RBAC to define access control to all services</li> <li>ARM templates are idempotent</li> <li>Can be used to trigger PowerShell or CLI scripts</li> <li>ARM Sandbox</li> </ul>"},{"location":"azure/afc/core_management_tools/#resource-monitoring","title":"Resource Monitoring","text":"<p>Azure Advisor</p> <ul> <li>Included no-cost service</li> <li>Evaluates your Azure resources and offers recommendations:<ul> <li>reliability</li> <li>security</li> <li>performance <li>operational excellence</li> <li>reduce costs</li> <li>Recommendations available in the Portal, through the API and via notifications</li> <li>Recommendations can also be downloaded from the Portal as CSV or PDF</li> <p>Azure Monitor</p> <ul> <li>A service for collecting, analysing and acting on telemetry from both cloud and on-premises resources</li> <li>Used to maximise availability and performance of applications and services</li> <li>Included no-cost service</li> <li>Data collected in central repositories</li> <li>Data can be viewed in Azure Monitor Dashboard or viewed in PowerBi or Kusto queries</li> <li>Can be used to setup custom alerts</li> <li>Metrics are collected for a resource automatically as soon as the resource is created</li> <li>VMs and other compute resources require an agent (Azure Monitor Agent) to collect metrics from the guest OS</li> <li>Azure Monitor Logs collects log and performance data from configured resources. A Log Analytics workspace needs to be configured and then the different sources need to be configured to send their data to the workspace</li> </ul> <p>Azure Service Health</p> <ul> <li>Free service providing alerts on:<ul> <li>Service Issues</li> <li>Planned Maintenance</li> <li>Health Advisories</li> <li>Root Cause Analysis for outtages</li> </ul> </li> <li>More detailed and tailored information than Azure Status</li> </ul>"},{"location":"azure/afc/core_services/","title":"Core Services","text":"<p>Moved to Azure Compute</p>"},{"location":"azure/afc/core_services/#networking","title":"Networking","text":"<p>Moved to Azure Networking</p>"},{"location":"azure/afc/core_services/#storage","title":"Storage","text":"<p>Moved to Azure Storage</p>"},{"location":"azure/afc/core_services/#mobile","title":"Mobile","text":"<p>Create backend services for iOS, Android and Windows apps</p>"},{"location":"azure/afc/core_services/#databases","title":"Databases","text":"<p>Azure Cosmos DB</p> <ul> <li>globally distributed and horizontally scalable table storage</li> <li>99.99% SLA for reads and writes</li> <li>row-level access control and data encryption in transit and at rest</li> <li>multi-model support</li> <li>supports schema-less data</li> <li>stores data in Atom-Record-Sequence (ARS)</li> <li>data is then projected as an API. APIs include:<ul> <li>SQL</li> <li>MongoDB</li> <li>Cassandra</li> <li>Tables</li> <li>Gremlin</li> </ul> </li> </ul> <p>Azure SQL Database</p> <ul> <li>Fully-managed PaaS offering - including upgrades, patching, backups and monitoring</li> <li>Based on latest version of Microsoft SQL Server database engine</li> <li>99.99% availability</li> <li>Single database or elastic pools available</li> <li>Priced by vCore and Data Transaction Units (DTU)</li> <li>Supports basic, general-purpose, business-critical and hyperscale tiers</li> <li>Scale-up and scale-down between tiers is available</li> <li>No horizontal scaling</li> <li>support for relational and non-relational data including: graphs, JSON, spatial and XML data</li> <li>Use Azure Database Migration Service to migrate on-prem SQL Servers to Azure SQL Database</li> <li>Use the Microsoft Database Migration Assistant to generate a report to guide the migration process</li> <li>Only provides a single collation option</li> </ul> <p>Azure Database for MySQL</p> <ul> <li>Based on MySQL CE, versions 5.6, 5.7 and 8.0</li> <li>99.99% availability</li> <li>Built-in security, fault-tolerance and data protection</li> <li>Automatic backups: Point-in-time restore up to 35 days previous</li> <li>Use Azure Database Migration Service to migrate on-prem MySQL Servers to Azure Database for MySQL</li> <li>Supports Single-Server, Flexible-Server and Hyperscale (Citus)</li> <li>Hyperscale (Citus) provides horizontal scaling</li> </ul> <p>Azure Database for PostgreSQL</p> <ul> <li>Based on the open-source PostgreSQL database engine</li> <li>Built-in high-availability</li> <li>Various service tiers available</li> <li>Automatic backups: Point-in-time restore up to 35 days previous</li> <li>Encryption at rest and in-transit</li> <li>Dynamic scalability</li> <li>Available in Single Server or Hyperscale (Citus) editions</li> <li>Hyperscale offers horizontal scaling across multiple machines using sharding</li> </ul> <p>SQL Server Managed Instance</p> <ul> <li>PaaS offering, similar to Azure SQL Database but with support for more features: <ul> <li>see feature comparison</li> <li>additional collations</li> <li>SQL Server Agent available</li> <li>Supports more functions, procedures, statements</li> <li>Time Zone choice</li> </ul> </li> <li>Good choice for lift-and-shift migrations</li> <li>Full SQL Server Enterprise Edition database engine</li> <li>Supports SQL Authentication or AAD Authentication</li> <li>Suports single-instance or instance pools</li> </ul> <p>Azure Database Migration Service - manage migrations to cloud</p> <p>Azure Cache for Redis</p> <p>Azure Database for MariaDB</p>"},{"location":"azure/afc/core_services/#web","title":"Web","text":"<p>Azure App Service - PaaS offering to deploy front-end web applications</p> <ul> <li>App Service Plan determines how much resource is allocated</li> <li>App Service Plan defines the number of VMs your App Service will use, the type and size of the VMs. You can also define the region, OS and application stack to use</li> <li>An App Service Plan can be used to host multiple App Services</li> <li>App Service Plans are available in a number of tiers:<ul> <li>Free and Shared: intended for testing and development. No autoscaling, hybrid connections or VNet connections</li> <li>Basic: intended for low-traffic usage. No autoscale or traffic management features. Built-in load balancing across instances. Custom domains and hybrid connectivity are supported</li> <li>Standard: intended for production workloads. Supports autoscaling, custom domains, hybrid and VNet connectivity</li> <li>Premium: intended for higher scale and performance workloads. Support for all features in Standard plus private endpoints</li> <li>Isolated: mission critical workloads running in a private, dedicated environment</li> </ul> </li> <li>App Service Plans are billed, even when no apps are running. This is because the VMs are still running</li> <li>Built-in load-balancer and traffic manager</li> <li>Supports Web Apps, API Apps, WebJobs and Mobile Apps</li> <li>Scale-out and Scale-Up are available for Basic and above service plans</li> </ul> <p>Azure Notification Hubs - push notifications to any platform from any back end</p> <p>Azure API Managment - publish APIs</p> <p>Azure Cognitive Search - search as a service</p> <p>Web Apps feature of Azure App Service</p> <p>Azure SignalR Service</p>"},{"location":"azure/afc/core_services/#iot","title":"IoT","text":"<p>Devices equipped with sensors can send data over the internet to an Azure endpoint. Message data can then be analysed. Additionally software updates can be sent to devices from Azure.</p> <ul> <li>IoT Hub - build your own IoT solutions. IoT PaaS solution. Messaging Hub for secure, bi-directional communications for devices, to monitor, collect data, update software and control the device.</li> <li>IoT Central - consume IoT solutions. IoT SaaS solution. Builds on IoT Hub by providing a dashboard to connect, monitor and manage IoT devices. Starter templates are available for various common scenarios. Device developers need to create code to run on the devices that match the chosen template.</li> <li>Azure Sphere is an end-to-end IoT solution covering everything from the hardware used to message handling. There are three key components:<ul> <li>Azure Sphere micro-controller unit, responsible for processing the signals from the sensors</li> <li>Customised Linux OS used to run the vendors software</li> <li>Azure Sphere Security Service (AS3) for authenticating to Azure and establishing a secure connection</li> </ul> </li> <li>Choose Azure Sphere if device security is critical</li> </ul>"},{"location":"azure/afc/core_services/#big-data","title":"Big Data","text":"<p>Traditional ETL is not appropriate to handle the volume, velocity, complexity and format of Big Data. ELT is preferred. Typical components of a Big Data architecture are:</p> <ul> <li>Data Sources: databases, log files, file stores, IoT, social media</li> <li>Data Storage: Data Lakes and Blob Storage</li> <li>Real-Time Message ingestion: Azure Event Hubs, Azure IoT Hubs, Kafka</li> <li>Batch Processing: HDInsight and Databricks</li> <li>Stream Processing: Azure Stream Analytics and HDInsight</li> <li>Analytical Data Store: Synapse Analytics, SQL Data Warehouse, HDInsight</li> <li>Analysis and Reporting: Synpase Analytics, Azure Analysis Services, PowerBi, Excel</li> <li>Orchestration: Data Factory</li> </ul> <p>Azure Synapse Analytics</p> <ul> <li>PaaS solution</li> <li>big-data analytics service</li> <li>fully managed data warehouse</li> <li>rebranded Azure SQL Data Warehouse</li> <li>built on the Massively Parallel Processing (MPP) technology</li> <li>supports Apache Spark as a fully-managed service</li> <li>Azure Synapse Studio provides a Web-Frontend to manage and interact with data</li> <li>Typically used to prepare data</li> </ul> <p>Azure HDInsight</p> <ul> <li>open-source, enterprise-level, fully-managed, big-data analytics service</li> <li>Uses Hadoop framework for distributed processing and analysing big datasets through clusters</li> <li>also supports a variety of other cluster types:<ul> <li>Apache Spark</li> <li>Apache Hadoop</li> <li>Apache Kafka</li> <li>Apache HBase</li> <li>Apache Storm</li> <li>Machine Learning Services</li> </ul> </li> <li>Supports ETL, data warehousing, machine learning and IoT</li> <li>Typically used to prepare data</li> </ul> <p>Azure Databricks</p> <ul> <li>Apache Spark PaaS service</li> <li>Databricks runs on fully managed Spark clusters</li> <li>Databricks workspace is a web-based frontend to manage and interact with data</li> <li>data analytics and AI solution</li> <li>Typically used to prepare and train datasets</li> <li>Apache Spark environment supports:<ul> <li>Python, including scikit-learn, TensorFlow, PyTorch</li> <li>Scala</li> <li>R</li> <li>Java</li> <li>SQL</li> </ul> </li> </ul> <p>Azure Data Lake Analytics</p> <ul> <li>on-demand analytics job-service</li> </ul>"},{"location":"azure/afc/core_services/#ai","title":"AI","text":"<ul> <li>Artificial Intelligence<ul> <li>The ability of computers to imitate intelligent human behaviour</li> </ul> </li> <li>Machine Learning<ul> <li>The ability for computers to improve at tasks through experience (learning). This is known as training and will produce a model that can be deployed and is said to be 'trained'. ML is based on algorithms whose output improves over time as they are fed more data (training datasets).</li> </ul> </li> <li>Deep Learning<ul> <li>The ability for a computer to train itself to perform a task. Based on multi-layered neural networks and is a subset of ML</li> </ul> </li> </ul> <p>Azure Machine Learning </p> <ul> <li>A platform for making predictions. With Azure Machine Learning you can:<ul> <li>Obtain data</li> <li>Define how to handle missing or bad data</li> <li>Split data into training and test datasets</li> <li>Deliver data to the training process</li> <li>Train predictive models</li> <li>Create pipelines to schedule compute-intensive experiments to score the predictive models</li> <li>Deploy the best-performing algorithm as an API to an endpoint to be consumed in real-time by other applications</li> </ul> </li> <li>Azure Machine Learning is recommended when you need complete control over the design and training of your algorithm using your own data</li> </ul> <p>Azure ML Studio - collaborative visual workspace</p> <p>Cognitive Services</p> <ul> <li>Pre-built machine learing models, including:<ul> <li>Vision - image processing</li> <li>Speech - text-to-speech and speech-to-text</li> <li>Translator</li> <li>Personaliser - predict user behaviour, offer personalised recommendations</li> <li>Decision support</li> <li>Natural Language Processing</li> </ul> </li> <li>Accessed by developers via the Cognitive Services API</li> </ul> <p>Azure Bot Service and Bot Framework - create virtual agents that understand and reply to questions. Used for automating repetitive tasks. May rely on other Cognitive services. Web App Bot templates can be used to get started. </p>"},{"location":"azure/afc/core_services/#devops","title":"DevOps","text":"<p>Azure DevOps</p> <ul> <li>A suite of services for the software development lifecylcle. Services include:<ul> <li>Azure Repos - source code repositories</li> <li>Azure Boards - Agile project managment suite</li> <li>Azure Pipelines - a CI/CD pipeline automation tool</li> <li>Azure Artifacts - a repository for artifacts that can be fed into CI/CD and testing pipelines</li> <li>Azure Test Plans - automated test tool</li> </ul> </li> <li>DevOps offers a more granular permissions model than GitHub</li> <li>DevOps provides a more sophisticated level of project mananagement and reporting tools</li> </ul> <p>GitHub and GitHub Actions</p> <ul> <li>Git is a decentralised source-code managment tool</li> <li>GitHub is a hosted version of Git. GitHub offers additional functionality:<ul> <li>source code review via comments and questions</li> <li>project managment tools</li> <li>issue tracking</li> <li>CI/CD automation</li> <li>Wiki</li> <li>Run from on the Cloud or on-prem</li> </ul> </li> <li>GitHub supports workflow automation</li> <li>GitHub offers similar tools to DevOps, and is more trusted in the open-source community. DevOps is aimed at enterprise-development, with stronger support for project managment and planning</li> <li>GitHub has limited permissions model over each repository</li> </ul> <p>Azure DevTest Labs</p> <ul> <li>Create on-demand environments to test/deploy applications from deployment pipeline</li> <li>Can be used to deploy any Azure Resource that supports ARM template deployment</li> <li>Automates provisioning and deprovisioning of environments</li> </ul>"},{"location":"azure/afc/costs/","title":"Cost Management and SLAs","text":""},{"location":"azure/afc/costs/#cost-management","title":"Cost Management","text":"<ul> <li>TCO Calculator<ul> <li>Define workloads - on-prem infrastructure</li> <li>Adjust Assumptions - Software Assurance discounts, estimated on-prem costs</li> <li>View the Report</li> </ul> </li> <li>Purchase Azure Services<ul> <li>Enterprise Agreement - commitment to spend a pre-determined amount over a 3 year period</li> <li>Web Direct - monthly billing by credit card or invoice</li> <li>Cloud Solution Provider - bills come from CSP</li> </ul> </li> <li>Cost Factors<ul> <li>Resource Type - each resource has a billing meter and unit cost. The unit cost multiplied by the usage quantity will give the resource cost. The unit of measure multiplied by the usage quantity will give the actual usage.</li> <li>Resource Usage - deallocating versus deleting</li> <li>Subscription Type</li> <li>Marketplace Resources - prices set by vendor</li> <li>Location - resources may have different prices in different regions. Network traffic can be reduced by choosing the closest region to your users</li> <li>Zones - outbound data-transfer pricing based on zones</li> <li>Dev/Test subscriptions - subscription with special pricing for Dev/Test workloads</li> <li>Some resources are free - have no billing meter or unit cost:<ul> <li>User account and groups</li> <li>Resource groups</li> <li>Virtual Networks</li> <li>Virtual Network Peering</li> <li>Network Interfaces</li> <li>Network Security groups</li> <li>Availability Sets</li> </ul> </li> </ul> </li> <li>Azure Pricing Calculator allows you to calculate the cost of a solution based on the resources and usage</li> <li>Azure Advisor - identifies unused or underutilised resources</li> <li>Spending Limits</li> <li>Azure Reservations - pre-pay for a resource for discounted pricing</li> <li>Offers - monitor and switch to latest Azure customer and subscription offers</li> <li>Cost Management and Billing - Free service to manage costs and billing<ul> <li>view and download invoices</li> <li>view payment methods</li> <li>make payments</li> <li>perform cost analysis</li> <li>set alerts</li> <li>create budgets</li> <li>receive recommendations</li> </ul> </li> <li>Tags - can be used to organise billing data</li> <li>Resize VMs</li> <li>Deallocate VMs out of hours</li> <li>Delete unused resources</li> <li>Prefer PaaS to IaaS services - usually cheaper</li> <li>Azure Hybrid Benefit - Windows Server and SQL Server licences purchased with Software Assurance can be re-used on VMs in Azure</li> <li>a billing zone is a geographical grouping of Azure Regions used to determine billing based on data transfers. Billing applies to both incoming and outgoing data and varies by billing zone. Data transfers between billing zones and regions are billed</li> </ul>"},{"location":"azure/afc/costs/#service-level-agreements","title":"Service Level Agreements","text":"<ul> <li>Formal agreement between service provider and customer</li> <li>Accessible at Service Level Agreements</li> <li>SLA details focus on uptime, but can also include latency</li> <li>Service credits are available if SLAs are not met</li> <li>Claims for service credits must be submitted by the end of the month following the outtage</li> <li>No SLAs for free services</li> <li>SLAs and Downtime:<ul> <li>99.9% == 43 minutes and 49 seconds</li> <li>99.95% == 21 minutes and 54 seconds</li> <li>99.99% == 4 minutes and 22 seconds</li> <li>99.999% == 26 seconds</li> </ul> </li> <li>Multiply the SLAs for each service in your application to calculate the application SLA value</li> <li>Improve SLAs by:<ul> <li>Using services that have an SLA or improve the existing SLA</li> <li>Adding redundant resources - duplication across regions to create redundancy and reduce overall downtime</li> <li>Adding availability solutions - availability zones have different schedules for maintenance, and therefore overall downtime decreases</li> </ul> </li> </ul>"},{"location":"azure/afc/costs/#preview-features","title":"Preview Features","text":"<ul> <li>Preview features in Azure do not come with a warranty or SLA</li> <li>Service Lifecycle stages are:<ul> <li>Development - not available to the public</li> <li>Private Preview - available to selected audience</li> <li>Public Preview - available to all customers</li> <li>General Availability - available to all customers</li> </ul> </li> <li>Azure Portal Previewi</li> <li>Azure Updates</li> </ul>"},{"location":"azure/afc/governance/","title":"Identity and Governance","text":""},{"location":"azure/afc/governance/#azure-ad","title":"Azure AD","text":"<p>With Cloud Computing and BYOD, identity is now the primary security boundary. Identity is  establish by the exchange of credentials to establish the identity of an agent  (authentication). Authorisation is about establishing the levels of access to be granted to an authenticated agent.</p> <ul> <li>Azure AD is a cloud-based identity and access management service (SaaS)</li> <li>AAD can be used to grant access to Azure Portal, Microsoft 365 and thousands of other SaaS applications</li> <li>A Tenant is a representation of an organisation. Each Microsoft 365, Office 365, Azure and Dynamics CRM tenant is automatically an Azure tenant</li> <li>An instance of Azure AD is created for each tenant</li> <li>Additional tenants can be created from the AAD Overview page</li> <li>Custom domain names can be added in addtion to the default <code>$ORG_NAME.onmicrosoft.com</code>. The custom domain should be registered with a domain name registrar</li> <li> <p>Azure AD is available with different licences:</p> <ul> <li>AAD Free<ul> <li>user and group management</li> <li>on-prem directory synchronisation</li> <li>basic reports</li> <li>self-service password change for cloud users</li> <li>SSO across Azure, M365 and many popular SaaS apps</li> <li>Included with any Microsoft Online business service</li> </ul> </li> <li>AAD Premium P1<ul> <li>paid licence built on top of existing free directory</li> <li>create custom RBAC roles</li> <li>grant hybrid users access to both on-prem and cloud resources</li> <li>dynamic groups</li> <li>self-service group management</li> <li>Microsoft Identity Manager</li> <li>Cloud write-back capabilities allows self-service password reset for your on-prem users</li> </ul> </li> <li>AAD Premium P2<ul> <li>paid licence built on top of AAD P1</li> <li>AAD Identity Protection:<ul> <li>risk-based Conditional Access</li> <li>Privileged Identity Management</li> <li>Just-In-Time access</li> </ul> </li> </ul> </li> <li>\"Pay as you go\" feature licences. Additional features such as Business-to-Customer (B2C) provide IAM for customer-facing apps</li> </ul> </li> <li> <p>The AAD Architecture consists of </p> <ul> <li>A primary replica that receives all write operations for the partition it belongs to. The write operation is immeadiately replicated to a secondary replica in a different data centre before returning success to the caller</li> <li>Secondary replicas are used to service all directory reads. Secondary replicas are distributed across different geographies. Data is replicated asynchronously. Directory reads are serviced from data centres that are closest to the customer</li> </ul> </li> <li>Write scalability is achieved by partitioning the data. Read scalability is achieved by having multiple secondary replicas</li> <li>High availability is guaranteed by the services ability to shift traffic across multiple geographically distributed data centres</li> <li>If a failure occurs on a primary replica, writes are immeadiately shifted to another replica. This can result in a 1-2 minute loss of write availability.</li> <li> <p>Azure AD holds objects known as security principles. These can be one of the following types:</p> <ul> <li>Users: can be a member of the tenants Azure AD or a guest. Guests can be B2B (business-to-business) or B2C (third-party identity provider)</li> <li>Application Service Principal: represents the identity of a service or application in Azure</li> <li>Managed Identity Service Principal: used by services or applications in place of a user identity. Can be either system-assigned or user-assigned</li> <li>Device: a device identity</li> </ul> </li> <li> <p>Azure AD provides:</p> <ul> <li>Authentication: includes self-service password resets, MFA, smart-lockout, password blacklist</li> <li>SSO</li> <li>Application Management</li> <li>Device Management</li> </ul> </li> <li> <p>Conditional access allows or denies access to resources based on signals: identity, location, service requested, device, device security status. Conditional Access requires an Azure AD Premium P1 or P2 licence. Signals could result in the logon attempt requiring MFA</p> </li> <li> <p>Azure AD Connect</p> <ul> <li>Free download tool used to synchronise user identities between on-prem Active Directory and Azure AD</li> </ul> </li> </ul>"},{"location":"azure/afc/governance/#governance","title":"Governance","text":"<p>Governance is the process of establishing and enforcing rules and policies</p> <ul> <li>Azure RBAC<ul> <li>Role-Based Access Control</li> <li>Pre-defined roles defining access-levels to resources</li> <li>Uses an allow model: roles define what is allowed</li> <li>Roles assigned to principals within a scope:<ul> <li>management group</li> <li>subscription</li> <li>resource group</li> <li>resource</li> </ul> </li> <li>Enforced against any action on a resource that is initiated through Resource Manager</li> <li>Does not apply at the application or data levels</li> <li>It is good practice to assign users to groups and then assign RBAC roles to the group, rather than assign roles directly to user accounts</li> </ul> </li> <li>Resource Locks<ul> <li>Prevents resources being accidentally deleted or modified</li> <li>Accessible from the 'Locks' pane in the Azure Portal</li> <li>Resource locks are inherited by child objects if these exist</li> <li>Two lock-types available<ul> <li>CanNotDelete - prevents deletion</li> <li>Read - prevents deletion or modification</li> </ul> </li> <li>Read locks can be used on resource groups to prevent additional resources being deployed</li> </ul> </li> <li>Resource Tags<ul> <li>Provide Metadata about a resource</li> <li>Up to 15 tags allowed per resource</li> <li>Tags can be used for:<ul> <li>Resource Management</li> <li>Billing and Cost Management</li> <li>Operations Management (SLAs)</li> <li>Security level</li> <li>Governance and Compliance</li> <li>Visualise complex workloads</li> </ul> </li> <li>Azure Policy can be used to force resources to inherit tags from their resource group</li> </ul> </li> <li>Azure Policy<ul> <li>Used to define policies that control or audit your resources</li> <li>Groups of policies are called initiatives</li> <li>Policy definitions are assigned to a scope:<ul> <li>management group</li> <li>subscription</li> <li>resource group</li> </ul> </li> <li>Policy assignments are inherited by all child resources in the assignment scope</li> <li>Initiatives can be defined in Azure Portal or via command-line tools</li> <li>New policies can be added to an initiative, and these will be applied without the need to re-assign the initiative</li> </ul> </li> <li>Azure Blueprints<ul> <li>Azure Blueprints allow for the definition of controls at the organisational level</li> <li>Each component in a blueprint is termed an artifact</li> <li>Blueprints ochestrate deployments and can include the following types of artifacts:<ul> <li>Role assignments</li> <li>Policy assignments</li> <li>ARM templates</li> <li>Resource groups</li> </ul> </li> <li>Artifacts can have zero or more parameters</li> <li>Parameter values can be set in the definition or during assignment</li> <li>Several Blueprints exist for ISO 27001</li> <li>Blueprints are backed by the globally available Cosmos DB</li> <li>Unlike ARM templates, Blueprint definitions exist natively in Azure. The relationship between the definition and the assignment is preserved</li> <li>When defining a Blueprint, you will also specify the location to save it to: either a Management Group or Subscription</li> <li>Blueprint definitions begin in draft mode. Before they can be assigned they need to be published</li> <li>Publishing involves assigning a Version string (up to 20 characters)</li> <li>Each version of a single Blueprint can be assigned and different versions of the same Blueprint can be assigned to a subscription</li> <li>When a published Blueprint is altered the original version is retained, along with unpublished changes. When the changes are published a new Version is assigned to the Blueprint definition</li> <li>Blueprints can be assigned at the Management Group level but this needs to be done using the REST API</li> <li>Blueprint creation requires the following roles:<ul> <li>Microsoft.Blueprint/blueprints/write - Create a blueprint definition</li> <li>Microsoft.Blueprint/blueprints/artifacts/write - Create artifacts on a blueprint definition</li> <li>Microsoft.Blueprint/blueprints/versions/write - Publish a blueprint</li> </ul> </li> <li>Blueprint deletion requires the following roles:<ul> <li>Microsoft.Blueprint/blueprints/delete</li> <li>Microsoft.Blueprint/blueprints/artifacts/delete</li> <li>Microsoft.Blueprint/blueprints/versions/delete</li> </ul> </li> <li>Blueprint assignment requires the following roles:<ul> <li>Microsoft.Blueprint/blueprintAssignments/write - Assign a blueprint</li> <li>Microsoft.Blueprint/blueprintAssignments/delete - Unassign a blueprint</li> </ul> </li> <li>Built-in Roles also have Blueprint permissions:<ul> <li>Owner - all Blueprint related permissions</li> <li>Contributor - can create and delete Blueprint defintions. No assignment permissions</li> <li>Blueprint Contributor - can manage Blueprint definitions. No assignment permissions</li> <li>Blueprint Operator - can assign published blueprints using a user-assigned managed identity. No permissions to create new Blueprint definitions</li> </ul> </li> <li>Rules for Updating Assignments:<ul> <li>Role Assignments - if the role or assignee has changed, an new role assignment is created. Previously deployed roles are retained</li> <li>Policy Assignments - assignment is updated if the parameters are changed. If the definition is changed a new policy assigment is created and the previous assignment is retained. If the policy assignment is removed from the blueprint, the previous assignment is retained</li> <li>ARM Templates are deployed using a PUT request. Different resources will respond differently (update, replace) to this request</li> </ul> </li> <li>Resource Locking in Blueprint Assignments<ul> <li>Resource locks applied by a Blueprint assignment are only applied to non-extension resources deployed by the blueprint. Existing resources do not have locks added to them</li> <li>There are three locking modes available:<ul> <li>Don't Lock</li> <li>Read Only</li> <li>Do Not Delete</li> </ul> </li> <li>Locking mode is applied during artifact deployment resulting from the assignment. A different locking mode can be set by updating the assignment or removed by deleting the assignment. The locking mode cannot be changed outside of Azure Blueprints</li> <li>To prevent a subscription owner from changing the locking mode of a resource deployed during BP assignment, the assigment can be made at the Management Group level. Then only Owners of the management group can change the assignment. Blueprints assigned to a management group must still specify the subscription that the assignment targets and so is still applied at the subscription level</li> <li>The locking mode will result in resources having one of four states depending on where the locking mode is assigned:<ul> <li>Not Locked - where 'Don't Lock' mode is selected in BP assignment</li> <li>Cannot Edit/Delete - where 'Read Only' mode is set on the resource group. The resource group is Read Only, but Not Locked resources can be added, changed or deleted from the resource group</li> <li>Read Only - where 'Read Only' mode is set on a resource deployed during the assignment</li> <li>Cannot Delete - where 'Do Not Delete' is set on a resource deployed during the assignment. 'Not Locked' resources can be added, moved, changed or deleted from a resource group in 'Cannot Delete' state</li> </ul> </li> <li>Read Only and Delete locks are implemented by Blueprints as RBAC 'deny' assignments. The deny assigments are added by the managed identity of the BP assignment, and can only be removed from the artifact resources by the same managed identity. Therefore the locks can only be removed from within Blueprints. During the assignment, specific principals can be excluded from the 'deny' assignments. Similarly, specific actions (read, write, etc) on specified resources can be excluded from the deny assignment</li> </ul> </li> <li>CAF Foundation<ul> <li>Deploys an Azure Key Vault to host secrets for VMs deployed in the shared services environment</li> <li>Log Analytics is deployed to ensure all actions and services log to a central location</li> <li>Defender Standard deployed</li> <li>Policies<ul> <li>CostCenter Tag applied to Resource Groups</li> <li>Append CostCenter Tag to Resources in Resource Groups</li> <li>Allowed Region for Resources and Resource Groups</li> <li>Allowed Storage Account and VM SKUs</li> <li>Require Network Watcher to be Deployed</li> <li>Require secure transfer encryption on Storage Accounts</li> <li>Deny Resource Types</li> </ul> </li> <li>Policy Initiatives<ul> <li>Enable Monitoring in Microsoft Defender for Cloud (100+ policy definitions)</li> </ul> </li> </ul> </li> </ul> </li> <li>Cloud Adoption Framework<ul> <li>Consists of tools, documentation and proven practice to drive success during adoption of cloud computing on Azure</li> <li>The following steps are included:<ul> <li>Define Your Strategy<ul> <li>Define and Document your Motivations - meet stakeholders and leadership</li> <li>Document Business Outcomes - document your goals</li> <li>Evaluate Financial Considerations - identify the return expected from the investment</li> <li>Understand Technical Considerations</li> </ul> </li> <li>Make a Plan<ul> <li>Digital Estate - inventory of existing assets</li> <li>Initial Organisational Alignment - involve the right people</li> <li>Skills Readiness Plan</li> <li>Cloud Adoption Plan - shared cloud adoption goal</li> </ul> </li> <li>Ready your Organisation<ul> <li>Azure Setup Guide</li> <li>Azure Landing Zone - build subscriptions to support each major area of your business</li> <li>Expand the Landing Zone</li> <li>Best Practices</li> </ul> </li> <li>Adopt the Cloud<ul> <li>Migrate<ul> <li>Migrate your First Workload</li> <li>Migration Scenarios</li> <li>Best Practices</li> <li>Process Improvements</li> </ul> </li> <li>Innovate<ul> <li>Business Value Consensus - ensure innovations add value</li> <li>Azure Innovation Guide - build an MVP (Minimum Viable Product)</li> <li>Best Practices</li> <li>Feedback Loops</li> </ul> </li> </ul> </li> <li>Govern and Manage your Cloud Environments<ul> <li>Govern<ul> <li>Methodology - consider end-state solution </li> <li>Benchmark</li> <li>Initial Governance Foundation - MVP</li> <li>Improve the Initial Governance Foundation - iteratively add governance controls</li> </ul> </li> <li>Manage<ul> <li>Establish a Managment Baseline - minimum commitment to operations management</li> <li>Define Business Commitments - document supported workloads and required management investments for each</li> <li>Expand the Management Baseline</li> <li>Advanced Operations and Design Principles</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Create a Subscription Governance Strategy<ul> <li>Teams often start their governance strategy at the subscription level</li> <li>Three main aspects to consider:<ul> <li>Billing - take account of billing requirements when planning subscriptions. Use Tags if needed to identify billing department</li> <li>Access Control - isolation of environments based on subscriptions</li> <li>Subscription Limits - subscriptions have resource limitations: e.g. number of ExpressRoute connections</li> <li>Quotas for resources in resource groups are per region rather than per subscription</li> </ul> </li> </ul> </li> <li>Microsoft Trusted Cloud Principles<ul> <li>The shared responsibility model means that depending on the service, some responsibilities will transfer to the service provider</li> <li>Microsofts Trusted Cloud Principles means that Microsoft will provides contractual agreements to ensure:<ul> <li>Data residency</li> <li>Data security</li> <li>Data privacy</li> <li>Data compliance</li> </ul> </li> <li>The Microsoft Trust Center can be used to access detailed statements from Microsoft on their security, privacy and compliance standards<ul> <li>The privacy statement details how each Microsoft service interacts with your data</li> <li>The Product Terms site lists the terms and conditions of Microsoft product licences. The Data Protection Addendum lists the data processing conditions associated with these licences</li> <li>Legal and Regulatory compliance documentation can be found at https://docs.microsoft.com/azure/compliance</li> </ul> </li> </ul> </li> </ul>"},{"location":"azure/afc/security/","title":"Security","text":""},{"location":"azure/afc/security/#threat-modeling","title":"Threat Modeling","text":"<p>The Threat Priority Model can be used to identify your threat priorities. It starts by identifying the Impact of a threat occuring on a resource, then identifies the probability of this happening by defining the vulnerability and identifying any existing counter-measures (mitigations). The Impact and Probability together define the Threat Priority</p>"},{"location":"azure/afc/security/#zero-trust","title":"Zero Trust","text":"<p>The attack chain (or kill chain) begins with a compromised account that is then used to gain elevated  priviledges to mount an attack. Zero Trust uses the principle of Never Trust: Always Verify</p>"},{"location":"azure/afc/security/#defence-in-depth","title":"Defence In Depth","text":"<p>A defence-in-depth strategy addresses security at the following layers:</p> <pre><code>1. Physical Security - protecting access to buildings and hardware\n2. Identity and Access - authentication and authorisation controls\n3. Perimeter - protecting the network perimeter using DDoS protection and firewalls\n4. Network - limiting communication between and to resources\n5. Compute - secure access to VMs and endpoints\n6. Application - securing applications, remediating vulnerabilities\n7. Data - securing access to data\n</code></pre> <ul> <li> <p>Microsoft Defender for Cloud (previously Azure Security Centre and Azure Defender)</p> <ul> <li>Monitor security posture for Azure and on-prem resources. Security Posture (CIA):<ul> <li>Confidentiality - sensitive data must be kept protected and accessed only by those who should have access through the principle of least priviledge</li> <li>Integrity - confidence that data has not been altered or tampered with</li> <li>Availability - data and systems should be available to those that need them</li> </ul> </li> <li>Automatically apply security settings to new resources</li> <li>Provides recommendations</li> <li>Continuously monitor and assess vulnerabilities</li> <li>Block malware with machine learning</li> <li>Define rules for allowed applications (adaptive application controls)</li> <li>Threat detection</li> <li>Just-in-time access control for network ports</li> <li>Secure score reflects compliance to assigned governance controls</li> <li>The Regulatory Compliance dashboard provides overall compliance score and the number of failing assessments</li> <li>Adaptive network hardening monitors network activity compared to NSGs and makes recommendations</li> <li>File integrity monitoring - monitor important files</li> <li>Use Workflow Automation (Logic Apps) to respond to security alerts</li> </ul> </li> <li> <p>Microsoft Sentinel (previously Azure Sentinel)</p> <ul> <li>Microsoft Managed (SaaS), cloud-based Security Information and Event Management (SIEM) and Security Orchestration, Automation and Response (SOAR) system</li> <li>Aggregates security data from many different sources, using open-standard logging format</li> <li>Threat detection uses built-in rules (AI) provided as templates or custom rules</li> <li>Threat response can be automated with Azure Monitor Workbooks, to set alerts or send emails. Email will include link to Block or Ignore threat</li> <li>Utilises Microsoft's analytics and threat intelligence</li> <li>Several methods exist for connecting security data sources to Sentinel, including native support for M365, AAD, Syslog, CEF, REST</li> <li>Data is stored in Log Analytics</li> </ul> </li> <li> <p>Azure Key Vault</p> <ul> <li>Secure, centralised storage for application secrets</li> <li>Monitor and control access to secrets</li> <li>Simplify management and renewal of certificates</li> <li>Integrates with other Azure services - storage accounts, container registries, event hubs, etc</li> <li>Available in Standard and Premium SKUs</li> <li>Premium adds support for HSM-protected Keys</li> <li>Resources must be in the same region and subscription to access key vault secrets</li> </ul> </li> <li> <p>Azure Dedicated Host</p> <ul> <li>VMs hosted on physical servers that are not shared with other customers</li> <li>Host groups provide for high availability</li> <li>Charge is per dedicated host - not per VM running on the host</li> </ul> </li> </ul> <p>Further Reading:</p> <ul> <li>Resolve Security Threats</li> <li>Security Monitoring Plan</li> <li>Manage App Secrets</li> <li>Manage Key Vault Secrets</li> <li>Implement Resource Management Security</li> <li>Secure Cloud Data</li> <li>Develop a Security and Compliance Plan</li> <li>Manage Security Operations in Azure</li> </ul>"},{"location":"azure/app_service/app_service/","title":"App Service Overview","text":""},{"location":"azure/app_service/app_service/#app-service-plans","title":"App Service Plans","text":"<p>App Service Plans defines:</p> <ul> <li>Region</li> <li>Number of VM instances</li> <li>Size of VM instances</li> </ul> <p>All applications in the plan will run all resources defined by the plan. </p> <p>Pricing Tier Categories</p> <ul> <li>Shared Compute (Free and Shared): resource pools are shared with other customers. Intended for development and testing only. Apps are assigned CPU quotas (in minutes) and cannot scale out</li> <li>Dedicated Compute (Basic, Standard, Premium, PremiumV2, PremiumV3): run on dedicated VMs. Only apps in the same Service Plan share resources. Higher Tiers provide more VMs for scale-out</li> <li>Isolated: adds dedicated VNet to dedicated VMs. Provides maximum scale-out capabilities</li> <li>Consumption: only available to Function Apps</li> </ul> Feature Free Shared Basic Standard Premium Isolated Usage DEV/Test DEV/Test DEV/Test Production Enhanced Scale Secure, High Performance Apps 10 100 Unlimited Unlimited Unlimited Unlimited Disk Space 1 GB 1 GB 10 GB 50 GB 250 GB 1 TB Autoscale - - - yes yes yes Deployment Slots - - - 5 20 20 Max Instances - - 3 10 30 100 <p>The App Service Plan is the scale unit for the apps. Apps in the Service Plan run on all VM instances configured by the Plan or the autoscale (scale-out) settings. Scaling up or down is achieved by changing the pricing tier of the plan.</p> <p>Because the App Service Plan determines the resources available, you can save money by running multiple apps in the same App Service Plan. You may want to consider having multiple plans to reflect the resource requirements of your apps or to isolate sensitive or  resource-hungry apps. </p> <p>Use <code>az appservice plan list</code> to view App Service Plans. </p>"},{"location":"azure/app_service/app_service/#app-deployment","title":"App Deployment","text":"<p>App Service supports both automated (continuous integration) or manual deployment. Automated  deployments can run from Azure DevOps, GitHub or BitBucket. Azure DevOps allows you to:</p> <ul> <li>build your code in the cloud</li> <li>run the tests</li> <li>generate a release</li> <li>push your code to an Azure Web App</li> </ul> <p>Github deployments will automatically deploy when you push to the production branch Manual Deployment Methods include:</p> <ul> <li>Git: configure a Git URL for the web app and deploy by pushing to the Git URL</li> <li>CLI: use the <code>az webapp up</code> command to create and package your app and deploy it</li> <li>Zip: send a zip of your application files using <code>curl</code> or similar command</li> <li>FTPS: code can be loaded to the App Service using FTP or FTPS</li> </ul> <p>You can deploy a static html site from your current working directory using:</p> <pre><code>az webapp up -g $RESOURCE_GROUP -n $APP_NAME --html\n</code></pre> <p>This command will:</p> <ul> <li>Create the resource group if one is not specified</li> <li>Create a default App Service Plan</li> <li>Create an app with the specified name</li> <li>Zip deploy the files from the current working directory</li> </ul> <p>Apps are deployed with the following default FQDN: <code>https://${APP_NAME}.azurewebsites.net</code></p> <p>You can query details for a webapp using <code>az webapp list</code> or <code>az webapp show</code>.</p>"},{"location":"azure/app_service/app_service/#authentication-and-authorisation","title":"Authentication and Authorisation","text":"<p>App Service provides built-in authentication and authorisation otherwise you can use the features provides by your web-application framework. App Service uses federated identity supporting AAD, Facebook, Twitter, Google, or any OpenID provider. The federated providers are configured by  setting the 'sign-in endpoint' for your app. Multiple providers can be used. The authentication flow can be configured as a server-directed flow where the user is redirected to the providers sign-in page or using the provider SDK, where the application handles the sign-in and returns the authentication token to the app. App Service can be configured to 'allow unauthenticated requests'. Unauthenticated requests can  then be handled by your application code. The App Service can also be configured to 'require authentication': in which case you can redirect all unauthenticated requests to the appropriate handler.</p>"},{"location":"azure/app_service/app_service/#networking","title":"Networking","text":"<p>By default, apps hosted in App Service are accessible via the internet and can communicate with  internet endpoints. The Isolated SKU hosts apps in an dedicated Azure VNet, but all the other tiers are hosted on a multi-tenant network. For this reason, you cannot connect the multi-tenant  app services directly to your network. App Service apps are distributed applications: incoming http requests are handle by front-end roles;  application workload is handled by worker roles. Different features are available depending on whether  traffic is incoming or outgoing for your app:</p> <ul> <li> <p>Inbound Features</p> <ul> <li>App-assigned address</li> <li>Access restrictions</li> <li>Service endpoints</li> <li>Private endpoints</li> </ul> </li> <li> <p>Outbound Features</p> <ul> <li>Hybrid Connections</li> <li>Gateway-required virtual network connection</li> <li>Virtual network integration</li> </ul> </li> </ul> <p>The Free, Shared, Basic, Standard and Premium plans all use the same worker VM Type. Premium V2 and  Premium V3 each use a different worker type. When the worker type is changed during a scale-up or down,  the outbound IP address is changed. The current outbound IP for your app can be listed in the  Properties for the app in the portal or by using the following command:</p> <pre><code>az webapp show -g $RG -n $NAME --query outboundIpAddress\n</code></pre> <p>To find all possible IP addresses for your app, use:</p> <pre><code>az webapp show -g $RG -n $NAME --query possibleOutboundIpAddresses\n</code></pre>"},{"location":"azure/app_service/app_service/#configuring-application-settings","title":"Configuring Application Settings","text":"<p>In App Service, application settings are passed as environment settings to the application code.  For Linux apps and Custom Containers, App Service uses the '--env' flag to pass these settings  to the app. Application settings are configured in the Portal via  'Configuration &gt; Application Settings'.  This way you can have one environment when  running the app locally and another environment for your app when running in Azure The settings in the app configuration are encrypted. Click 'New Application Setting' to add a new app setting. For nested JSON keys use an underscore instead of a colon for the name. By clicking 'Advanced' you can add settings in bulk using a  JSON array.</p> <pre><code>[\n  {\n    \"name\": \"\",\n    \"value\": \"\",\n    \"slotSetting\": false\n  },\n  {\n    \"name\": \"\",\n    \"value\": \"\",\n    \"slotSetting\": false\n  },\n  ...\n]\n</code></pre> <p>For ASP.NET developers, connection strings can also be cofigured in the apps Configuration page.  For other languages, you may need to set the connection string within the codebase to ensure correct formatting. However, certain database types are only backed-up with the app if the  connection is configured in the configuration settings of the app. Connection strings  can also be bulk-inserted using JSON:</p> <pre><code>[\n  {\n    \"name\": \"\",\n    \"value\": \"\",\n    \"type\": \"SQLServer\n    \"slotSetting\": false\n  },\n  {\n    \"name\": \"\",\n    \"value\": \"\",\n    \"type\": \"PostgreSQL\"\n    \"slotSetting\": false\n  },\n  ...\n]\n</code></pre>"},{"location":"azure/app_service/app_service/#configuring-general-settings","title":"Configuring General Settings","text":"<p>General settings for the app can be configured via 'Configuration &gt; General Settings'. These  include:</p> <ul> <li>Stack settings: language, major and minor versions. For container apps you can also set a startup command</li> <li> <p>Platform settings:</p> <ul> <li>32-bit or 64-bit</li> <li>Websocket Protocol</li> <li>Always On: keep the app loaded even when there's no requests. Required by WebJobs or WebJobs triggered by a cron expression. If disabled, app is unloaded after 20 minutes of inactivity</li> <li>IIS pipeline mode. Set to 'Classic' if you require a legacy version of IIS</li> <li>HTTP version</li> <li>ARR affinity: set to 'On' to ensure that the client is routed to the same worker for the duration of the session. Set to 'Off' for stateless applications</li> </ul> </li> <li> <p>Debugging: used to enable debugging for 48 hours for an ASP or Node app</p> </li> <li>Incoming Client Certificates: require client certificates for mutual authentication</li> </ul>"},{"location":"azure/app_service/app_service/#configuring-path-mappings","title":"Configuring Path Mappings","text":"<p>Use 'Configuration &gt; Path Mappings' to configure handler mappings, and virtual application and  directory mappings. In Windows apps, handler mappings can be used to add custom scripts to handle specific file extensions. Default root-path settings can be changed in Path Mappings. For Linux apps,  you can add custom storage</p>"},{"location":"azure/app_service/app_service/#diagnostic-logging","title":"Diagnostic Logging","text":"<p>Application logging tracks logs generated by the application code. Deployment logging tracks  information concerning app deployment. Application and Deployment logging is available for both  Windows and Linux apps. Additional logs are available for Windows apps only: - Web server logging: raw HTTP request data - Detailed error logging: copies of the .html error pages that would have been sent to the client - Failed request tracing: detailed tracing information Application logging for Windows allows you to log to File (for temporary logging over a 12 hour period) or Blob and lets you choose the log level (Disabled, Error, Warning, Information, Verbose). For Linux apps you set the logging to file system and choose the retention period Stream logs can be used by writing to the '/LogFiles' directory using '.htm', '.txt', '.log'  file extensions Configured log files can be accessed from the Storage Account. For logs stored in the App Service file system use: https://.scm.azurewebsites.net/api/logs/docker/zip (for Linux) or  https://.scm.azurewebsites.net/api/dump (for Windows)."},{"location":"azure/app_service/app_service/#configuring-certificates","title":"Configuring Certificates","text":"<p>Azure App Service comes with tools for managing private and public certificates. A certificate uploaded to an app is stored in a deployment unit that is bound to the apps resource-group and  region combination (referred to as the 'webspace'). The certificate is then accessible to other apps in the same webspace. There are various options for adding certificates in App Service:</p> <ul> <li>Create a free App Service managed certificate: a private certificate to secure your custom domain in App Service</li> <li>Purchase an App Service certificate: a private certificate managed by Azure. Provides automated certificate management and renewal</li> <li>Import a certificate from Key Vault</li> <li>Upload a private certificate</li> <li>Upload a public certificate: can't be used with custom domains</li> </ul> <p>Use the TLS/SSL settings for your app to enforce HTTPS only</p>"},{"location":"azure/app_service/app_service/#feature-flags","title":"Feature Flags","text":"<p>Azure App Service supports the use of feature flags to enable or disable features in an  application. </p>"},{"location":"azure/app_service/app_service/#autoscaling","title":"Autoscaling","text":"<p>Scale-up increases the amount of CPU, Memory and Disk Space available to your app, and  therefore requires changing the pricing ter of the App Service Plan. </p> <p>Scale-out increases the number of VM instances that run your application. </p> <p>Autoscaling performs scale-in or scale-out and can be triggered by:</p> <ul> <li>A schedule</li> <li>CPU usage</li> <li>Memory occupancy</li> <li>Number of incoming requests</li> <li>A combination of factors</li> </ul> <p>Autoscaling monitors the resource metrics of a web app as it runs and responds to changes by adding or removing web servers and load balancing between them. Autoscaling does not affect the  resources available to the web servers: only the number of web servers. Autoscaling responds to  rules that define thresholds to trigger an autoscale event.</p> <p>Care should be taken when defining autoscale rules: scaling-up in response to a DDoS attack would be pointless and expensive. Instead, DDoS protection should be enabled to filter such  attacks.</p> <p>Autoscaling improves availability and fault-tolerance by ensuring enough servers are available to  service requests and that crashed servers are replaced. Autoscaling might not help cope with  resource-instensive requests: scaling-up might be a better response in this case. Autoscaling  does take time to spin up new instances, and so availability and response time may be affected  during periods of surge in demand</p> <p>Autoscale rules can combine both schedules and metric-based rules. Metrics include:</p> <ul> <li>CPU Percentage</li> <li>Memory Percentage</li> <li>Disk Queue Length</li> <li>Http Queue Length</li> <li>Data In</li> <li>Data OUt</li> </ul> <p>Metrics can also be used from other Azure services, e.g Service Bus queue length. Metrics are  aggregated over a period of time known as a time grain (usually 1 minute). The autoscale event is  only triggered if the rule crosses the threshold over a time duration (usually 10 minutes). Several aggregations are available. If the aggregation is MAX, then a single time grain exceeding the  threshold will be enough to trigger the autoscale. If the aggregation is AVG, then multiple time grains will need to exceed the threshold to trigger the autoscale for the duration. </p> <p>Autoscaling can either trigger an incremental increase or decrease or the trigger can set the  number of instances explicitly. Autoscale rules should be paired: setting the value for when  to scale-out and when to scale-in in response to a specific metric. Scaling rules can combine multiple conditions. Scale-out will occur if any scale-out rule is met: scale-in will occur only if all scale-in rules are met. If you want to scale-in when only one scale-in rule is met, then you should define the rules separately</p> <p>Once auto-scaling is enabled for an app, scale conditions can be defined. The default  condition can be use to define a default scale for the app (e.g. one instance). The default condition will be applied when no other condition is met, or when there are no metrics available due to a system crash. Additional  conditions can be used to define the scale according to a schedule or in response to metrics.  The conditions allow you to set minimum, maximum and default scales for the app. Autoscaling activity can be monitored in the 'Run History' tab of the autoscale blade Thresholds are calculated across instances. Flapping can occur if there is not an adequate margin between scale-out and scale-in conditions. For instance consider a rule to scale out if  queue length is &lt;80 and scale-in if queue length is &gt;80. When three instances are running  with a queue length of 75 if scale-in were to occur the queue length would be &gt;80 for the  two instances (75 x 3 / 2), so the app would immeadiately have to scale-out again. To avoid  flapping, autoscale doesn't scale-in when it detects that the scale-out threshold will be  met if it scales-in. By setting an adequate margin between scale-out and scale-in conditions,  flapping can be avoided.</p> <p>Autoscaling logs activity and these can be used to trigger alerts or used by webhook or  email notifications. Autoscaling logs the following:</p> <ul> <li>issue a scale operation</li> <li>successfully complete a scale action</li> <li>failed scale action</li> <li>metrics not available</li> <li>metrics available again</li> </ul>"},{"location":"azure/app_service/app_service/#deployment-slots","title":"Deployment Slots","text":"<p>Standard, Premium and Isolated Service Plans provide Deployment Slots for your application, allowing the same app to run in different environments. Each slot has it's own hostname. Slots can be swapped,  allowing you to test a new version of the app before swapping it into production. Slot swapping  provides the following benefits:</p> <ul> <li>validate changes in a development slot before swapping it into the production slot</li> <li>swapping a development slot into production, means that all instances of the slot are warmed-up before being swapped, thus speeding up the deployment into production</li> <li>The previous production slot is preserved in the slot is was swapped with, enabling quick failback if errors are found in the new production slot</li> </ul> <p>Standard Plans provide 5 deployment slots whereas Premium and Isolatd plans provide up to 20 slots. New deployment slots have no content, even if you clone them from an existing environment slot.  You can deploy to the slot from a repository branch or a different repository When slots are swapped, App Service ensures that the target slot does not experience downtime as  follows:</p> <ul> <li>Apply the following settings from the target to the source slot:<ul> <li>slot-specific application settings and connection strings</li> <li>continuous deployment settings</li> <li>app service authentication settings</li> </ul> </li> <li>Restart source slot instances. If using 'swap with preview' the swap is paused to allow you to validate this phase of the swap. If an instance fails to restart, the changes are reverted and the swap is discontinued</li> <li>initialise local caches on each instance</li> <li>trigger application warm-up on each instance in the source slot</li> <li>swap the two slots by switching the routing rules for the two slots</li> <li>re-apply the process to the source slot, so that it has a copy of the previous app</li> </ul> <p>Since all the activities are applied to the source slot, the target slot experiences minimal  downtime.</p> <ul> <li> <p>Settings that are NOT swapped (slot specific):</p> <ul> <li>Publishing endpoints</li> <li>Custom domain names</li> <li>non-public certificates and TLS/SSL settings</li> <li>Scale settings</li> <li>WebJobs schedulers</li> <li>IP restrictions</li> <li>Always On</li> <li>Diagnostic log settings</li> <li>Cross-Origin Resource Sharing (CORS)</li> </ul> </li> <li> <p>Settings that are swapped:</p> <ul> <li>General settings</li> <li>Handler settings</li> <li>Public certificates</li> <li>WebJobs content</li> </ul> </li> <li> <p>Settings that are swapped but can be configured to NOT swap:</p> <ul> <li>App settings</li> <li>Connection strings To configure the settings to not swap, select 'deployment slot setting' next to the setting in the configuration page for that slot</li> </ul> </li> <li> <p>Setting that are currently swapped but planned to be NOT swapped:</p> <ul> <li>Hybrid connections</li> <li>Virtual network integration</li> <li>Service Endpoints</li> <li>Azure Content Delivery Network</li> </ul> </li> </ul> <p>Auto swap can be configured for a slot, so that every time code is pushed to a slot, App Service  will automatically swap the slot into the target slot. Auto swap is not currently available for  web apps on Linux By default, all traffic to the app's production URL are routed to the production slot. You can  route traffic to another slot by setting the percentage of traffic to be routed to the new slot.  Information about the slot a request is routed to, can be inspected in the HTTP header by checking the value of 'x-ms-routing-name'. You can also manually route traffic to a different slot by  including an 'x-ms-routing-name' link on your website page. New slots are automatically assigned  a routing percentage of zero. If you set this value to zero explicitly, then developers can  access the slot using 'x-ms-routing-name' for testing purposes</p>"},{"location":"azure/app_service/flask_app_service_part_one/","title":"Deploy a Flask App to Azure: Part 1","text":""},{"location":"azure/app_service/flask_app_service_part_one/#intro","title":"Intro","text":"<p>In this series of articles, we're going to build a Flask App connected to a backend PostgreSQL  database. We'll develop the application and database locally using Docker,  then we'll install a production version on Azure using Azure App Service and  Azure Database for PostgreSQL</p> <p>On this page we'll focus on developing the Flask application locally. </p>"},{"location":"azure/app_service/flask_app_service_part_one/#references","title":"References","text":"<p>For the Flask application we're going to be following Miguel Grinbergs Tutorial For the deployment to Azure App Services we'll be using the Microsoft Sample.  For connecting Flask to Postgres we're using Abdel Dyouri's post on Digital Ocean Jinja2</p>"},{"location":"azure/app_service/flask_app_service_part_one/#setup-local-development-environment","title":"Setup Local Development Environment","text":"<p>For our local development in Docker, we'll be using to containers:</p> <ol> <li>A customised Python image to run the Flask Application and</li> <li>A standard PostgreSQL image</li> </ol> <p>Begin by creating a project directory (flask_pg_app_service) and create a subfolder to hold the  application (ear)</p> <pre><code>mkdir flask_pg_app_service\ncd flask_pg_app_service\nmkdir ear\n</code></pre> <p>To run our Flask application we will use a custom docker image based on Python 3.8. Add a Dockerfile to the Flask application directory <code>flask_pg_app_service/ear/Dockerfile</code>:</p> <pre><code>FROM python:3.8-slim-buster\n\nARG user_id=1000\nARG user_name=ear\n\nEXPOSE 5000\n\nRUN groupadd -g $user_id $user_name &amp;&amp; \\\nuseradd -u $user_id -g $user_id -d /home/$user_name -m $user_name\n\nWORKDIR /opt\n\nCOPY requirements.txt requirements.txt\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nUSER $user_id\n\nENV FLASK_APP=/opt/ear.py\n\nENV FLASK_DEBUG=1\n\nCMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n</code></pre> <p>We'll build and run this image using Docker Compose. The docker-compose.yml file will be stored  in the root of the project directory <code>flask_pg_app_service/docker-compose.yml</code></p> <pre><code>version: '3.8'\n\nservices:\n  app:\n    build: \n      context: ./ear\n    image: ear:1.0\n    ports: \n      - 5000:5000\n    volumes:\n      - ./ear:/opt\n    depends_on: [db]\n  db:\n    image: postgres:11.0\n    restart: always\n    environment:\n      POSTGRES_PASSWORD: example\n      POSTGRES_USER: example\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\nvolumes:\n  pgdata:\n</code></pre> <p>Before we can build and run the containers, we'll need to add some code for the Flask App.</p>"},{"location":"azure/app_service/flask_app_service_part_one/#setup-a-minimal-flask-application","title":"Setup a Minimal Flask Application","text":"<p>We can start with the requirements file for the application <code>flask_pg_app_service/ear/requirements.txt</code></p> <pre><code>flask\nflask-wtf\npsycopg2-binary\nflask-sqlalchemy\nflask-migrate\n</code></pre> <p>Then add the package code in <code>flask_pg_app_service/ear/app/__init__.py</code></p> <pre><code>from flask import Flask\n\napp=Flask(__name__)\n\nfrom app import routes\n</code></pre> <p>Next create the routes module in <code>flask_pg_app_service/ear/app/routes.py</code></p> <pre><code>from app import app\n\n@app.route('/')\n@app.route('/index')\ndef index():\n    return \"Hello World!\"\n</code></pre> <p>Then add the Flask application file <code>flask_pg_app_service/ear/ear.py</code></p> <pre><code>from app import app\n</code></pre> <p>With both the Flask app written and a Docker container defined, let's build and run the container:</p> <pre><code>flask_pb_app_service/ear$ docker image build --build-arg user_id=$(id -u) -t ear:0.1 .\nflask_pg_app_service/ear$ docker container run --rm -p 5000:5000 --name ear_we_go ear:0.1\n</code></pre> <p>We can test that this works by visiting http://localhost:5000. If all is working well, we can stop  this container and remove the image, since we'll build the image from the docker-compose file later. </p>"},{"location":"azure/app_service/flask_app_service_part_one/#connect-the-flask-app-to-the-postgresql-database","title":"Connect the Flask App to the PostgreSQL Database","text":"<p>We can now expand our application to use a backend database. The application will start simple, and will display a list of books. The book list will be stored in a single table on our PostgreSQL  database. We'll be using SQLAlchemy to connect to the database. Lets add some configuration  data in `flask_pg_app_service/ear/config.py</p> <pre><code>class Config(object):\n    SQLALCHEMY_DATABASE_URI = 'postgresql+psycopg2://example:example@db/example'\n    SQLALCHEMY_TRACK_MODIFICATIONS = False\n</code></pre> <p>Now we can use this configuration in our app module to define a db and migrate objects for use by the application <code>flask_pg_app_service/ear/app/__init__.py</code></p> <pre><code>from flask import Flask\nfrom config import Config\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_migrate import Migrate\n\napp=Flask(__name__)\napp.config.from_object(Config)\ndb = SQLAlchemy(app)\nmigrate = Migrate(app, db)\n\nfrom app import routes, models\n</code></pre> <p>Now we add the Models to the app with a single table to hold the book list <code>flask_pg_app_service/ear/app/models.py</code></p> <pre><code>from datetime import datetime\nfrom app import db\n\nclass Book(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    title = db.Column(db.String(120), index=True, unique=True)\n    author = db.Column(db.String(120), index=True)\n    status = db.Column(db.String(30))\n\n    def __repr__(self):\n        return '{} by {}'.format(self.title, self.author)\n</code></pre> <p>We want our index page to display the list of books, so we need to update the Routes module: <code>flask_pg_app_service/ear/app/routes.py</code></p> <pre><code>from flask import render_template, flash, redirect, url_for\nfrom app import app, db\nfrom app.models import Book\n\n@app.route('/')\n@app.route('/index')\ndef index():\n    books = Book.query.all()\n    return render_template('index.html', title = 'Home Page', books=books)\n</code></pre> <p>We're using <code>render_template</code> to display handle our views, so we need to add a template for the  index page: <code>flask_pg_app_service/ear/app/templates/index.html</code></p> <pre><code>{% extends \"base.html\" %}\n\n{% block content %} \n    &lt;h1&gt;Reading List&lt;/h1&gt;\n    {% for book in books %}\n    &lt;div&gt;&lt;p&gt;{{ book.title }} by {{ book.author }} ( {{ book.status }} )&lt;/b&gt;&lt;/p&gt;&lt;/div&gt;\n    {% endfor %}\n{% endblock %}\n</code></pre> <p>We're using a base template to hold the overall layout for pages in our site, so we need to create this also <code>flask_pg_app_service/ear/app/templates/base.html</code></p> <pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    {% if title %}\n    &lt;title&gt;{{ title }} - EAR&lt;/title&gt;\n    {% else %}\n    &lt;title&gt;Welcome to EAR&lt;/title&gt;\n    {% endif %}\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div&gt;Ear: \n    &lt;a href=\"{{ url_for('index') }}\"&gt;Home&lt;/a&gt;\n    &lt;/div&gt;\n    &lt;hr&gt;\n    {% with messages = get_flashed_messages() %}\n    {% if messages %}\n    &lt;ul&gt;\n    {% for message in messages %}\n    &lt;li&gt;{{ message }}&lt;/li&gt;\n    {% endfor %}\n    &lt;/ul&gt;\n    {% endif %}\n    {% endwith %}\n    {% block content %}{% endblock %}\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"azure/app_service/flask_app_service_part_one/#create-the-database-tables","title":"Create the Database Tables","text":"<p>We now have all the code in place to run our Flask application but the books table does not exist  yet. We can use the app container to create the table. First we need to build the app container:</p> <pre><code>flask_pg_app_service/ear$ docker compose build --build-arg user_id=$(id -u) app\n</code></pre> <p>Once built we can start both the database and the Flask app with</p> <pre><code>flask_pg_app_service/ear$ docker compose up\n</code></pre> <p>If we try to connect to the index page on the application, we will receive an error that the books  table does not exist. We can use flask-migrate to create the books table. First, connect to the  app container:</p> <pre><code>docker compose exec app bash\n</code></pre> <p>From here we can run the flask-migrate commands. First time around you will run:</p> <pre><code>flask db init\n</code></pre> <p>This will setup the repository to hold the database migrations. Once complete you can  create the first migration using: </p> <pre><code>flask db migrate -m \"Add book table to database\"\n</code></pre> <p>This will create python scripts to install the Model to the database. To install the Model you will need to run: </p> <pre><code>flask db upgrade\n</code></pre> <p>Now with the book table created, we can connect to the index page http://localhost:5000/, which  will diplay the page header ('Reading List'), but there will be no books displayed. We need to add some data into the books table. </p>"},{"location":"azure/app_service/flask_app_service_part_one/#add-data-to-the-books-table","title":"Add Data to the Books Table","text":"<p>You can manually connect to the db container and add some rows to the book table using the psql  command. Alternatively, you can use Python from the app container to do the same. If you use  Python, you will need to import the db and book objects. An alternative method is to use  the Flask shell. To use Flask shell you will need to setup a shell context configuration in  <code>ear/ear.py</code>:</p> <pre><code>    from app import app, db\n    from app.models import Book\n\n    @app.shell_context_processor\n    def make_shell_context():\n        return {'db': db,  'Book': Book}\n</code></pre> <p>Now you can run <code>flask shell</code> on the app container to open a interactive Python session  with app, db, and Book already defined. With the 'flask shell' you can run DDL interactively:</p> <pre><code>&gt;&gt;&gt; b1 = Book(title='East of Eden', author='John Steinbeck', status='In Progress')\n&gt;&gt;&gt; db.session.add(b1)\n&gt;&gt;&gt; db.session.commit()\n&gt;&gt;&gt; quit()\n\nOnce we add some data to the book table we can refresh the index page on the app and the book\nshould display. \n</code></pre> <p>We can also add a page to the app for adding books to the book table. We'll start by adding a route to the <code>flask_pg_app_service/ear/app/routes.py</code></p> <pre><code>from flask import render_template, flash, redirect, url_for\nfrom app import app, db\nfrom app.forms import BookForm\nfrom app.models import Book\n\n@app.route('/')\n@app.route('/index')\ndef index():\n    books = Book.query.all()\n    return render_template('index.html', title = 'Home Page', books=books)\n\n@app.route('/add_book', methods=['GET', 'POST'])\ndef add_book():\n    form = BookForm()\n    if form.validate_on_submit():\n        book = Book(\n            author=form.author.data,\n            title=form.title.data, \n            status=form.status.data\n        )\n        db.session.add(book)\n        db.session.commit()\n        flash('Your changes have been saved')\n        return redirect(url_for('index'))\n    return render_template('add_book.html', title='Add Book', form=form)\n</code></pre> <p>The new route references a form object and a new template which we'll need to create. First  there's the BookForm object. We'll create this in the `flask_pg_app_service/ear/app/forms.py:</p> <pre><code>from flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, BooleanField, SubmitField\nfrom wtforms.validators import DataRequired\n\nclass BookForm(FlaskForm):\n    author = StringField('Author', validators=[DataRequired()])\n    title = StringField('Title', validators=[DataRequired()])\n    status = StringField('Status', validators=[DataRequired()])\n    submit = SubmitField('Add Book')\n</code></pre> <p>Now we can create the page to display this form <code>flask_pg_app_service/ear/app/templates/add_book.html</code></p> <pre><code>{% extends \"base.html\" %}\n\n{% block content %}\n    &lt;h1&gt;Add New Book&lt;/h1&gt;\n    &lt;form action=\"\" method=\"Post\" novalidate&gt;\n        {{ form.hidden_tag() }}\n        &lt;p&gt;\n            {{ form.author.label }}&lt;br&gt;\n            {{ form.author(size=120) }}\n        &lt;/p&gt;\n        &lt;p&gt;\n            {{ form.title.label }}&lt;br&gt;\n            {{ form.title(size=120) }}\n        &lt;/p&gt;\n        &lt;p&gt;\n            {{ form.status.label }}&lt;br&gt;\n            {{ form.status(size=30) }}\n        &lt;/p&gt;\n        &lt;p&gt;{{ form.submit() }}&lt;/p&gt;\n    &lt;form&gt;\n{% endblock %}\n</code></pre> <p>The <code>form.hidden_tag()</code> in the page, allows us to use CSRF, but we need to add a SECRET_KEY to the  application configuration <code>flask_pg_app_service/ear/config.py</code></p> <pre><code>import os\n\nclass Config(object):\n    SQLALCHEMY_DATABASE_URI = 'postgresql+psycopg2://example:example@db/example'\n    SQLALCHEMY_TRACK_MODIFICATIONS = False\n    SECRET_KEY = os.environ.get('SECRET_KEY') or 'you-will-never-guess'\n</code></pre> <p>Now we can add books by navigating to http://localhost:5000/add_book. To make life easier, we can add this link to either the navigation or to the index page. Let's add it to the index page <code>flask_pg_app_service/ear/app/templates/index.html</code></p> <pre><code>{% extends \"base.html\" %}\n\n{% block content %} \n    &lt;h1&gt;Reading List&lt;/h1&gt;\n    {% for book in books %}\n    &lt;div&gt;&lt;p&gt;{{ book.title }} by {{ book.author }} ( {{ book.status }} )&lt;/b&gt;&lt;/p&gt;&lt;/div&gt;\n    {% endfor %}\n    &lt;p&gt;&lt;a href=\"{{ url_for('add_book') }}\"&gt;Add a New Book&lt;/a&gt;&lt;/p&gt;\n{% endblock %}\n</code></pre>"},{"location":"azure/app_service/flask_app_service_part_one/#summary","title":"Summary","text":"<p>We now have a working Flask application that displays data from a database. The application still has a lot of features missing:</p> <ol> <li>Authentication and Authorisation have not been implemented</li> <li>Additional CRUD operations are missing</li> <li>Data can be added without validation</li> <li>Site does not yet run with SSL</li> </ol> <p>In a production environment, you would want to fix these issues. We can do this as we develop the  app further, but we have enough code now to try to deploy the app to Azure. </p>"},{"location":"azure/db/cosmosdb/","title":"CosmosDB","text":"<p>Azure CosmosDB documentation</p>"},{"location":"azure/db/cosmosdb/#cosmosdb-for-mongodb","title":"CosmosDB for MongoDB","text":"<p>Create a MongoDB in CosmosDB:</p> <pre><code>resource \"azurerm_cosmosdb_account\" \"cosmosdb_example\" {\n  name                            = \"cosmosdb_example\"\n  location                        = var.location\n  resource_group_name             = var.rg_name\n  offer_type                      = \"Standard\"\n  kind                            = \"MongoDB\"\n  enable_automatic_failover       = false\n  enable_multiple_write_locations = false\n  mongo_server_version            = \"4.0\"\n\n  # Restrict access to chosen subnet in VNet\n  # Requires service endpoint 'Microsoft.AzureCosmosDB' defined on the subnet\n\n  is_virtual_network_filter_enabled = true\n  virtual_network_rule {\n    id                                   = azurerm_subnet.snet_example.id\n    ignore_missing_vnet_service_endpoint = true\n  }\n\n  # Allow access from Azure Portal (https://docs.microsoft.com/azure/cosmos-db/how-to-configure-firewall#allow-requests-from-the-azure-portal)\n  ip_range_filter = \"104.42.195.92,40.76.54.131,52.176.6.30,52.169.50.45,52.187.184.26\"\n\n  tags = var.default_tags\n\n  capabilities {\n    name = \"EnableServerless\"\n  }\n\n  lifecycle {\n    ignore_changes = [capabilities]\n  }\n  consistency_policy {\n    consistency_level = \"Session\"\n  }\n\n  geo_location {\n    location          = var.location\n    failover_priority = 0\n    zone_redundant    = false\n  }\n}\n\n# ------------------------------------------------------------------------------------------------------\n# Deploy cosmos mongo db and collections\n# ------------------------------------------------------------------------------------------------------\nresource \"azurerm_cosmosdb_mongo_database\" \"mongodb\" {\n  name                = \"Todo\"\n  resource_group_name = var.resource_group_name\n  account_name        = azurerm_cosmosdb_account.cosmosdb_example.name\n}\n\nresource \"azurerm_cosmosdb_mongo_collection\" \"list\" {\n  name                = \"TodoList\"\n  resource_group_name = var.resource_group_name\n  account_name        = azurerm_cosmosdb_account.cosmosdb_example.name\n  database_name       = azurerm_cosmosdb_mongo_database.mongodb.name\n  shard_key           = \"_id\"\n\n\n  index {\n    keys   = [\"_id\"]\n    unique = true\n  }\n}\n\nresource \"azurerm_cosmosdb_mongo_collection\" \"item\" {\n  name                = \"TodoItem\"\n  resource_group_name = var.resource_group_name\n  account_name        = azurerm_cosmosdb_account.cosmosdb_example.name\n  database_name       = azurerm_cosmosdb_mongo_database.mongodb.name\n  shard_key           = \"_id\"\n\n  index {\n    keys   = [\"_id\"]\n    unique = true\n  }\n}\n</code></pre> <p>Create the service endpoint in the subnet:</p> <pre><code>resource \"azurerm_subnet\" \"snet_example\" {\n  name                 = \"snet_example\"\n  resource_group_name  = var.resource_group_name\n  virtual_network_name = var.vnet_name\n  address_prefixes     = [\"10.0.1.0/24\"]\n  service_endpoints    = [\"Microsoft.AzureCosmosDB\"]\n}\n\n</code></pre>"},{"location":"azure/db/cosmosdb/#connect-to-cosmosdb-for-mongodb-from-python","title":"Connect to CosmosDB for MongoDB from Python","text":"<p>See the quickstart guide</p> <pre><code>import os\nimport sys\nfrom random import randint\n\nimport pymongo\nfrom dotenv import load_dotenv\n\nload_dotenv()\n# use 'az cosmosdb keys list --type connection-strings --resource-group &lt;resource-group-name&gt; --name &lt;cosmosdb-name&gt;'\n# then 'export COSMOS_CONNECTION_STRING=&lt;primary_connection_string&gt;'\nCONNECTION_STRING = os.environ.get(\"COSMOS_CONNECTION_STRING\")\n\nDB_NAME = \"adventureworks\"\n\nCOLLECTION_NAME = \"products\"\n\nclient = pymongo.MongoClient(CONNECTION_STRING)\n\ndb = client[DB_NAME]\nif DB_NAME not in client.list_database_names():\n  db.command({\"customAction\": \"CreateDatabase\"})\n  print(\"Created db '{}' with shared throughput.\\n\".format(DB_NAME))\nelse:\n  print(\"Using database: '{}'.\\n\".format(DB_NAME))\n\n# Create collection if it doesn't exist\ncollection = db[COLLECTION_NAME]\nif COLLECTION_NAME not in db.list_collection_names():\n  # Creates a unsharded collection that uses the DBs shared throughput\n  db.command( {\"customAction\": \"CreateCollection\", \"collection\": COLLECTION_NAME})\n  print(\"Created collection '{}'.\\n\".format(COLLECTION_NAME))\nelse:\n  print(\"Using collection: '{}'.\\n\".format(COLLECTION_NAME))\n\nindexes = [ {\"key\": {\"_id\": 1}, \"name\": \"_id_1\"}, {\"key\": {\"name\": 2}, \"name\": \"_id_2\"}, ]\ndb.command( { \"customAction\": \"UpdateCollection\", \"collection\": COLLECTION_NAME, \"indexes\": indexes, })\nprint(\"Indexes are: {}\\n\".format(sorted(collection.index_information())))\n\n\"\"\"Create new document and upsert (create or replace) to collection\"\"\"\nproduct = { \"category\": \"gear-surf-surfboards\", \"name\": \"Yamba Surfboard-{}\".format(randint(50, 5000)), \"quantity\": 1, \"sale\": False, }\nresult = collection.update_one( {\"name\": product[\"name\"]}, {\"$set\": product}, upsert=True)\nprint(\"Upserted document with _id {}\\n\".format(result.upserted_id))\n\ndoc = collection.find_one({\"_id\": result.upserted_id})\nprint(\"Found a document with _id {}: {}\\n\".format(result.upserted_id, doc))\n\n\"\"\"Query for documents in the collection\"\"\"\nprint(\"Products with category 'gear-surf-surfboards':\\n\")\nallProductsQuery = {\"category\": \"gear-surf-surfboards\"}\nfor doc in collection.find(allProductsQuery).sort( \"name\", pymongo.ASCENDING):\n  print(\"Found a product with _id {}: {}\\n\".format(doc[\"_id\"], doc))\n</code></pre>"},{"location":"azure/db/postgresql/","title":"PostgreSQL","text":"<p>Continue here</p>"},{"location":"azure/db/postgresql/#core-concepts","title":"Core Concepts","text":"<p>Azure Database for PostgreSQL is a PaaS service that provides a fully-managed, highly available and horizontally scalable instance of community PostgreSQL in the cloud. </p> <p>Azure Database for PostgreSQL is available in three modes on Azure:</p> <ul> <li>Single Server: suitable for deployments that do not require low latency or extensive customisations</li> <li>Flexible Server: suitable for deplyments requiring zone-resilient high-availability, predictable performance, custom maintenance window, cost optimisation controls and simpler developer experience</li> <li>Hyperscale (Citus): delivers scale across multiple machines. Hyperscale can be deployed and managed on-prem, on the edge, or on multi-cloud environments using Azure Arc</li> </ul> <p>Each deployment option offers 99.99% uptime. Data is automatically encrypted and backed-up. Automatic Threat Detection makes it easier to mitigate threats. </p> <p>Administrative overheads are reduced with Azure providing optimised performance,  automated maintenance and updates for the hardware, OS and database engine and  automated management and analytics</p> <p>Hyperscale is based on a PostgreSQL extension called Citus and is recommended for data needs  exceeding 100GB. Hyperscale allows you to scale-out to multiple servers and the scaling process  is handled automatically in Azure. The servers can be globally distributed so as to be closer to  the users connecting to them</p> <p>Azure places the database engine in a compute container and the data files in Azure Storage.  Separating the database engine and the data facilitates automated maintenance. The compute container can be selected during deployment, and altered later allowing for simple scale-up or scale-down. </p> <p>Automated backups allow you to restore the data to any point in time within the last 35 days. Servers can be replicated on up to 5 read-only copies improving the performance of read-only workloads  such as analytics. Storage can be configured to 'auto-grow', ensuring there is always enough  storage available.</p>"},{"location":"databricks/fundamentals/","title":"Fundamentals of the Databricks Lakehouse Platform","text":""},{"location":"databricks/fundamentals/#what-is-a-data-lakehouse","title":"What is a Data Lakehouse","text":"<p>In the 1980s, Data Warehouses were developed to organise huge volumes  of data for analytics and BI. Data Warehouses are organised with  pre-defined schemas. </p> <p>Drawbacks for traditional data warehouses were:</p> <ul> <li>They failed to provide support for unstructured data </li> <li>they had long processing times</li> <li>they could not cope with the increasing velocity and volume of data from digital sources</li> </ul> <p>In the 2000s, Data Lakes were developed to cope with Big Data: handling structured,  semi-structured and unstructured data generated in huge volumes and high velocity. Multiple data types could be stored side-by-side in cloud object-stores. Built to handle  streaming data. Drawbacks of Data Lakes:</p> <ul> <li>do not support transactional data<ul> <li>lack of ACID transaction support</li> </ul> </li> <li>do not enforce data quality (data swamps)<ul> <li>lack of schema enforcement</li> </ul> </li> <li>slow analysis performance</li> <li>governance concerns</li> </ul> <p>The drawbacks led to companies developing Data Warehouses and Data Lakes side-by-side,  leading to complex, silo-ed environments involving copying data backwards and forwards between the specialist systems. </p> <p>The Data Lakehouse emerged to provide a platform to unify data, analytics and AI workloads.  Built on a Data Lake, the Data Lakehouse can handle all types of data, becoming a single source of truth. Data Lakehouses provide:</p> <ul> <li>Transaction support </li> <li>Schema enforcement and governance</li> <li>Data Governance</li> <li>BI Support</li> <li>Decouples storage from compute</li> <li>Open storage formats (e.g. Apache Parquet)</li> <li>Support for diverse data types</li> <li>Support for diverse workloads</li> <li>End-to-end streaming for real-time reports</li> </ul>"},{"location":"databricks/fundamentals/#what-is-a-data-lakehouse_1","title":"What is a Data Lakehouse","text":"<p>Databricks was founded in 2013 by the original founders of Apache Spark,  Delta Lake and MLflow. The Lakehouse platform was first proposed in 2021,  based on a paper entitled: 'Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics'. The Lakehouse  paradigm specifies:</p> <ul> <li>Platform for all data processing workloads (AI, ML, SQL Analytics, BI, and streaming)</li> <li>One security and governance approach for all data assets</li> <li>A reliable data platform for all data types</li> </ul> <p>This is met in Databricks through</p> <ul> <li>Delta Lake - for data reliability and performance</li> <li>Unity Catalog - for governance</li> <li>Persona-based use cases</li> </ul> <p>The Databricks Lakehouse Platform provides instant, serverless compute and is built on open standards and open source and is multicloud. </p>"},{"location":"databricks/fundamentals/#architecture-and-security","title":"Architecture and Security","text":""},{"location":"databricks/fundamentals/#data-reliability-and-performance","title":"Data Reliability and Performance","text":"<p>Databricks Lakehouse Platform solves the problems of Data Lakes using  two technologies: </p> <ul> <li>Delta Lake<ul> <li>file-based open source storage format</li> <li>provides ACID transaction guarantees</li> <li>scalable data and metadata handling (using Spark)</li> <li>audit history and time travel through a transaction log</li> <li>schema enforcement and schema evolution</li> <li>support for deletes, updates and merges</li> <li>unified streaming and batch data processing</li> <li>runs on top of existing Data Lake technologies</li> <li>uses Delta tables based on Apache Parquet</li> </ul> </li> <li>Photon<ul> <li>next-generation query engine</li> <li>supports Spark and SQL APIs but provides improved performance</li> </ul> </li> </ul>"},{"location":"databricks/fundamentals/#unified-governance-and-security","title":"Unified Governance and Security","text":"<p>Protecting against data breaches. Databricks Lakehouse Platform addresses the data and AI governance challenges through:</p> <ul> <li>Unity Catalog<ul> <li>unified governance solution for all data assets</li> <li>uses SQL to define and enforce fine-grained access controls on all data and AI assets on any cloud</li> <li>provides one consistent model to discover, access and share data</li> <li>provides single source of truth for all user identities and data assets</li> <li>access can be controlled by rows or columns and using attribute-based controls</li> <li>provides a detailed audit trail of who has performed what actions against the data</li> <li>provides an interface for data search and discovery</li> <li>provides data lineage and supports impact analysis for data changes</li> </ul> </li> <li>Delta Sharing<ul> <li>open solution to securely share live data to any computing platform</li> <li>data providers retain the ability to track and audit usage</li> <li>share data without copying it</li> <li>privacy-safe data clean-rooms</li> <li>REST protocol to share access to part of a cloud dataset</li> </ul> </li> <li>Divided Architecture<ul> <li>control plane<ul> <li>where the applications reside</li> <li>mangaged backend services that Databricks provides. Held in Databricks' own cloud account. </li> <li>Databricks runs the workspace application and manages notebooks, configuration and clusters</li> <li>Notebook, configuration and log data is encrypted at-rest and in-transit<ul> <li>User Identity and Access:<ul> <li>Table ACLs</li> <li>IAM instance profiles</li> <li>Securely stored access keys</li> <li>Secrets API</li> </ul> </li> </ul> </li> </ul> </li> <li>data plane<ul> <li>where the data and compute resources reside</li> <li>runs inside the business owners own cloud account</li> <li>clusters use the latest, hardened server images</li> <li>clusters are short-lived and often terminated after a job</li> </ul> </li> </ul> </li> <li>Databricks support staff require a support ticket to access a workspace<ul> <li>ticket is time-limited</li> <li>allows access to specific group of employees</li> </ul> </li> <li>Compliance on Multicloud<ul> <li>SOC 2 Type II</li> <li>ISO 27001</li> <li>ISO 27017</li> <li>ISO 27018</li> </ul> </li> <li>Additional compliance per cloud-provider<ul> <li>FedRAMP High</li> <li>HITRUST</li> <li>HIPPA</li> <li>PCI</li> </ul> </li> <li>GDPR and CCPA ready</li> </ul>"},{"location":"databricks/fundamentals/#instant-compute-and-serverless","title":"Instant Compute and Serverless","text":"<p>Configuring the data plane can be complicated and leads to over-provisioning of resouces and higher administration costs. Databricks offers serverless compute or serverless data plane which is available for Databricks SQL. Databricks Serverless SQL is provisioned and managed by Databricks in the Databricks cloud account. Resources are provisioned on-demand and destroyed when they are finished with. Serverless Compute relies on pre-configured  database clusters which are assigned to customers as needed and supports elastic scaling in response to demand. The compute has three layers of isolation:</p> <ul> <li>the container hosting the runtime</li> <li>the VM hosting the container</li> <li>the virtual network for the workspace</li> </ul> <p>When finished the VM is terminated and not reused. Instead a new VM is deployed and allocated to the pool. </p>"},{"location":"databricks/fundamentals/#lakehouse-data-management-terminology","title":"Lakehouse Data Management Terminology","text":"<p>Delta Lake </p> <ul> <li>provides a data storage format built for the Lakehouse and Unity Catalog. </li> </ul> <p>Unity Catalog</p> <ul> <li>provides a common governance model for data and AI assets</li> </ul> <p>Metastore</p> <ul> <li>top-level logical construct for organising data and associated metadata. Functions as a reference for a collection of metadata and a link to the cloud storage container.</li> </ul> <p>Catalog</p> <ul> <li>top-most container for data objects in Unity Catalog. Several Catalogs can exist in a Unity Catalog. Each Catalog represents the start of the data objects namespace, followed by the schema and tablename</li> </ul> <p>Schema</p> <ul> <li>acts as a container for data assets like tables, views and functions</li> </ul> <p>Tables</p> <ul> <li>defined by two distinct elements: metadata and data. Metadata here is the comments, tags and list of columns and data types. Tables can be either managed or external. External tables store their data in an external data store: with managed tables, data is stored in the Metastores defined storage location. </li> </ul> <p>Views</p> <ul> <li>read-only queries: are not able to modify the underlying data. </li> </ul> <p>Storage Credentials</p> <ul> <li>created by admins and used to authenticate to cloud storage containers</li> </ul> <p>External Location</p> <ul> <li>used to provide access control at the file level</li> </ul> <p>Shares and Recipients</p> <ul> <li>Used in Delta Sharing, an open source protocol for sharing data across organisations. Shares are read-only, logical collections of tables. </li> </ul>"},{"location":"databricks/fundamentals/#supported-workloads","title":"Supported Workloads","text":"<ul> <li>Data Warehousing <ul> <li>using Databricks SQL to support SQL Analytics and BI tasks (ETL, queries, dashboards, reporting). Data analysts can use the tools of their choice to interact with Databricks SQL. </li> </ul> </li> <li>Data Engineering <ul> <li>ingesting, cleaning and orchestrating (delivering) data. Building data pipelines. </li> <li>Auto Loader is an optimised data ingestion tool that processes new data files as they arrive in the lakehouse cloud storage. Auto detects schemas and enforces it. The <code>COPY INTO SQL</code> command uses the 'lake-first' approach, loading data from a folder into a Delta Lake tables. </li> <li>Delta Live Tables (DLT) uses a declarative syntax to build reliable data pipelines. DLT supports both SQL and Python and works with streaming and batch workloads. With DLT, engineers treat their data as code, and can use common practices from software engineering: separate dev and prod environments, test before deploying, using parameters to define environments, unit testing and documentation. </li> <li>Databricks workflows can be built in the UI, using the Databricks Workflows API or external orchestrators such as Apache Airflow. </li> </ul> </li> <li>Data Streaming<ul> <li>handling real-time data </li> <li>supports real-time analysis, real-time ML, real-time applications</li> </ul> </li> <li>Data Science and Machine Learning<ul> <li>Databricks ML Runtime<ul> <li>optimised and pre-configured ML Frameworks</li> <li>distributed ML</li> <li>built-in Auto-ML</li> <li>GPU support</li> </ul> </li> <li>MLflow is an open-sourced ML platform created by Databricks<ul> <li>track model training sessions</li> <li>package and re-use models</li> <li>feature store to create new features or re-use existing features, to train or score models</li> <li>serve models to production</li> </ul> </li> </ul> </li> </ul>"},{"location":"databricks/fundamentals/#acid-transations","title":"ACID transations","text":"<ul> <li>Atomicity     All changes to data are performed as if they are a single operation. That is, all the changes are performed, or none of them are.     For example, in an application that transfers funds from one account to another, the atomicity property ensures that, if a debit is made successfully from one account, the corresponding credit is made to the other account.</li> <li>Consistency     Data is in a consistent state when a transaction starts and when it ends.     For example, in an application that transfers funds from one account to another, the consistency property ensures that the total value of funds in both the accounts is the same at the start and end of each transaction.</li> <li>Isolation     The intermediate state of a transaction is invisible to other transactions. As a result, transactions that run concurrently appear to be serialized.     For example, in an application that transfers funds from one account to another, the isolation property ensures that another transaction sees the transferred funds in one account or the other, but not in both, nor in neither.</li> <li>Durability     After a transaction successfully completes, changes to data persist and are not undone, even in the event of a system failure.     For example, in an application that transfers funds from one account to another, the durability property ensures that the changes made to each account will not be reversed. </li> </ul>"},{"location":"databricks/overview/","title":"Azure Databricks","text":"<p>Azure Databricks Documentation</p> <p>The Azure Databricks Lakehouse Platform provides a unified set of tools for processing, storing and analysing data at scale.  To use Azure Databricks you  need to configure a workspace to integrate Azure Databricks infrastructure with your cloud account. Azure Databricks then deploys ephemeral  compute clusters using cloud resources in your account to process and  store data. </p> <p>The Azure Databricks workspace provides user interfaces for many core data tasks, such as:</p> <ul> <li>Interactive notebooks</li> <li>Workflows scheduler and manager</li> <li>SQL editor and dashboards</li> <li>Data ingestion and governance</li> <li>Data discovery, annotation and exploration</li> <li>Compute management</li> <li>ML experiment tracking</li> <li>ML model serving</li> <li>Feature store</li> <li>Source control with Git</li> </ul> <p>You can also interact with Databricks using CLI, Terraform or REST. </p> <p>Common use cases for Azure Databricks:</p> <ul> <li>Enterprise Data Lakehouse: combines data warehouse and data lake techniques for a unified enterprise data solution</li> <li>ETL and Data Engineering: combines Apache Spark with Delta Lake for ETL</li> <li>Machine Learning, AI and Data Science: expands core functionality with MLflow and Databricks Runtime for Machine Learning</li> <li>Data Warehousing, Analytics and BI: SQL warehousing can be queried using Notebooks supporting Python, R, Scala or SQL. </li> <li>Data Governance: using Unity Catalog to configure a unified data governance model. Delta Sharing can be leveraged for sharing outside of your secure environment. </li> <li>DevOps, CI/CD and Orchestration</li> <li>Real-time and Streaming Analytics: using Apache Spark</li> </ul>"},{"location":"databricks/overview/#databricks-concepts","title":"Databricks Concepts","text":"<ul> <li>Accounts and Workspaces<ul> <li>A Workspace is an Azure Databricks deployment in the cloud providing an environment for your team to access Databricks assets. An account is a single entity that can include multiple workspaces</li> </ul> </li> <li>Billing<ul> <li>based on DBUs (DataBricks Units), or processing capability per hour</li> </ul> </li> <li>Authentication and Authorisation<ul> <li>User: a unique individual represented by an email address</li> <li>Service Principal: a service identity for use with jobs, automated tools and systems</li> <li>Group: a collection of identities</li> <li>ACL: list of permissions attached to the workspace, cluster, job, table or experiment. Specifies which users are granted access to the object and the operations they are allowed to perform</li> <li>Personal Access Token: used to authenticate to the REST API and to connect to SQL warehouses</li> </ul> </li> <li>Databricks Data Science and Engineering<ul> <li>Notebook: web-based interface to run commands, create visualisations and narrative text</li> <li>Dashboard: collection of visualisations</li> <li>Library: a package of code</li> <li>Repo: folder synchronised to a remote Git repository</li> <li>Experiment: a collection of MLflow runs for training an ML model</li> <li>Interfaces: UI, REST API and a CLI</li> <li>Databricks File System (DBFS): a filesystem abstraction layer over a blob store</li> <li>Database: an organised collection of information</li> <li>Table: a representation of structured data</li> <li>Metastore: stores all the structural information of the various tables and partitions in the data warehouse</li> <li>Visualisation: a graphical representation of query results</li> <li>Cluster: a set of computation resources and configurations used to run notebooks and jobs. Job clusters are automatically created and destroyed by the job scheduler for each job run. All-purpose clusters can be manually created to allow sharing between multiple users.</li> <li>Pool: a set of ready-to-use instances that are used to reduce cluster start and auto-scaling times.</li> <li>Databricks runtime: a set of core components that run on the clusters. There are several types:<ul> <li>Databricks Runtime: includes Apache Spark</li> <li>Databricks Runtime for Machine Learning: built on the Datbricks Runtime and adds popular libraries such as TensorFlow, Keras, PyTorch and XGBoost</li> <li>Databricks Light: used for non-interactive job submission</li> </ul> </li> <li>Workflows: frameworks to develop and run data processing pipelines</li> <li>Workload<ul> <li>Data Engineering: an automated workload running on a job cluster</li> <li>Data Analytics: an interactive workload running on an all-purpose cluster</li> </ul> </li> <li>Execution Context: the state for a REPL (read-eval-print loop) environment (interactive shell) </li> </ul> </li> <li>Databrick Machine Learning<ul> <li>Experiments: main unit of organisation for tracking ML model develpment. Used to organise, display and control access to individual logged-runs of model training code</li> <li>Feature Store: a centralised repository of features. </li> <li>Models: a trained machine learning or deep learning model that has been registered in the Model Registry</li> </ul> </li> <li>Databricks SQL: geared towards data analysts who prefer to work with SQL queries and BI tools<ul> <li>REST API: an interface for task automation on Databricks SQL objects Dashboard: collection of visualisations Alert: notification of a threshold that was reached Query: a valid SQL statement SQL warehouse: computation resources used to execute SQL queries Query history: list of executed queries and their performance history</li> </ul> </li> </ul>"},{"location":"databricks/overview/#create-an-azure-databricks-workspace-with-azure-cli","title":"Create an Azure Databricks Workspace with Azure CLI","text":"<p>Sign-in to your Azure Subscription and install the Azure CLI extension for databricks:</p> <pre><code>az extension add --name databricks\n</code></pre> <p>Create the Azure Databricks workspace:</p> <pre><code>RG_NAME=databricks-quickstart\nLOCATION=uksouth\nWS_NAME=mydatabricksws\n\naz group create --name $RG_NAME --location $LOCATION\n\naz databricks workspace create \\\n    --resource-group $RG_NAME \\\n    --name $WS_NAME \\\n    --location $LOCATION \\\n    --sku standard\n</code></pre>"},{"location":"devops/cloudinit/","title":"Cloud Init","text":"<p>Cloud-init  provides tools for initialising compute resources deployed acoss various  cloud platforms. Cloud images typically come with cloud-init pre-installed and  configured to run on first boot. </p> <p>Cloud providers also provide an IMDS - Instance MetaData Service. The IMDS is  used to expose cloud-init configuration to the instance. During early boot,  cloud-init connects to the IMDS webserver to collect configuration.</p>"},{"location":"devops/cloudinit/#configuration-sources","title":"Configuration Sources","text":"<p>Cloud-init builds a single configuration from multiple sources:</p> <ol> <li>Base Config - consists of multiple sources (lowest to highest priority):     a. Hard Coded Config: Config that is part of cloud-init and cannot be changed     b. Configuration Directory: anything defined in <code>/etc/cloud/cloud.cfg</code> and <code>/etc/cloud/cloud.cfg.d/</code>     c. Runtime Config: anything defined in <code>/run/cloud-init/cloud.cfg</code>     d. Kernel Command Line: anything found between <code>cc</code> and <code>end_cc</code> on the kernel command line </li> <li>Vendor and User Data: provided by the datasource and added to the Base Config</li> <li>Network Configuration: occurs independantly of other cloud-init configuration</li> </ol> <p>End Users can pass configuration is specified as User Data. Distro Providers will modify the Base Config and Cloud Providers will modify Vendor Data.</p> <p>Cloud-init functionality is integrated into the five boot stages:</p> <ol> <li>Generator</li> <li>Local</li> <li>Network<ul> <li>also runs disk_setup and mounts modules, to configure disks, partitions and mount points. On Azure, this stage is used to setup mounts necessary for cloud-init to work. Additional disks configuration should be added after this stage completes</li> </ul> </li> <li>Config<ul> <li>runs config modules only</li> </ul> </li> <li>Final<ul> <li>runs as late in the boot process as possible. Typically this stage is for running commands that would otherwise be run by a user logging-in to the system to run the commands. This will include package installation and post-installation scripts. </li> </ul> </li> </ol>"},{"location":"devops/cloudinit/#determining-first-boot","title":"Determining First Boot","text":"<p>Cloud-init configuration is divided into per-instance and per-boot configuration. Per-instance configuration is only run on the first boot: per-boot configuration runs at every boot.  Cloud-init stores a cache for use across stages and boots. If the cache is already present,  then cloud-init has already run on this instance. </p> <p>However, a cache will exist if the instance is built from an image that has  previously been launched. To determine if the presence of a cache represents a first boot for an instance, cloud-init runs a check to compare the Instance ID in the  cache to the Instance ID found at runtime: if they do not match, then this is determined to be a first-boot for the instance.  This behaviour can be over-ridden by using a trust behaviour: which tells cloud-init not to compare the Instance ID and assume that this is not a  first boot if a cloud-init cache exists. Using the trust behaviour, cloud-init  per-instance configuration will only run if no cache is found. </p>"},{"location":"devops/cloudinit/#re-running-cloud-init-configuration","title":"Re-Running Cloud-Init Configuration","text":"<p>You can re-run the config and final stages of a cloud-init configuration by logging-on to the instance and executing:</p> <pre><code>sudo cloud-init clean --logs\nsudo cloud-init init --local\nsudo cloud-init init\nsudo cloud-init modules --mode=config\nsudo cloud-init modules --mode=final\n</code></pre>"},{"location":"devops/cloudinit/#user-data","title":"User Data","text":"<p>User Data can be provided in a variety of formats:</p> <ol> <li>Cloud Config: yaml format. Begins with either:<ul> <li><code>#cloud-config</code> or</li> <li><code>Content-Type: text/cloud-config</code></li> </ul> </li> <li>User Data: used to execute shell scripts. Begins with either:<ul> <li><code>#!</code> or </li> <li><code>Content-Type: text/x-shellscript</code></li> </ul> </li> <li>Gzip-compressed Data: user data is limited to 16,384 bytes. Gzipped data can be used where scripts would exceed these limits</li> <li>Mime Multipart File: Can be used to specify configuration using any combination of the other formats available. </li> <li>Include File: contains a list of URLs, one per line. Each URL is retrieved and processed in order. Begins with:<ul> <li><code>#include</code> or </li> <li><code>Content-Type: text/x-include-url</code></li> </ul> </li> <li>Cloud-boothook: boothook data is stored in /var/lib/cloud and executed immeadiately. Begins with <code>#cloud-boothook</code></li> <li>Part-handler: Python code. Begins with:<ul> <li><code>#part-handler</code> or </li> <li><code>Content-Type: text/part-handler</code></li> </ul> </li> </ol>"},{"location":"devops/cloudinit/#debugging-cloud-init","title":"Debugging Cloud-Init","text":"<p>To check the status of your cloud-init on a booted instance, run: </p> <pre><code>cloud-init status --wait\n</code></pre> <p>Cloud-init log files are kept in <code>/var/log/cloud-init.log</code>, <code>/var/log/cloud-init-output.log</code> and <code>/run/cloud-init</code>. Search <code>/var/log/cloud-init.log</code> for <code>part-00</code> to find records related to user cloud-init script messages. </p> <p>Cloud-init configuration files are found in <code>/etc/cloud/cloud.cfg</code> and <code>/etc/cloud/cloud.cfg.d/*.cfg</code>.</p> <p>The <code>/var/lib/cloud/instance</code> directory points to the most recently used instance-id directory and contains the the data received from the datasources including user and vendor data. User scripts are found at <code>/var/lib/cloud/instance/scripts/</code>. It also contains a datasource file with information about the datasource used. You can also run the <code>cloud-id</code> command to find out which datasource is being used. </p> <p>The <code>/var/lib/cloud/data</code> directory contains information from the previous boot. </p>"},{"location":"devops/devops/","title":"Overview","text":"<p>Notes on DevOps drawn mostly from Implementing Azure Devops Solutions, Henry Been, Maik van der Gaag, Packt Publishing, 2020<p></p>"},{"location":"devops/devops/#devops-defined","title":"Devops Defined","text":"<p>Traditionally development and operations teams were isolated from each other and worked towards different goals: developers add value to by delivering new features to products, requiring changes to infrastructure; operations teams add value by maintaining the stability of deployed infrastructure, requiring  strict controls on any changes being implemented to the deployed infrastructure DevOps removes this conflict of interest by creating teams that are each  responsible for both the development of new features and the delivery of these  features. DevOps teams consist of both developers and operations staff working  together on a common goal or product, to deliver new features to production  in a fast and reliable manner.  This closely aligns with Agile metholdology where the focus is on reducing the  time to delivery of new business value. The introduction of DevOps is often  associated with the adoption of Agile. When adopting DevOps teams will attempt to standarise on a common set of tools and standards for source control, development and architecture deployment. As the pipelines for infrastructure deployment mature, self-service capabilities become available where developers become able to create and destroy environments  on-demand through a published API. </p>"},{"location":"devops/devops/#key-metrics-for-devops","title":"Key Metrics for Devops","text":"<p>The key metrics for measuring the success of your DevOps teams are:</p> Lead Time The time between requesting a feature and starting the implementation of the feature Cycle Time The time between starting work on a feature and users being able to use the feature Time to Market The sum of Lead Time and Cycle Time Amount of Work in Progress DevOps should focus on the flow of value to the user. This implies that instead of switching from one incomplete task to another, each team member should be working on no more than one task at a time Mean Time To Recovery DevOps shifts the focus from Mean Time Between Failures to Mean Time To Recovery. Mean Time Between Failures encourages resistance to change. Less change means less failures. By focussing on Recovery Time, teams are not dis-incentivised to implement change Change Rate Increasing the number of implemented changes implies that you are delivering value more rapidly Change Failure Rate Focussing on the failure rate for changes encourages the implementation of smaller changes: increasing the Change Rate and reducing the likelihood of failure"},{"location":"devops/devops/#seven-pracitices-of-devops","title":"Seven Pracitices of Devops","text":"<p>The seven practices of DevOps are:</p> Configuration Management or Configuration As Code Configuration for the applcation and its components is stored as JSON/YAML in a version control repository and serves as input for deployment tools. Configuration describes the desired state of the application and should be applied regularly to ensure that there is no configuration drift. Re-applying configuration will revoke manual changes made outside of configuration management. Release Managment Release management is about controlling which versions of the software is deployed to which environments. Versions of the software and its configuration are stored as immutable artifacts in a repository and release management tools are used to manage the deployment of versions. Continuous Integration Continuouse integration requires each developer to integrate their work with that of every other developer at least once a day. A continuous integration build ensures that it compiles and passes unit tests. Frequent integrations ensures that changes are minor and less likely to result in problems. When problems arise they should be easier to fix. Continuous Deployment Continuous deployment is the automated deployment of each new version to prodution as long as it passes quality tests. The automated deployment pipeline involves deploying each version through a hierarchy of stages where it must pass the tests for that stage before proceeding to deployment to the next stage. If a release fails at any stage it is completely revoked. This ensures that any fixes to a broken release pass through all stages of testing before hitting production. Infrastructure As Code Infrastructure to run the application is also stored in a source control repository. Infrastructure code describes the desired state and should be idempotent: no matter how many times you deploy it, the result should be the same. To avoid configuration drift, the desired state for infrastructure should be applied regularly to each environment using continuous deployment pipelines and is often done before new software versions are released to the environments. Test Automation Continuous deployment involves frequent releases and therefore manual testing becomes impractical. Each deployment stage will require different types of tests that are triggered automatically when the release hits that stage. Manual testing should be reduced to the bare minimum, and should occur only at the final stages before deployment to production. Application Performance Monitoring Involves monitoring performance and usage of the software in production, to gather intelligence about the value of the features in each release."},{"location":"devops/devops/#seven-habits-of-devops","title":"Seven Habits of Devops","text":"<p>The seven habits of DevOps are:</p> Team Autonomy and Enterprise Alignment Each team should be working autonomously towards their current goal, however controls will need to be in place to ensure teams stay aligned to organisational goals. Rigourous Management of Technical Debt Technical debt is the cost associated with delay in addressing issues that affect the development pipelines. Fixing bugs and architectural issues should be prioritised and kept to a minimum. Focus on Flow of Customer Value Teams should be focussed on the delivery of features that are used by the customer. Hypothesis-driven Development Minimal implementations are released to production to determine whether they deliver actual value to the product based on usage patterns. If the hypothesis proves popular, the feature should be completed. Evidence Gathered In Production Performance measurements should be gathered from the production environment and compared to previous measurements. Live-Site Culture The production environment is prioritised above all others: issues or changes affecting production or release to production are tackled before all other tasks. Manage Infrastructure as a Flexible Resource Infrastructure is deployed when needed and discarded when no longer needed."},{"location":"devops/devops/#source-control","title":"Source Control","text":"<p>Source control can either be centralised or decentralised. In a centralised system, all changes and branches are stored in a single location. Developers only receive the latest version of the code when they checkout the repository. To view history or other branches, they have to access the central server. Centralised source control reduces the amount of storage that is required locally and can  allow for better access control on branches, directories and files. In a decentralised source control repository, each developer takes a full copy  of the repository by cloning a local copy. This means each developer can view the full history of changes on their local copy without being connected to the central server. Azure DevOps supports Git as a decentralised source control system. Git is  optimised for working with text files. If your project also needs to track  images or binary files then you will need to use Git LFS (Large File Storage). Further information on Git and Git LFS. It  is worth noting that Azure Repos do not support ssh in repositories with Git LFS  tracked files</p>"},{"location":"devops/devops/#branching-strategies","title":"Branching Strategies","text":"<p>There are three popular branching strategies:</p> GitHub Flow In GitHub Flow there is one main branch that is always in a deployable state. New features are started in a new branch and are not merged back to the main branch until complete. GitFlow GitFlow involves creating a branch from main called develop whenever you start work on new features. From develop, new branches are created for each new feature being developed. When the new feature is complete it is merged back into develop. When you want to release a new version of the application, you create a release branch from the develop branch. Once testing and bug fixes are applied to the release branch, this branch is then merged into main and tagged with the version number. Changes from testing the release are also merged back to the develop branch for the next round of development. Urgent bug fixes are carried out on a new branch from main, and are merged back to both main and develop branches when ready. Release Flow Involves working with short-lived topic branches just as with GitFlow. However, the main branch is never released to production. Instead when a new version is to be deployed, a new branch from the main branch is created named release-{version} and this is pushed to production. Urgent bug fixes can be merged from the main branch to the current release branch."},{"location":"devops/devops/#pull-requests","title":"Pull Requests","text":"<p>Pull requests are a way to ensure that a merge only occurs on a branch when the appropriate approval processes have been completed. Pull requests allow other team members to review the changes in the branch and add comments before the merge Once a pull request is approved there are three methods available for the  merge commit: - Merge Commit - retains full visibility of the changes on both the source and target branch. Commit history will reference both the source and target branches. - Squash Commit - all the changes from the source branch are combined into one new commit on the target branch - Rebase - changes from the source branch are temporarily stored elsewhere while changes from the master branch are applied to the source. Then the source branch has its own commits re-applied. Once the source is rebased, the commits can then be applied to the master branch. Rebasing retains all the change history on a single branch - the master branch</p>"},{"location":"devops/devops/#continuous-integration","title":"Continuous Integration","text":"<p>Continuous integration involves combining your changes with those of other developers on your project and testing whether the combined code works as  expected. Frequent code merges means that the changes are smaller and less likely to result in merge conflicts. Continuous integration adds value by testing the resulting integrated work for the team. Azure DevOps uses Azure Pipelines to create a continuous integration  build, with YAML pipelines being the preferred method. Storing your build  definitions in YAML, means that you can include this definition in the source  control for your project and it is then available on all branches and will be subject to the same policies applied to changing the code. After saving a YAML file in the repository, you can create a build definition from it. The default YAML pipeline file in Azure DevOps is called 'azure-pipelines.yml'.  The pipeline definition should begin with trigger actions that can be  monitoring either pull requests (pr) or commits (trigger):</p> <pre><code>pr:\n  branches:\n    include: [\"main\", \"release\", \"develop\"]\n\ntrigger:\n- main\n- release\n- develop\n- feature/*\n</code></pre> <p>Pipelines are configured to run on an agent using the pool block:</p> <pre><code>pool:\n  vmImage: ubuntu-18.04\n</code></pre> <p>Variables can be configured for later use in the pipeline:</p> <pre><code>variables:\n  folder_context: $(System.DefaultWorkingDirectory)/Terraform/environments\n  terraform_version: 1.1.7\n  is_main: $[eq(variables['Build.SourceBranch'], 'refs/heads/main')]\n  is_test: $[eq(variables['Build.SourceBranch'], 'refs/heads/release')]\n  is_dev: $[eq(variables['Build.SourceBranch'], 'refs/heads/develop')]\n  is_feature: $[startsWith(variables['Build.SourceBranch'], 'refs/heads/feature/')]\n</code></pre> <p>The rest of the pipeline will define the various stages to be executed.  Typically this will include:</p> <ol> - a build stage, where the new code is tested and - deployment stages, where the code is deployed to various environments </ol> <p>Each stage will define a series of jobs, with each job executing one or more steps:</p> <pre><code>stages:\n- stage: terraform_build\n  jobs:\n  - job: terraform_plan\n    steps:\n    - task: AzureKeyVault@1\n      inputs:\n        azureSubscription: $(subscription_id)\n        KeyVaultName: $(keyvault_name)\n        SecretsFilter: 'state-resource-group, state-storage-container, state-storage-account, state-file-name, state-sas-token, client-id, client-secret, subscription-id, tenant-id'\n      displayName: 'Get key vault secrets as pipeline variables'\n    - bash: |\n        ...\n        ---\n\n</code></pre> <p>Combining the above snippets will define a build pipeline that will be triggered by commits to main, develop,  release and any branch that begins with feature/ For IaC, the deployment stages should be executed conditionally: deployments should only occur to each environment when the corresponding source-code branch is updated:</p> <pre><code>- stage: development_env\n  condition: eq(variables.is_dev, 'true')\n  jobs:\n  - deployment: development_environment\n    displayName: development_environment\n    pool:\n      vmImage: Ubuntu-18.04\n    environment: dev\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - checkout: self\n          - task: AzureKeyVault@1\n            inputs:\n              azureSubscription: $(subscription_id)\n              KeyVaultName: $(keyvault_name)\n              SecretsFilter: 'state-resource-group, state-storage-container, state-storage-account, state-file-name, state-sas-token, client-id, client-secret, subscription-id, tenant-id'\n            displayName: 'Get key vault secrets as pipeline variables'\n\n          - bash: |\n            ...\n            ...\n\n</code></pre> <p>Adding the above code to our pipeline will mean that any pull request to the develop branch will result in both the build  stage (Continuous Integration) and the deployment to the development environment (Continuous Delivery) being executed Stages can be added for both the testing environment and the production environment to conditionally execute after pull  requests to their respective branches: release and main.  GitLab and Jenkins offer tools to build continuous integration pipelines See Create a Build Pipeline with Azure Pipelines and YAML Schema for Azure Pipelines</p>"},{"location":"devops/devops/#continuous-deployment","title":"Continuous Deployment","text":"<p>Continuous Deployment is not the same as Continuous Delivery. Continuous  Delivery is about building artefacts that are tested and ready to be deployed to production. Continuous Deployment, takes this one step further and actually includes deployment to production. In Azure Devops delivery and deployment are realised using releases. Release definitions are created from Pipelines and often begin with an artefact that  triggers deployment. Then the enviroment (or stage) to deploy to is identified. As with CI, YAML pipelines are the recommended means to define your CD  pipelines Continuous deployment could become challenging to users who have to deal  with the challenges of new releases, potential bugs and possible rollbacks. To  minimise the impact a suitable deployment strategy should be chosen. Potential deployment strategies are:</p> <ol> - Blue-Green Deployments. Two identical groups of servers are maintained: one to run the production instance and a second to act as the target for new deployments. A load balancer is used to direct users to the production environment. When a new release is deployed to the other group it is tested and when authorised as ready, the load balancer is used to switch users to the new group. The original environment can be used for rollback should problems occur or can be used as the target for the next release if no problems are found - Immutable Servers - a variation of blue-green deployments, where new servers are configured for each deployment and the original production environment is discarded after a grace period (in case rollback is required). - Canary deployments - initially only a selected group of users are exposed to the new environments, with further groups added if no problems are found - Ring-based deployments - several production environments are maintained for different groups of users. New deployments are issued to one group at a time until the release is propogated to all groups in the ring - Feature Flags - new versions are deployed but with the new features turned-off. Feature flags are used to slowly enable the new features. If the features passes quality acceptance, then the flag can be removed from the code base, and the feature becomes part of production environment - Fail forward - instead of rolling back when issues are encountered in new releases, bug fixes can be applied to fix the issues. </ol> <p>Further reading: Azure Pipeline Docuemntation</p>"},{"location":"devops/devops/#dependancy-management","title":"Dependancy Management","text":"<p>To reduce build times, shared libraries can be pre-built for re-use across  multiple projects. Azure Artifacts can be used to manage the library of  re-usuable components (packages and artifacts) across projects. Further reading:  Azure Artifacts</p>"},{"location":"devops/devops/#infrastructure-as-code","title":"Infrastructure As Code","text":"<p>Infrastructure As Code (IaC) and Configuration As Code (CaC) is one of the seven practices for DevOps. Using declarative templates for deploying your  infrastructure means that configuration and infrastructure can be managed in the same way that application code is managed. Several tools are available for  implementing IaC and CaC:</p> <ol> <li>ARM templates - Azure Resource Manager templates are JSON-like templates for declaring the desired state of your infrastructure, and can be deployed using Powershell, Azure CLI or Azure Pipelines     <li>Cloud Formation - the IaC tool for AWS     <li>Chef - a CaC tool for describing and enforcing server configuration     <li>Puppet - used for deployment and configuration of servers     <li>Ansible - IaC and CaC tool that can be used for both Linux and Windows servers     <li>Terraform - a multicloud infrastructure management tool"},{"location":"devops/devops/#databases","title":"Databases","text":"<p>Managing schema changes in a CI/CD enviroment is challenging if downtime is  to be minimised. ORMs are used to represent relational schemas as code: there are two common approaches to managing schema changes with ORMs: migration-based  and state-based. Migration-based approaches keep an ordered set of changes that need to be applied to the database to match the changes to ORM code. This becomes difficult when  code is being changed on multiple branches. Where multiple migrations are needed, they need to be applied in order to match the merge sequence State-based approaches ignore the individual changes (migrations) that are needed on a database and only focus on the changes required to fulfill the  desired end-state of the database. However, such an approach may additionally  require scripts to migrate the data as the schema is changed, involving data  dumps and insert actions. Both migrations and end-state approaches present challenges that need to be  carefully managed and may result in taking a non-DevOps approach to schema changes required by new releases. A Blue-Green approach might work, combined with ETL  processes to migrate data from old to new servers Using a schemaless database might be a suitable alternative to manage the  synchronisation of data and code. As new properties are added/removed from the objects in the schemaless database, code can be tweaked to handle objects in  both the old and new format, until all old records are migrated to the new format Entity Framework is the Microsoft ORM for .Net and supports migrations.</p>"},{"location":"devops/devops/#testing-to-maintain-quality","title":"Testing to Maintain Quality","text":"<p>Continuous Deployment requires continuous testing to ensure quality is  maintained or improved with each release. Common Metrics for measuring quality include:</p> <ul> <li>The percentage of integration builds that fail</li> <li>The percentage of code covered by automated tests</li> <li>The change failure rate</li> <li>The amount of unplanned work</li> <li>The number of defects that are being reported by users</li> <li>The number of known issues</li> <li>The amount of technical debt - the cost of sacrificing code quality for something else. As with financial debt, technical debt should be paid off as quickly as possible.</li> </ul> <p>Automated Functional Tests fall into one of three categories:</p> <ul> <li>Unit Tests - used to test the smallest possible scope in an application, e.g. a class or method</li> <li>Integration Tests - testing that multiple units that are supposed to work together</li> <li>System Tests - run against a fully assembled and running application Manual Functional Tests fall into one of two categories</li> <li>Scripted Test - formal test cases are prepared to test a new feature and a script is prepared of the actions to carry out the test. This script is then manually executed by the test team</li> <li>Exploratory Test - the test team explore the parts of the application they consider most at risk in the upcoming release. Records are kept of the risks that were checked and the issues that were found Non-Functional Tests include</li> <li>Performance testing - how quickly a particular task can be performed in the fully assembled system. Can be automated and results can be compared to results from previous versions of the system</li> <li>Load testing - measuring response times as the number of requests are increased. The breaking point is the moment that response time begins to rise exponentially</li> <li>Usability testing - collecting feedback from users regarding their experience using the new release (user stories)</li> </ul> <p>Different test types are executed at different stages in the CI/CD pipeline.  Unit tests should be executed at each commit and before a merge or pull request.  Integration tests should be done both before and after a merge or pull request. System tests and other testing will occur after deployment to a staging environment</p>"},{"location":"devops/git_commands/","title":"Getting a Repository","text":"<p>Turn an existing local directory into a git repository:</p> <p><code>git init</code></p> <p>Add a .gitignore to exclude files from tracking by Git Clone a repository with:</p> <p><code>git clone &lt;repository_url&gt;</code></p> <p>Clone takes a full copy of all the data the remote repository has.  To see all the branches of the repository, use:</p> <p><code>git branch -a</code></p>"},{"location":"devops/git_commands/#recording-changes","title":"Recording Changes","text":"<p>To start tracking changes to a file, use:</p> <p><code>git add &lt;filename&gt;</code></p> <p>To see what you've changed but not yet staged:</p> <p><code>git diff</code></p> <p>To see what you've staged but not yet committed:</p> <p><code>git diff --staged</code></p> <p>To unstage a file</p> <p><code>git reset HEAD &lt;filename&gt;</code></p> <p>Or</p> <p><code>git restore --staged &lt;filename&gt;</code></p> <p>Commit staged changes with a commit message:</p> <p><code>git commit -m \"Commit message\"</code></p> <p>To amend the previous commit, stage the additional files and then use:</p> <p><code>git commit --amend</code></p> <p>To discard changes to a file since the last commit:</p> <p><code>git checkout -- &lt;filename&gt;</code></p> <p>Newer version of above command</p> <p><code>git restore &lt;filename&gt;</code></p> <p>To remove a file from the staging area and your local directory:</p> <p><code>git rm &lt;filename&gt;</code></p> <p>To remove a file from the staging area but keep it in your local directory:</p> <p><code>git rm --cached &lt;filename&gt;</code></p> <p>To remove all files matching a pattern (the wildcard needs to be escaped with backslash):</p> <p><code>git rm data/\\*.csv</code></p> <p>To rename a file:</p> <p><code>git mv &lt;filename.old&gt; &lt;filename.new&gt;</code></p>"},{"location":"devops/git_commands/#viewing-changes","title":"Viewing Changes","text":"<p>To view the history of commits:</p> <p><code>git log</code></p> <p>To view the changes associated with the last two commits:</p> <p><code>git log -p -2</code></p> <p>To view changes for a particular file:</p> <p><code>git log -- &lt;path_to_file&gt;</code></p> <p>To view all commits for current branch with one line per commit:</p> <p><code>git log --oneline</code></p> <p>To view all commits for all branches with one line per commit:</p> <p><code>git log --oneline --all</code></p>"},{"location":"devops/git_commands/#working-with-remotes","title":"Working with Remotes","text":"<p>You can query the remote servers configured for your local copy with:</p> <p><code>git remote -v</code></p> <p>For a single remote you will receive two URLs: one for fetch and one for push After cloning a repository the default remote is named origin. You can add additional remotes:</p> <p><code>git remote add &lt;abbreviaion&gt; &lt;URL&gt;</code></p> <p>You can then fetch from the new remote:</p> <p><code>git fetch &lt;abbreviaion&gt;</code></p> <p>You can then checkout the new remote or merge it into one of your branches</p> <p><code>git checkout abbreviation/master</code></p> <p>The fetch command pulls all the data from the specified remote. <code>git fetch origin</code> will pull any new data from the origin since you last cloned it. Fetch will not  automatically merge the data with your own work. If your current branch is set up to track a  remote branch <code>git pull</code> will fetch and merge that branch to your current branch To push your changes use</p> <p><code>git push origin master</code></p> <p>This will not work if someone else has pushed to origin/master since you cloned it. You will first need to fetch their work and merge this into yours before you can push successfully To inspect the remote branch, use:</p> <p><code>git remote show origin</code></p> <p>The output of this command will show you:</p> <ul> <li>which local branches are tracking which remote branches</li> <li>which remote branches you don't currently have</li> <li>which local branches no longer exist on the remote (stale branches)</li> </ul> <p>Running <code>git fetch origin</code> will collect the remote branches that you don't currently  have and <code>git remote prune</code> or <code>git remote prune origin</code> to remove stale branches.</p>"},{"location":"devops/git_commands/#tagging","title":"Tagging","text":"<p>To add a tag to the current commit:</p> <p><code>git tag -a v0.1 -m \"Tag message\"</code></p> <p>Add a tag to a specific commit</p> <p><code>git tag -a v0.2 &lt;commit checksum&gt;</code></p> <p>List tags</p> <p><code>git tag</code></p> <p>Tags are not automatically transferred to the remote. They need to be explicitly mentioned:</p> <p><code>git push origin v0.2</code></p> <p>To push all tags:</p> <p><code>git push origin --tags</code></p> <p>Tags can be removed using:</p> <p><code>git tag -d v0.2</code></p> <p>To delete the tag from a remote use:</p> <p><code>git push origin --delete &lt;tagname&gt;</code></p> <p>Show the commit annotated with a specific tag:</p> <p><code>git show v0.1</code></p>"},{"location":"devops/git_commands/#branches","title":"Branches","text":"<p>Commits are pointers to the snapshots for the objects that had been staged. The commit also contains commit metatdata (author, email, committer) and a pointer to the previous commit. The pointer  points to the root tree for the commit, which contains pointers to the blobs stored with the commit. A branch in Git is simply a pointer to a commit. Each time you commit on a branch, the branch  pointer moves forward to the current commit. Creating a new branch, creates a new pointer to the  current commit. A special pointer called HEAD is maintained to represent the current  branch that you are on.  There are multiple ways to create and switch to a new branch. This can be done in two steps:</p> <pre><code>    git branch &lt;branchname&gt;\n    git checkout &lt;branchname&gt;\n</code></pre> <p>Or either of the following can be used:</p> <ul> <li><code>git checkout -b &lt;branchname&gt;</code></li> <li><code>git switch -c &lt;branchname&gt;</code></li> </ul> <p>To switch to an existing branch use either one of:</p> <ul> <li><code>git checkout &lt;branchname&gt;</code></li> <li><code>git switch &lt;branchname&gt;</code></li> </ul> <p>New branches will have their commit pointer pointing to the same commit as the branch it  was created from. When new commits are added to the branch, the pointer is moved on to a  new commit. To bring these commits into another branch you can use the merge subcommand</p> <pre><code>git switch -c feature123\n### edit and commit some files\ngit switch main\ngit merge feature123\n</code></pre> <p>Providing no changes have been made in main the git merge will create a fast-forward merge: where the commits from the merged branch are added to the commits on the main branch and the pointer from main is moved to the last commit from the merged branch.</p> <p>If the main branch has commits after the branch to be merged was created, the current branch of main is no longer a direct ancestor of the commits on the branch to be merged-in. In this case, Git creates a new snapshot that points to the latest commits on both branches and  incorporates the changes from both branches. This is known as a merge commit.</p> <p>Occaisionally, conflicts will occur during merge commits. This happens when both branches have made changes to the same file. In this case, <code>git merge</code> will report the conflict and the merge commit will be paused. Run <code>git status</code> to see which files need manual  attention. The file will contain the conflicting lines from both files. After the conflicts have  been resolved, the merge commit can be completed using <code>git add</code> followed by <code>git commit</code>. </p> <p>The <code>git branch</code> subcommand contains lots of useful options for managing your branches. The  <code>--merged</code> switch shows branches that have been merged into the current branch and  the <code>--no-merged</code> command shows branches that have not yet been merged in. To  delete a branch, use:</p> <p><code>git branch -d &lt;branchname&gt;</code></p> <p>If the branch to be deleted contains work that has not yet been merged in, the command will  fail. You can still force the delete using:</p> <p><code>git branch -D &lt;branchname&gt;</code></p> <p>Use the <code>--move</code> switch to rename a branch:</p> <p><code>git branch --move &lt;old_name&gt; &lt;new-name&gt;</code></p> <p>This will change the name of the local copy of the branch. To replicate this on the  server you will need to both push the new branch name and delete the old branch name on the  remote:</p> <pre><code>git push -u origin &lt;new-name&gt;\ngit push origin --delete &lt;old_name&gt;\n</code></pre> <p>Care should be taken when re-naming and deleting remote branches that may be in use or ancestors for other branches held locally elsewhere. Also, if pipelines depend on the branch to be renamed,  these scripts will also need to be adjusted for the new name Remote tracking branches are local references to the state of remote branches at the time that you last pulled them. To synchronise your remote tracking branches use:</p> <p><code>git fetch &lt;remotename&gt;</code></p> <p>When you want to share a local branch, you can push that up to the server:</p> <p><code>git push &lt;remotename&gt; &lt;branchname&gt;</code></p> <p>This will allow others to pull the branch from the server using:</p> <p><code>git fetch origin</code></p> <p>To create a local copy of the new branch, you will need to checkout the branch:</p> <p><code>git checkout -b &lt;branchname&gt; &lt;origin/branchname&gt;</code></p> <p>If the local branch does not already exists and exists on only one remote,  this command can be shortened to:</p> <p><code>git checkout &lt;branchname&gt;</code></p> <p>The <code>-b</code> option allows you to specify an alternative name for the tracking branch To see the difference between your local tracking branches and the remote branches at the time you last fetched them, use:</p> <p><code>git branch -vv</code></p> <p>To see the difference between your local tracking branches and the current state of the remotes,  you will need to fetch them first:</p> <pre><code>git fetch -all \ngit branch -vv\n</code></pre> <p>Fetching a remote tracking branch will not update your local working directory. You will need  to merge the remote tracking branch into your working directory manually:</p> <p><code>git merge &lt;origin/branchname&gt;</code></p> <p>Alternatively <code>git pull</code> does both a fetch and a merge</p>"},{"location":"devops/git_commands/#rebasing","title":"Rebasing","text":"<p>When two branches have divergent commit histories, then a merge commit will create a new  snapshot from the most recent commits on both branches and the most recent common ancestor commit for both branches. An alternative approach is the available using the <code>rebase</code> command.  Rebase will:</p> <ul> <li>stash away the divergent commits from the current branch</li> <li>rewind the current branch to the common ancestor for both branches</li> <li>apply the commits from the source branch</li> <li>re-apply the commits that were previously stashed away</li> </ul> <p>To rebase the feature1 branch to the main branch:</p> <pre><code>git checkout feature1\ngit rebase main\n</code></pre> <p>The feature1 branch will now have the commit history from main,  with the additional commits from feature1 appended. When you are ready to merge  feature1 back to main a fast-forward merge will suffice:</p> <pre><code>git checkout main\ngit merge feature1\n</code></pre> <p>Rebasing is typically used to where you want to cleanly apply your commits to a remote branch.  You can rebase your local branch to the remote branch and then create a pull request, so that when the request is approved, the new commits can be applied without fear of merge conflicts. Rebasing whilst keeping all the changes applied in a branch will result in new commits being  created when the original commits are re-applied. This could cause a problem if someone else  has created work based on your original branch. The simple lesson from all this is never rebase anything that you've pushed somewhere else.</p>"},{"location":"devops/git_commands/#reversing-changes","title":"Reversing Changes","text":"<p>HEAD is the symbolic name for the currently checked out commit. HEAD is normally attached to a branch but you can attach it to a commit:</p> <p><code>git checkout commit_hash</code></p> <p>Since commit hashes are awkward to work with, you can use relative refs to move around: </p> <ul> <li>Use the carat symbol to move up one commit at a time</li> <li>Use the tilde symbol followed by an integer to move upwards that integer number of time</li> </ul> <p>Thus you can move HEAD using relative refs. The following code moves HEAD back two commits before the $current_commit:</p> <pre><code>git checkout $current_commit\ngit checkout HEAD^\ngit checkout HEAD^\n</code></pre> <p>The same effect can be produced with:</p> <p><code>git checkout HEAD~2</code></p> <p>Instead of moving HEAD, you can reassign a branch to to a previous commit:</p> <p><code>git branch -f main HEAD~3</code></p> <p>This will move the main branch to three commits behind HEAD. Or you can move the branch forward to a specific commit hash:</p> <p><code>git branch -f main $specific_commit</code></p> <p>You can reverse changes by moving a branch reference backwards in time to an older commit:</p> <p><code>git reset HEAD~1</code></p> <p>'reset' works for local branches on your own machine. To reverse changes for remote branches  that others are using, use 'revert':</p> <p><code>git revert HEAD</code></p> <p>'revert' appends a new commit that has the effect of undoing the previous commit. This commit can be  pushed to the remote for others to pull</p>"},{"location":"devops/git_commands/#cherry-picking","title":"Cherry Picking","text":"<p>Git cherry-pick allows you to incorporate changes from another branch into the current branch:</p> <p><code>git cherry-pick commit_hash1, commit_hash3, ..., commit_hashN</code></p> <p>The above commands appends the selected commits to the commits on the current branch An interactive rebase allows you to reorder or omit commits from the current branch:</p> <p><code>git rebase -i HEAD~7</code></p> <p>The above command moves you back in the commit tree by seven commits and then allows you to reorder or  omit commits. You can even change the commit message if you wish</p>"},{"location":"devops/github_actions/","title":"Github Actions","text":"<p>GitHub Actions are the CI/CD platform for GitHub repositories, that allow you to trigger workflows  in response to events in your repositories.</p>"},{"location":"devops/github_actions/#github-action-components","title":"GitHub Action Components","text":"Workflows A workflow is a configurable automated process that will run one or more jobs. Workflows are defined in YAML files stored in the <code>.github/workflows</code> directory Events Events are specific activities in the repository that can trigger a workflow. These include: <ol><li>pull requests</li><li>opening an issue</li><li>commits</li></ol> Jobs A Job is a set of steps in a workflow that execute on the same runner. Each step is either a shell script or an action Actions An action is a custom application for GitHub Actions platform that performs a complex, repeatable task. The action can be defined in one of <ol><li>a public repository</li><li>the same repository as the workflow file</li><li>a published Docker container image</li></ol> Runners A runner is a server used to run workflows. Each runner can run a single job at a time"},{"location":"devops/github_actions/#example-workflow-definition","title":"Example Workflow Definition","text":"<p>Workflows are defined in YAML files</p> <pre><code>name: my-github-workflow\n\non: # define Events that will trigger the workflow\npush:\n    branches: [main, develop, release]\nschedule:\n    # 1am each night\n    - cron: \"0 1 * * *\"\n\njobs: \nrun-first-job:\n    name: Run the first job\n    runs-on: ubuntu:latest  \n    if: github.ref == 'refs/heads/main'\n    steps:\n    - name: Checkout\n        uses: actions/checkout@v3\n    - name: Install Node\n        uses: actions/setupnode@v3\n        with:\n        node-version: '14'\n    - name: Run the Hello World Action\n        uses: ./.github/actions/hello-world-action\n        with:\n        username: \"John Doe\"\n        secrets:\n        password: $&amp;#123;&amp;#123; secrets.JOHN_PASSWD &amp;#125;&amp;#125;\n    - name: Salutation\n        run: |\n        echo \"Thanks for running the workflow\"\n</code></pre>"},{"location":"devops/porter/","title":"Intro","text":"<p>Continue Here</p> <p>Porter is an  implementation of the Cloud Native Application Bundle specification. CNAB is used to create bundles that can be used to install your application,  together with required infrastructure and instance-specific configuration. </p> <p>A CNAB bundle is made up of three components:</p> <ul> <li>Application Images - typically docker images for your application</li> <li>Invocation Images - installer for the application</li> <li>Bundle Descriptor - metadata for the bundle: name, version, parameters, credentials, outputs</li> </ul> <p>The CNAB bundle therefore allows you to package your application with everything needed to install it. The bundle can be written to support air-gapped deployments: allowing installation  without access to the original network used to develop the bundle and without access to the internet. </p> <p>The bundle can be an OCI-compliant artifact and therefore published to a OCI registry, such as  Docker or Azure Container Registry</p>"},{"location":"devops/porter/#installation","title":"Installation","text":"<p>Install porter for the current user:</p> <pre><code>export VERSION=\"latest\"\ncurl -L https://cdn.porter.sh/$VERSION/install-linux.sh | bash\n</code></pre> <p>Then add <code>~/.porter</code> to your $PATH (e.g. by editing ~/.bashrc).</p>"},{"location":"devops/porter/#example-bundle","title":"Example Bundle","text":"<p>A porter bundle is defined in YAML. An example <code>porter.yaml</code> file  will contain sections for each aspect of the bundle:</p> <pre><code>schemaVersion: 1.0.0-alpha.1\nname: examples/porter-hello\nversion: 0.2.0\ndescription: \"An example Porter configuration\"\nregistry: ghcr.io/getporter\n\nparameters:\n  - name: name\n    type: string\n    default: porter\n    path: /cnab/app/foo/name.txt\n    source:\n      output: name\n\noutputs:\n  - name: name\n    path: /cnab/app/foo/name.txt\n\nmixins:\n  - exec\n\ninstall:\n  - exec:\n      description: \"Install Hello World\"\n      command: ./helpers.sh\n      arguments:\n        - install\n\nupgrade:\n  - exec:\n      description: \"World 2.0\"\n      command: ./helpers.sh\n      arguments:\n        - upgrade\n\nuninstall:\n  - exec:\n      description: \"Uninstall Hello World\"\n      command: ./helpers.sh\n      arguments:\n        - uninstall\n</code></pre>"},{"location":"devops/porter/#parameters","title":"Parameters","text":"<p>Bundles can be authored to accept parameters, allowing the installation to  be customised for each instance. Parameter values can be specified, in order of priority, using:</p> <ol> <li>Using the <code>--param</code> flag (or setting the parameters directly on an installation resource)</li> <li>Using the <code>--parameter-set</code> flag (or setting the parameter set directly on an installation resource)</li> <li>Re-using previous values from the last run of the bundle</li> <li>Defaults specified in the bundle YAML definition</li> </ol> <p>Use <code>porter explain</code> to review a bundles parameters.  Explain will also tell which Actions the parameter can be used in (e.g. install,  upgrade or uninstall). If the bundle has custom actions defined, explain will  also indicate if the parameter can be used with those Actions also.</p> <p>Parameters specified using the <code>--param</code> switch, expect a key-value pair:</p> <p><code>porter install --reference getporter/hello-llama:v0.1.1 --param name=\"Larry\"</code></p> <p>For more complicated or sensitive bundles, parameter sets can be defined to  provide the parameter values more securely and consistently. The  <code>porter parameters generate</code> command can be used to create a  parameter set:</p> <p><code>porter parameters generate hello-llama --reference getporter/hello-llama:v0.1.1</code></p> <p>The parameter set can be viewed with <code>porter parameters show</code>:</p> <p><code>porter parameters show hello-llama</code></p> <p>The parameter set can be specified with the <code>--parameter-set</code> switch:</p> <p><code>porter upgrade hello-llama --parameter-set hello-llama</code></p> <p>Because the <code>--param</code> flag takes precedence over <code>--parameter-set</code>,  you can use to the <code>--param</code> flag to override one or more of the values in a parameter set. </p> <p>Parameters values used during the last run of the bundle can be viewed with:</p> <p><code>porter installation show</code></p>"},{"location":"devops/porter/#credentials","title":"Credentials","text":"<p>Credentials required by a bundle can be specified in the the bundle using the <code>credentials</code> key and referenced in the actions section as <code>${bundle.credentials.CRED_NAME}</code></p> <p>Credential values can be sourced from environment variables, local files or from an external secret store (if using the <code>secrets</code> plugin). </p> <p>Generate credentials with <code>porter credentials generate</code>:</p> <p><code>porter credentials generate github --reference getporter/credentials-tutorial:v0.1.0</code></p> <p>Use <code>list</code> or <code>show</code> to see the credentials defined on your workstation</p> <p>The credential set can be specified with the <code>--credential-set</code> parameter:</p> <p><code>porter install --credential-set github --reference getporter/credentials-tutorial:v0.1.0</code></p> <p>Porter will re-use credentials from the previous run if no credential-set is  specified. Unlike parameters, once the installation has run, credentials are not stored in the installation history. </p> <p>Credentials and Sensitive Parameters should be stored in key vault  rather than environment variables or local files</p> <p>Mixins allow porter to work with additional tools such as Terraform, Azure, Kubernetes or Helm. The 'exec' mixin can be used to run custom scripts from a shell. Mixins are installed with:</p> <p><code>porter mixin install &lt;mixin_name&gt;</code></p> <p>You can begin to configure your Manifest using the <code>porter create</code> command. This will create a cnab directory and porter.yaml file in the current directory Once the Manifest is configured the <code>porter build</code> command is used to build the bundle. Metadata is used to build the Bundle Descriptor. The Invocation image is built from the Steps defined in the Manifest. Runtimes are added based on  specified mixins. Once the bundle is built it can be published to any OCI registry,  e.g. dockerhub or ACR. </p> <p>Plugins are installed with:</p> <p><code>porter plugin install &lt;mixin_name&gt;</code></p> <p>View information about a bundle with <code>porter explain</code>:</p> <p><code>porter explain --reference getporter/porter-hello:v0.1.0</code></p> <p>To install the bundle use <code>porter install</code>:</p> <p><code>porter install porter-hello --reference getporter/porter-hello:v0.1.0</code></p> <p>List bundle installations with <code>porter list</code>. To see information about an installation use <code>porter show</code>:</p> <p><code>porter show porter-hello</code></p> <p>You can upgrade an installation using <code>porter upgrade</code>:</p> <p><code>porter upgrade porter-hello --reference getporter/porter-hello:v0.1.1</code></p> <p>To clean up the resources installed from the bundle use <code>porter uninstall</code>:</p> <p><code>porter uninstall porter-hello</code></p>"},{"location":"devops/terraform/blocks/","title":"Terraform Blocks","text":"<p>HCL scripts are used to declare the desired state using a series of code blocks. Each block has a type and serves a specific purpose:</p>"},{"location":"devops/terraform/blocks/#terraform","title":"Terraform","text":"<p>The terraform block is used to configure the terraform version, which providers will be used and the backend for the configuration. Required providers must be specified in the terraform block. </p> <p>Provider sources can be listed as hostname/namespace/name.  If the provider is in the hashicorp namespace then only the name is needed. If the provider is  not listed in a registry, then terraform will attempt to find the provider in the local  terraform plugins directory: <code>~/.terraform.d/plugins/${host_name}/${namespace}/${type}/${version}/${target}</code></p> <pre><code>terraform {\n    required_providers {\n        azurerm = {\n            source = \"hashicorp/azurerm\"\n            version = \"&gt;= 2.86\"\n        }\n\n        random = {\n            source = \"hashicorp/random\"\n            version = \"3.1.0\"\n        }\n    }\n\n    required_version = \"&gt;= 1.1\"\n\n    backend azurerm {}\n}\n</code></pre> <p>The <code>required_version</code> specifies that only terraform binaries greater the v1.1  can run this configuration. The <code>required_providers</code> block specifies version 3.1.0 of the random provider and the latest version of azurerm provider that  is greater than 2.86. </p> <p>When you initialise a Terraform configuration, Terraform will create a <code>.terraform.lock.hcl</code> file in the current working directory to save the versions downloaded to satisfy the version requirements in the  required_providers block. If the <code>.terraform.lock.hcl</code> already exists in the directory, then Terraform will download the versions listed there,  as long as they satisfy the versions in the required_providers block. </p> <p>The <code>.terraform.lock.hcl</code> should be included in version control to ensure that Terraform uses the same versions of providers across your team. </p> <p>If you want to use a newer version of a provider from that listed in the lock file, you can execute <code>terraform init -upgrade</code> to download the  current latest versions that satisfy the provider version requirements.  The same command can also be used to downgrade the versions if you  have specified an earlier version in the required_providers block.</p>"},{"location":"devops/terraform/blocks/#provider","title":"Provider","text":"<p>Terraform Provider Registry</p> <p>Used to configure provider plugins:</p> <pre><code>provider \"azurerm\" {\n  features {}\n}\n</code></pre> <p>Sensitive values should be supplied as environment variables. The azurerm  provider will automatically use the following environment variables, if set:</p> <ul> <li>ARM_CLIENT_ID</li> <li>ARM_CLIENT_SECRET</li> <li>ARM_SUBSCRIPTION_ID</li> <li>ARM_TENANT_ID</li> </ul> <p>Alternatively, you can specify these values in the provider block:</p> <pre><code>provider \"azurerm\" {\n  client_id         = var.client_id\n  client_secret     = var.client_secret\n  subscription_id   = var.subscription_id\n  tenant_id         = var.tenant_id\n\n  features {}\n}\n</code></pre>"},{"location":"devops/terraform/blocks/#variable","title":"Variable","text":"<p>Used to accept external values as parameters to be used by all other blocks,  apart from the terraform block. See Variables</p>"},{"location":"devops/terraform/blocks/#resource","title":"Resource","text":"<p>Used to represent a resource instance in the remote infrastructure that will  be managed by terraform. The resource block defines the desired resource  configuration. </p> <p>Resource blocks declare a resource type and a name. Resource types always begin with the provider name followed by an underscore: <code>azurerm_</code> or <code>random_</code> or <code>azuread_</code> or some other provider. </p> <p>Each provider, provides a unique list of resources: the random provider currently provides: </p> <ul> <li>random_id</li> <li>random_integer</li> <li>random_password</li> <li>random_pet</li> <li>random_shuffle</li> <li>random_string</li> <li>random_uuid</li> </ul>"},{"location":"devops/terraform/blocks/#output","title":"Output","text":"<p>Terraform Outputs Documentation</p> <p>Used to export data about your resources. The data can be used to configure other parts of your infrastructure or as a data source for another Terraform workspace. </p> <p>Output values can be set directly from variables or as part of an expression:</p> <pre><code>output \"direction\" {\n  value = var.compass\n}\n\noutput \"region\" {\n  value = \"UK ${var.compass}\"\n}\n</code></pre> <p>Outputs should be defined with a 'description' attribute, to explain the intent and content of the output. Terraform stores output values in the state, and therefore the state needs to be applied before the output values are accessible. Use  <code>terraform output</code> to see the output values. To see a specific output, include its name: <code>terraform output region</code>. String values are retrieved in double-quotes: use the <code>-raw</code>  switch to retrieve the value without quotes: </p> <pre><code>curl $(terraform output -raw web_url)\n</code></pre> <p><code>terraform output</code> also supports a <code>-json</code> flag to retrieve data in JSON format. </p> <p>output blocks also support the 'sensitive' attribute, to prevent accidentally  displaying sensitive data on the console. Sensitive outputs are stored in plain text  in your state file. Terraform will also display sensitive outputs if queried directly by name or in JSON outputs. </p>"},{"location":"devops/terraform/blocks/#data","title":"Data","text":"<p>Terraform Data Sources Documentation</p> <p>A data block can be used to read information from existing resources. For  example, to retrieve secrets from a keyvault, the following data blocks can  be used:</p> <pre><code>data \"azurerm_key_vault\" \"projectkv\" {\n  name = var.keyvault_name\n  resource_group_name = var.keyvault_rg\n}\n\ndata \"azurerm_key_vault_secret\" \"admin_user\" {\n  name = var.administrator_login\n  key_vault_id = data.azurerm_key_vault.projectkv.id\n}\n\ndata \"azurerm_key_vault_secret\" \"admin_password\" {\n  name = var.administrator_password\n  key_vault_id = data.azurerm_key_vault.projectkv.id\n}\n\nresource \"azurerm_virtual_machine\" \"virtual_machine\" {\n  count = 3\n  ...\n  os_profile {\n    computer_name = ${var.machine_name}-${count.index}\n    admin_username = data.azurerm_key_vault_secret.admin_user.value\n    admin_password = data.azurerm_key_vault_secret.admin_password.value\n  }\n  ...\n}\n</code></pre> <p><code>azurerm_key_vault</code> and <code>azurerm_key_vault_secret</code> are data sources provided by the  azurerm provider. </p> <p>The <code>terraform_remote_state</code> data source can be used to access output data from another workspace: </p> <pre><code>data \"terraform_remote_state\" \"cvm\" {\n  backend = \"local\"\n\n  config = {\n        path = \"../vm-image-gallery/terraform.tfstate\"\n  }\n}\n</code></pre> <p>The above example uses a local backend to access state from the local filesystem. To  access remote state from Terraform cloud, set the backend to 'remote': </p> <pre><code>data \"terraform_remote_state\" \"lz\" {\n  backend = \"remote\"\n\n  config = {\n    organization = \"my-terracloud-org\"\n    workspaces = {\n      name = \"lz\"\n    }\n  }\n}\n</code></pre>"},{"location":"devops/terraform/blocks/#module","title":"Module","text":"<p>Used to include external modules in your scripts. See Modules</p>"},{"location":"devops/terraform/blocks/#local","title":"Local","text":"<p>Local Variables Documentation</p> <p>locals allow you to set a name for the value of an expression. Locals differ from variables,  in that they cannot be set directly from user input (for example using <code>-var</code> or <code>.tfvars</code> or <code>TF_VAR_</code> expressions). </p> <p>Locals can be used in much the same way as variables, but are prefixed with <code>local.</code> instead of  <code>var.</code>. </p> <p>Locals can be useful to ensure default tag values are applied to resources:</p> <pre><code>locals {\n  required_tags = {\n    project     = var.project_name,\n    environment = var.environment\n  }\n  tags = merge(var.resource_tags, local.required_tags)\n}\n</code></pre> <p>Now referring to <code>local.tags</code> will pick up the tags specified in <code>var.resource_tags</code> and additionally ensure that the <code>project</code> and <code>environment</code> tags are also included.</p>"},{"location":"devops/terraform/functions/","title":"Terraform Functions","text":"<p>Terraform Functions</p> <p>You can use the Terraform console to test out the various built-in functions  of the Terraform language</p>"},{"location":"devops/terraform/functions/#ternary-operator","title":"Ternary Operator","text":"<p>Terraform provides the traditional ternary operator '?:' to implement conditional logic within blocks:</p> <pre><code>variable \"user_machines\" = {\n  type = map(string)\n  default = {\n    \"vm_harry\" : \"rg-env-dev\",\n    \"vm_larry\" : \"rg-env-live\",\n    \"vm_barry\" : \"rg-env-test\"\n  }\n}\n\nresource azurerm_virtual_machine \"user_vm\" {\n  for_each = var.user_machines\n  name = each.key\n  resource_group_name = each.value\n  vm_size = (each.key == \"vm_larry\" ? \"Standard_DS1_v2\" : \"Standard_FS1_v2\")\n...\n...\n}\n</code></pre>"},{"location":"devops/terraform/functions/#merge","title":"Merge","text":"<p>The merge operator can be used to join two or more maps together:</p> <pre><code>locals = {\n  default_tags = {\n    \"cost center\" : \"Central 234\",\n    \"created_by\" : \"terraform\"\n  }\n}\n\nresource \"azurerm_virtual_machine\" \"virtual_machine\" {\n...\n  tags = merge(local.default_tags, var.custom_tags)\n...\n}\n</code></pre> <p>If the same key is defined in more than one map, the value from the right-most  map is retained</p>"},{"location":"devops/terraform/functions/#lookup","title":"Lookup","text":"<p>The lookup operator can be used to retrieve a single value from a map using its key:</p> <pre><code>lookup(map, key, default)\n</code></pre>"},{"location":"devops/terraform/functions/#file","title":"File","text":"<p>The <code>file()</code> function returns the content of the specified file</p> <pre><code>\nresource \"aws_instance\" \"web\" {\n...\n  user_data = file(\"init_user.sh\")\n...\n}\n</code></pre>"},{"location":"devops/terraform/functions/#csvdecode","title":"CSVDecode","text":"<p>The csvdecode() function can be used to convert the rows in a CSV file to elements in a list of maps. Use the file function to specify the file path:</p> <pre><code>locals {\n    users = csvdecode(file(\"${path.module}/users.csv\"))\n}\n</code></pre>"},{"location":"devops/terraform/functions/#for","title":"For","text":"<p>'For' expressions can be used to iterate over any collection type to output  another collection:</p> <pre><code>{ for name, location in var.vm_map: lower(name) =&gt; lower(location) }\n[ for name, location in var.vm_map: \"${name}-${location}\" ]\n</code></pre> <p>The new collection can be used to initialise a for_each iterator:</p> <pre><code>for_each = { for user in local.users :  user.first_name =&gt; user }\n</code></pre>"},{"location":"devops/terraform/functions/#format","title":"Format","text":"<p>The <code>format()</code> function can be used to generate strings, using a  format string and values:</p> <pre><code>user_principal_name = format(\n  \"%s%s-%s@%s\",\n  substr(lower(each.value.first_name), 0 , 1),\n  lower(each.value.last_name),\n  random_pet.suffix.id,\n  local.domain_name\n)\n</code></pre>"},{"location":"devops/terraform/intro/","title":"Terraform Overview","text":"<p>Terraform's configuration language  is used to declare resources representing infrastructure objects. Terraform has a core set of language features to read the configuration and create a dependancy graph. Plugins are used to add features to interact with upstream APIs. Plugins are either 'providers' or 'provisioners'.</p> <p>Provider plugins implement resources through basic CRUD operations. The Azure REST API exposes a client library that encapsulates the request-response  objects of the API. The Terraform Azure provider uses this  client library to connect to and invoke remote API endpoints for Azure. </p> <p>Terraform providers exist for a vast array of upstream APIs. The Terraform Registry provides a list of existing providers.  </p> <p>Terraform maintains a state file that is created the first time a Terraform configuration is  executed in a working directory. The state file is a JSON representation of the remote infrastructure and is used by Terraform to plan and execute the changes to be made to the  infrastructure when Terraform commands are run. </p> <p>Use <code>terraform state list</code> to list the objects in the state file. To inspect  a specific object use <code>terraform state show &lt;ID&gt;</code>. The ID is declared in the resource block, and is the result of joining the resource type and resource name with a '.'. </p> <pre><code>resource \"azurerm_virtual_machine\" \"my_vm\" {\n..\n}\n</code></pre> <p>In the above example, the resource_id is <code>azurerm_virtual_machine.my_vm</code>.</p>"},{"location":"devops/terraform/intro/#terraform-workflow","title":"Terraform Workflow","text":"<p>The general workflow for deploying infrastructure in Terraform is:</p> <ol> <li>Initialise the working directory: <code>terraform init</code></li> <li>Validate the scripts: <code>terraform validate</code></li> <li>Run the plan: <code>terraform plan</code></li> <li>Deploy the infrastructure: <code>terraform apply</code></li> <li>Destroy the infrastructure: <code>terraform destroy</code></li> </ol> <p>Terraform 'init' is executed in the directory containing the Terraform scripts  and will initialise the state file and download required providers and modules  defined by the scripts. Modules and providers are downloaded and stored in the  hidden '.terraform' folder in the working directory. Once you've initialised the state you can use <code>terraform console</code> to inspect variables and expressions in the  context of your configuration. </p> <p>Terraform 'validate' can be used to validate the syntax of the scripts in the  working directory: there is also a 'fmt' command available to apply best  practice formatting to the scripts</p> <p>The 'terraform plan' command is used to determine what changes will be  made when the scripts are applied. It compares the target infrastructure to the  current state file and then compares the declared configuration to the state  file. The difference between the two allows terraform to determine what changes need to be made when the configuration is applied and to output these changes as a plan.</p> <p>After reviewing the plan, 'terraform apply' can be used to apply the  desired configuration as defined in the scripts to the target infrastructure.  If the apply is successful, Terraform generates a state file which, by default,  is stored in the current working directory as 'terraform.tfstate'. Use  <code>terraform show</code> to see the full state file. When using <code>azuread</code> provider to create users, groups and assign the users to groups, you need to create  the users before you try to assign group membership. You can run  <code>terraform apply -target azuread_user.users</code> to create the users first,  before running <code>terraform apply</code> to create the groups and group  memberships.</p> <p>The 'terraform destroy' command can be used to remove resources from  environment and also from the state file. The '-target' option will allow  you to specify a specific resource to remove:</p> <pre><code>terraform destroy -target azurerm_virtual_machine.user_vm[1]\n</code></pre>"},{"location":"devops/terraform/meta/","title":"Terraform Meta-Arguments","text":""},{"location":"devops/terraform/meta/#count","title":"Count","text":"<p>The 'count' attribute can be added to any resource and the value of this  attribute determines the number of resource instances to be deployed. The  'count' attribute provides an 'index' property which can be use with  variable interpolation to assign unique names to each resource created. The  index begins at 0.</p> <pre><code>resource \"azurerm_virtual_machine\" \"user_vm\" {\n  count = 3\n  name = \"${var.vm_name}-0${count.index}\"\n...\n...\n}\n\noutput vm_ids {\n  value = azurerm_virtual_machine.user_vm[*].id\n}\n</code></pre> <p>Terraform stores the list of resources created as an array in the terraform state and the splatting operator (an asterisk) can be used to refer to this array as a  whole.</p> <p>The 'count' meta-element can also be used with a list variable, allowing more  tailored names to be given to the resources created:</p> <pre><code>variable \"vm_names\" {\n  type = list(string)\n  default = [\"vm_harry\", \"vm_larry\", \"vm_barry\"]\n}\n\nresource \"azurerm_virtual_machine\" \"user_vm\" {\n  count = length(var.vm_names)\n  name = \"${var.vm_names[count.index]}\"\n...\n...\n}\n\noutput vm_ids {\n  value = azurerm_virtual_machine.user_vm[*].id\n}\n</code></pre>"},{"location":"devops/terraform/meta/#for-each","title":"For Each","text":"<p>The 'for_each' meta element can be used to iterate a map or set  to provision multiple resources from a single  resource block. 'for_each' provides a 'each.key' and an 'each.value' attribute to further tailor the properties of the resources created:</p> <pre><code>variable \"user_machines\" = {\n  type = map(string)\n  default = {\n    \"vm_harry\" : \"rg-env-dev\",\n    \"vm_larry\" : \"rg-env-live\",\n    \"vm_barry\" : \"rg-env-test\"\n  }\n}\n\nresource azurerm_virtual_machine \"user_vm\" {\n  for_each = var.user_machines\n  name = each.key\n  resource_group_name = each.value\n...\n...\n}\n</code></pre> <p>The 'for_each' meta-element can also be used to iterate at the property level: instead of using for_each to define multiple instance of the same resource type, for_each can be used to define multiple instances of a property on a singe resource:</p> <pre><code>resource \"azurerm_app_service\" \"app_service\" {\n  name = var.app_service_name\n  location = azurerm_resource_group.resource_group.location\n  resource_group_name = azurerm_resource_group.resource_group.name\n  app_service_plan_id = azurerm_app_service_plan.app_service_plan.id\n\n  dynamic \"connection_string\" {\n    for_each = var.connection_strings\n      content {\n        name = connection_string.name\n        type = connection_string.type\n        value = connection_string.value\n      }\n  }\n\n  https_only = true\n}\n</code></pre> <p>Property iterations use the keyword 'dynamic' along with the name of the  property being iterated</p>"},{"location":"devops/terraform/meta/#depends-on","title":"Depends On","text":"<p>Terraform can identify resource dependancies automatically, and will create resources with no dependancies in parallel, whilst resources with dependancies are created sequentially. But where a resource depends on another resources  behaviour rather than it's data, we have to explicitly declare the dependancy. A  data dependancy is clear to see: where the value for an attribute is declared by reference to the value of an attribute of another resource. For example:</p> <pre><code>resource azurerm_virtual_machine \"my_database_server\" {\n...\n  resource_group_name = azurerm_resource_group.resource_group.name\n...\n}\n</code></pre> <p>Implicit dependancies must be declared as a list of references to other  resources, using a <code>depends_on</code> block:</p> <pre><code>resource_azurerm_virtual_machine \"my_app_server\" {\n...\n  depends_on = [\n      azurerm_virtual_machine.db_server.id,\n      azurerm_virtual_machine.controller.id\n  ]\n}\n</code></pre> <p>A <code>depends_on</code> block can be included in any resource or module block.</p> <p>Terraform creates a dependancy graph from the dependancy information to ensure that resources are created in the correct order. </p>"},{"location":"devops/terraform/meta/#provider","title":"Provider","text":"<p>Where multiple providers are being used in Terraform scripts, you can add a provider attribute to a resource to tell it to use the non-default provider</p>"},{"location":"devops/terraform/meta/#lifecycle","title":"Lifecycle","text":"<p>The 'lifecycle' meta-argument can be used to change terraform's default  behaviour when creating, updating or destroying resources</p>"},{"location":"devops/terraform/meta/#template-file","title":"Template File","text":"<p>The <code>template_file()</code> function allows you to interpolate values into a script file at resource creation time. A simple template file might contain references to variables such as <code>${group}</code> and <code>${user}</code>. The <code>template_file()</code> function allows you to specify the values for the variables:</p> <pre><code>\nuser_data = template_file(\"user_data.tftpl\", { group = var.user_group, user = var.user_name })\n</code></pre> <p>The <code>file()</code> function does not interpolate values into the files contents, but allows you to read a file into your configuration without modification:</p> <pre><code>resource \"aws_key_pair\" \"ssh_key\" {\n  key_name = \"ssh_key\"\n  public_key = file(\"ssh_key.pub\")\n}\n</code></pre>"},{"location":"devops/terraform/modules/","title":"Terraform Modules","text":"<p>Modules in Terraform are reusable libraries of code that can be combined  with a root configuration to define the required infrastructure. A module  is a folder of Terraform files with the definition and configuration of  resources but without a root configuration. Because modules lack a root  configuration they can be executed directly and have no associated state file. Modules need to be combined with a root configuration to deploy  infrastructure. Re-useable modules helps a development team to maintain  coding standards and policies with the team. A module folder will typically contain the definition of a group of  resouces that are logically related and have the same lifecycle.  Provisioning the module would result  in provisioning all the resources defined in the module. Build your modules in a dedicated folder e.g. 'modules &gt; resources'. Inside the modules\\resources folder you would create a separate folder for each module. Each module folder should contain the following files: </p> <ul> <li>dependencies.tf - defines the dependancies of the module (optional)</li> <li>main.tf - defines the resources supplied by the module</li> <li>outputs.tf - defines the outputs of the module</li> <li>variables.tf - defines the variables used by the module</li> <li>versions.tf - defines the versions for providers</li> </ul>"},{"location":"devops/terraform/modules/#example-resource-group-module","title":"Example Resource Group Module","text":"<p>Suppose we want to create a module to ensure that all the resource_groups  developed by a team are provisioned in the 'UK South' Azure region. We would  create a 'groups' folder for our module and add the following to our main.tf  script:</p> <pre><code>resource \"azurerm_resource_group\" \"resource_group\" {\n  name     = var.resource_group_name\n  location = 'UK South'\n\n  tags = var.resource_group_tags\n}\n</code></pre> <p>This script has the location for the resource group hard-coded, but also  has two variables for the name of the resource group and the tags to be applied. The variables should be defined in the variables.tf for the module:</p> <pre><code>variable \"resource_group_name\" { type = string }\nvariable \"resource_group_tags\" { type = map(string) }\n</code></pre> <p>The module should also provide some outputs that can be used both for testing the module and whilst using the module:</p> <pre><code>output \"complete_resource_group\" {\n  value = azurerm_resource_group.resource_group\n}\n\noutput \"resource_group_identifier\" {\n  value = azurerm_resource_group.resource_group.id\n}\n\noutput \"resource_group_name\" {\n  value = azurerm_resource_group.resource_group.name\n}\n\noutput \"resource_group_tags\" {\n  value = azurerm_resource_group.resource_group.tags\n}\n\noutput \"resource_group_location\" {\n  value = azurerm_resource_group.resource_group.location\n}\n</code></pre> <p>We can then define the version for the azurerm provider in our  versions.tf:</p> <pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"=2.51.0\"\n    }\n  }\n}\n</code></pre>"},{"location":"devops/terraform/modules/#testing-the-module","title":"Testing the Module","text":"<p>The simplest test that can be performed is:</p> <pre><code>terraform fmt -check\n</code></pre> <p>This will check that the basic syntax for your module files are correct. More sophisticated testing will require using the module in a test configuration.  Each module that you build in the modules\\resources folder should have a  corresponding folder in the modules\\fixtures folder. In the fixtures folder you  will define a root configuration that uses the module. Instead of using the  azurerm_resource_group module we'll use the groups module. Our main.tf file in  modules\\fixtures\\groups will look like this:</p> <pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n      version = \"=2.51.0\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\nmodule \"resource_group\" {\n  source = \"../../resources/groups\"\n  resource_group_name = var.resource_group_name\n  resource_group_tags = var.resource_group_tags\n}\n</code></pre> <p>Note that we have not specified a location for the resource group, but we have specified two variables for the resource group. We'll need to therefore  define these variables somewhere. This could be done in main.tf, but can also  be done in a variables.tf file in the modules\\fixtures\\groups folder:</p> <pre><code>variable resource_group_name { type = string }\nvariable resource_group_tags { type = map(string) }\n</code></pre> <p>We can also specify the desired values for these variables in a  'testing.auto.tfvars' file:</p> <pre><code>resource_group_name = \"rg-uks-testing-modules\"\nresource_group_tags = {\n  \"project_name\": \"deep dive terraform\",\n  \"created_by\": \"David Ramlakhan\",\n}\n</code></pre> <p>Because we are using the modules\\fixtures\\groups folder for testing our  module, we'll also want to access all of the outputs defined by the module. So, we need also to add an outputs.tf script:</p> <pre><code>output \"complete_resource_group\" {\n  value = module.resource_group.complete_resource_group\n}\n\noutput \"resource_group_identifier\" {\n  value = module.resource_group.resource_group_identifier\n}\n\noutput \"resource_group_name\" {\n  value = module.resource_group.resource_group_name\n}\n\noutput \"resource_group_tags\" {\n  value = module.resource_group.resource_group_tags\n}\n\noutput \"resource_group_location\" {\n  value = module.resource_group.resource_group_location\n}\n</code></pre> <p>Note that the ouput values all refer to the module outputs. With the main.tf,  variables.tf, outputs.tf and the testing.auto.tfvars file in place we can test our module using the usual terraform workflow:</p> <pre><code>terraform init\nterraform validate\nterraform plan\nterraform apply \nterraform destroy\n</code></pre> <p>If we later alter the scripts in the module, then we'd have to re-run terraform init to pick up the changes.</p>"},{"location":"devops/terraform/modules/#module-versioning","title":"Module Versioning","text":"<p>Based on this blog at Gruntwork When using the module in a configuration, the source for our modules can be  set to reference a github repository:</p> <pre><code>module \"resource_group\" {\n  source = \"github.com/&lt;owner&gt;/&lt;repo_name&gt;.git//modules/resources/groups\"\n  resource_group_name = var.resource_group_name\n  resource_group_tags = var.resource_group_tags\n}\n</code></pre> <p>Note that the double forward-slash is required to specify the path to the  module within the repository. If your modules are stored in a private repo, then it is easier to use ssh  instead of https to access the repo:</p> <pre><code>module \"resource_group\" {\n  source = \"git::git@github.com/&lt;owner&gt;/&lt;repo_name&gt;.git//modules/resources/groups\"\n  resource_group_name = var.resource_group_name\n  resource_group_tags = var.resource_group_tags\n}\n</code></pre> <p>You can tag the commits to simplify the process of using different versions  of the module in your different environments:</p> <pre><code>git tag -a \"v0.0.1\" -m \"First release of the groups module\"\ngit push --follow-tags\n# make some changes to the module...\ngit add .\ngit commit -m \"Made some experimental changes to groups module\"\ngit push origin master\n# now add a tag for the new version\ngit tag -a \"v0.0.2\" -m \"Second release of the groups module\"\ngit push --follow-tags\n</code></pre> <p>Now in your different environments you can reference the module version by using the tag. For v0.0.1, use:</p> <pre><code>source = \"git::git@github.com:/&lt;owner&gt;/&lt;repo_name&gt;//modules/resources/groups?ref=v0.0.1\"\n</code></pre> <p>For v0.0.2, use:</p> <pre><code>source = \"git::git@github.com:/&lt;owner&gt;/&lt;repo_name&gt;//modules/resources/groups?ref=v0.0.2\"\n</code></pre>"},{"location":"devops/terraform/provisioner/","title":"Provisioners","text":"<p>Terraform allows 'provisioner' blocks to be added to a resources attributes  to enable execution of custom scripts or to copy files to the resource. There are three built-in provisioner types:</p> <ul> <li>local-exec - used to execute scripts from the local machine</li> <li>file - used to copy files to the deployed resource</li> <li>remote-exec - used to execute scripts on the remote resource</li> </ul> <p>The 'local-exec' scripts has one mandatory attribute (command) and three optional attributes (working_dir, interpreter, environment, and when).  Terraform also provides a 'null_resource' that can be used to execute a  local script without associating the script execution to a specific resource:</p> <pre><code>resource \"azurerm_resource_group\" \"resource_group\" {\n  name = var.resource_group_name\n  location = var.location\n\n  provisioner \"local-exec\" {\n    command = \"echo ${azurerm_resource_group.resource_group.id}\"\n  }\n}\n\nresource \"null_resource\" \"my_web_app\" {\n  provisioner \"local-exec\" {\n    command = \"catalyst.pl\"\n    working_dir = \"./bin\"\n    interpreter = [\"perl\", \"-wc\"]\n    environment = {\n      DEBUG = 1\n      PORT = 8080\n    }\n  }\n}\n</code></pre> <p>The local-exec provisioner also provides a 'when' attribute. If the value is set to 'destroy' the command is only executed when 'terraform destroy' is  called. The 'file' provisioner is used to copy folders and files to the target  resource:</p> <pre><code>provisioner \"file\" {\n  source = \"./resources/*.xml\"\n  destination = \"/app/resources/\"\n}\n</code></pre> <p>The 'remote-exec' provisioner is used to execute scripts and commands on  remote resources</p> <pre><code>provisioner \"remote-exec\" {\n  inline = [\n    \"apt-get update\",\n    \"apt-get install -y postgres\"\n  ]\n}\n</code></pre>"},{"location":"devops/terraform/state/","title":"Remote State","text":"<p>Terraform documentation on Terraform Backend</p> <p>Microsoft documentation on Remote State Storage for terraform</p> <p>Here's another good article by Matthew Davis which contains the info to use an SAS token for your backend</p>"},{"location":"devops/terraform/state/#create-azure-storage","title":"Create Azure Storage","text":"<p>Storing Terraform state remotely allows multiple developers to work on the same environment from different machines. Terraform state can be stored in Azure and then referenced in the backend configuration of your Terraform scripts. Public access is allowed to Azure storage to store Terraform state. First create the remote state storage account in your subscription using the  Azure CLI:</p> <pre><code>    az login\n    az account set --subscription &lt;subscription_id&gt;\n\n    export RESOURCE_GROUP_NAME=&lt;resource-group-name&gt;\n    export STORAGE_ACCOUNT_NAME=&lt;storage-account-name&gt;\n    export CONTAINER_NAME=&lt;container-name&gt;\n\n    az group create --name $RESOURCE_GROUP_NAME --location uksouth\n\n    az storage account create --resource-group $RESOURCE_GROUP_NAME --name $STORAGE_ACCOUNT_NAME --sku Standard_LRS --encryption-services blob\n\n    az storage container create --name $CONTAINER_NAME --account-name $STORAGE_ACCOUNT_NAME\n</code></pre>"},{"location":"devops/terraform/state/#configure-backend","title":"Configure Backend","text":"<p>To use remote state in Azure Storage, configure the backend block of your terraform block:</p> <pre><code>    terraform {\n        required_version = \"~&gt;v1.1.5\"\n        required_providers {\n            azurerm = {\n            version = \"~&gt;2.36\"\n            source  = \"hashicorp/azurerm\"\n            }\n        }\n        backend \"azurerm\" {\n            resource_group_name  = \"&lt;resource-group-name&gt;\"\n            storage_account_name = \"&lt;storage-account-name&gt;\"\n            container_name       = \"&lt;container-name&gt;\"\n            key                  = \"project1.terraform.tfstate\"\n        }\n    }\n</code></pre> <p>When you run <code>terraform init</code>, terraform will need to authenticate to Azure to create the backend. If you have already logged-in with Azure CLI, terraform will use this authenticated  connection to create the backend for you. If you're deploying your infrastructure using Terraform  Cloud or Azure DevOps you can create an SAS token to authenticate to the backend Storage Account. To obtain SAS certificate for a Storage Account, you can use the following Azure CLI commands:</p> <pre><code>ACCOUNT_KEY=$(az storage account keys list --resource-group $RESOURCE_GROUP_NAME --account-name $STORAGE_ACCOUNT_NAME --query '[0].value' -o tsv)\nEND_DATE=`date -u -d \"1 year\" '+%Y-%m-%dT%H:%MZ'`\nSAS_KEY=$(az storage container generate-sas -n $CONTAINER_NAME --account-key $ACCOUNT_KEY --account-name $STORAGE_ACCOUNT_NAME --https-only --permissions dlrw --expiry $END_DATE -o tsv)\nARM_SAS_TOKEN=$SAS_KEY\n</code></pre> <p>Terraform will automatically use the SAS key if you assign this to the environment variable  $ARM_SAS_TOKEN, or you can add the value to your backend config using 'sas_token':</p> <pre><code>  backend \"azurerm\" {\n    resource_group_name  = \"&lt;resource-group-name&gt;\"\n    storage_account_name = \"&lt;storage-account-name&gt;\"\n    container_name       = \"&lt;container-name&gt;\"\n    key                  = \"project1.terraform.tfstate\"\n    sas_token        = var.sas_key_value\n  }\n</code></pre> <p>Alternatively you could use an access key for the Storage Account to authenticate the backend. This can also be done using an environment variable:</p> <pre><code>ACCOUNT_KEY=$(az storage account keys list --resource-group $RESOURCE_GROUP_NAME --account-name $STORAGE_ACCOUNT_NAME --query '[0].value' -o tsv)\nexport ARM_ACCESS_KEY=$ACCOUNT_KEY\n</code></pre> <p>The SAS token is the better option since this gives access to the Storage Blob for the backend  which is what is needed in this case: the Storage Account Key gives access to other resources  in the storage account. Either way, the access key should be stored and retrieved from a key vault for added security:</p> <pre><code>export ARM_SAS_TOKEN=$(az keyvault secret show --name terraform-backend-key --vault-name myKeyVault --query value -o tsv)\n</code></pre>"},{"location":"devops/terraform/state/#using-backend-config","title":"Using Backend Config","text":"<p>Rather than placing the values for the backend-config keys in your  Terraform scripts you can store these values in a config file and use the '-backend-config' option in during terraform initialisation to use those values:</p> <pre><code>#### BEGIN /app/backend_config.conf #######\nresource_group_name = \"\"\nstorage_account_name = \"\"\ncontainer_name = \"\"\nkey = \"\"\nsas_token = \"\"\n#### END /app/backend_config.conf   #######\n</code></pre> <pre><code>terraform init --backend-config=/app/backend_config.conf\n</code></pre> <p>The value for sas_token can be omitted from the backend-config file, and  instead supplied using the ARM_SAS_TOKEN environment variable.</p>"},{"location":"devops/terraform/state/#inspecting-state","title":"Inspecting State","text":"<p>Whether you use a remote or local state storage, you can inspect the configuration of  individual items using <code>terraform state show</code>. For instance if you have defined  a resource group in your config files as <code>azurerm_resource_group.rg</code>, you can  query the configuration using:</p> <pre><code>terraform state show azurerm_resource_group.rg\n</code></pre>"},{"location":"devops/terraform/state/#importing-resources","title":"Importing Resources","text":"<p>You can import resources into your state file using  terraform import.  However you will also need to create the configuration file for the  imported resource. You can automate the generation of the configuration  file using configuration-driven import. Create an import.tf, e.g.:</p> <pre><code>import {\n  id = \"resource_id\"\n  to = \"azurerm_linux_virtual_machine.my_vm\"\n}\n</code></pre> <p>You can then run <code>terraform plan</code> to generate a corresponding configuration file:</p> <pre><code>terraform plan -generate-config-out=generated.tf\n</code></pre>"},{"location":"devops/terraform/state/#renaming-resources","title":"Renaming Resources","text":"<p>Sometimes you might want to rename a resource in your state file without  causing the resource to be re-deployed. For instance if you originally gave your azurerm_resource_group a specific name, e.g. 'project_x', but now just want to call it 'rg'. You can do this by first creating a copy of the  resource block and then using the <code>terraform state mv</code> command. Once  removed, you can then delete the original block and update any references to point to the new resource name: </p> <pre><code>resource \"azurerm_resource_group\" \"project_x\" {\n    name     = var.rg_name\n    location = var.location\n\n    tags     = var.default_tags\n}    \n\nresource \"azurerm_resource_group\" \"rg\" {\n    name     = var.rg_name\n    location = var.location\n\n    tags     = var.default_tags\n}    \n</code></pre> <p>Use <code>terraform state mv azurerm_resource_group.project_x azurerm_resource_group.rg</code> to update the state without changing the deployed resources. Before running <code>terraform plan</code>, remove the <code>azurerm_resource_group.project_x</code> block and update any references to it from other resources to point to <code>azurerm_resource_group.rg</code>.</p>"},{"location":"devops/terraform/variables/","title":"Terraform Variables","text":"<p>Input Variables Documentation</p> <p>Terraform variable blocks allow an existing configuration to be tailored  using external input values. A variable block starts with the keyword 'variable'  followed by the variable name and a block of attributes associated to the variable. These attributes include:</p> <ul> <li>type (\"string\", \"bool\", \"number\", \"object\", \"map\", \"list\", \"tuple\", \"sets\", \"any\")</li> <li>default: a default value to use if none is supplied</li> <li>description: a text string explaining the use of the variable</li> <li>validation: a block with code used to validate the value supplied for the variable</li> <li>sensitive: a boolean value to determine if the variable value is displayed by the Terraform CLI</li> </ul> <p>Variables and their default values can be declared anywhere in your configuration.  However, it is preferable to create a separate 'variables.tf' so that others will know where to look to customise the configuration.</p> <p>Variables values can be assigned on the command-line:</p> <p><code>terraform plan -var azure_region=\"UK South\"</code></p> <p>This becomes impractical if you want to override several default values. Instead you can  additionally create a 'variable definitions' file. The variable definitions file should consist of only variable assignments:</p> <pre><code>azure_region = \"West Europe\"\nlist_of_fruit = [\"mango\", \"pineapple\", \"blueberry\", \"grape\"]\nres_tags = {\n    location   = \"UK South\"\n    department = \"Accounts\"\n    owner      = \"Freda Perry\"\n}\n</code></pre> <p>Terraform will automatically load variable defintion files named either 'terraform.tfvars' or  files ending in '.auto.tfvars'. If you prefer to use JSON format for your variable definition files,  Terraform will also load any files named 'terraform.tfvars.json' or ending in 'auto.tfvars.json'. </p> <pre><code>{\n  \"azure_region\" : \"West Europe\",\n  \"list_of_fruit\" : [\"mango\", \"pineapple\", \"blueberry\", \"grape\"],\n  \"res_tags\" : {\n    \"location\"   : \"UK South\",\n    \"department\" : \"Accounts\",\n    \"owner\"      : \"Freda Perry\"\n  }\n}\n</code></pre> <p>Variables can also assigned in environment variables by prefixing the variable name with \"TF_VAR_\":</p> <pre><code>export TF_VAR_location=\"UK South\"\n</code></pre> <p>The corresponding variable \"location\" will need to be declared in your scripts to be accessible:</p> <pre><code>variable \"location\" {\n  default = \"\"\n}\n\noutput \"region\" {\n  value = var.location\n}\n</code></pre>"},{"location":"devops/terraform/variables/#string-variables","title":"String Variables","text":"<p>Values for string variables are assigned with double-quoted strings:</p> <pre><code>variable \"azure_region\" {\n  type        = string\n  default     = \"UK West\"\n  description = \"The Azure region to deploy resources\"\n}\n\noutput \"az_region\" {\n  value = var.azure_region\n}\n</code></pre> <p>Terraform supports string interpolation to allow the generation of strings by combining variable values and string values. String interpolation expressions  are declared within a double-quoted string and references to variable values  or other resources are enclosed within '${}':</p> <pre><code>name = \"vm-${var.user_name}-${var.res_tags[\"department\"]}\"\n</code></pre> <p>Terraform also supports 'String Directives' which allow the generation of  strings using loops or conditional statements. String directives are declared within double-quoted strings and the control statements are enclosed within  '%{}':</p> <pre><code>output \"users_created\" {\n  value = \"%{ for val in var.user_names }${val} %{ endfor }\"\n}\n</code></pre> <p>Note that the string directive will output everything between the '%{}' blocks: so that the space after '${val}' is also part of the output in the above example</p> <p>String directives also support 'if' statements and HEREDOCS:</p> <pre><code>enable_public_ip = &lt;&lt;EOT\n%{ if var.location == \"UK South\" }\ntrue\n%{ else }\nfalse\n%{ endif }\nEOT\n</code></pre>"},{"location":"devops/terraform/variables/#bool-variables","title":"Bool variables","text":"<p>Boolean values can be stored in variables and used to toggle a feature in your  configuration:</p> <pre><code>variable \"enable_ultra_ssd\" {\n    type    = bool\n    default = false\n    description = \"Should UltraSSD_LRS disks be available\"\n}\n\nresource \"azurerm_virtual_machine\" \"my_vm\" {\n    ...\n    ultra_ssd_enabled = var.enable_ultra_ssd\n    ...\n}\n</code></pre>"},{"location":"devops/terraform/variables/#list-variables","title":"List Variables","text":"<p>Lists are assigned as comma-separated lists inside square brackets:</p> <pre><code>variable \"private_subnet_cidr_blocks\" {\n    description = \"Available cidr blocks for private subnets\"\n    type = list(string)\n    default = [\n        \"172.30.1.0/24\",\n        \"172.30.2.0/24\",\n        \"172.30.3.0/24\",\n        \"172.30.4.0/24\",\n        \"172.30.5.0/24\",\n        \"172.30.6.0/24\",\n    ]\n}\n\nvariable \"number_of_subnets_required\" {\n    description = \"Number of subnets required\"\n    type    = number\n    default = 2\n}\n</code></pre> <p>Values can be retrieved from list variables by index or using the slice() function:</p> <pre><code>\noutput \"third_private_subnet_block\" {\n    value = var.private_subnet_cidr_blocks[2]\n}\n\noutput \"first_three_subnets\" {\n    value = slice(var.private_subnet_cidr_blocks, 0, 3)\n}\n\noutput \"selected_subnets\" {\n    value = slice(var.private_subnet_cidr_blocks, 0, var.number_of_subnets_required)\n}\n</code></pre>"},{"location":"devops/terraform/variables/#map-variables","title":"Map Variables","text":"<p>Map variables are collection types, and contain key-value pairs. The values should adhere to the  declared datatype. Each key should be unique, where the keys and values are of type string:</p> <pre><code>variable \"res_tags\" {\n  type       = map(string)\n  default = {\n    location   = \"UK West\"\n    department = \"Finance\"\n    owner      = \"Fred Perry\"\n  }\n}\n\noutput \"this_department\" {\n  value = var.res_tags[\"department\"]\n}\n</code></pre>"},{"location":"devops/terraform/variables/#object-variables","title":"Object Variables","text":"<p>Object variables are collection types, with key-value pairs where the values for each key can  be of different types:</p> <pre><code>variable \"user_account\" {\n  type = object({\n    logon    = string\n    email    = string\n    roles    = list(string)\n    projects = map(string)\n    enabled  = bool\n  })\n  default = {\n    logon = \"fred001\"\n    email = \"fred@example.com\"\n    roles = [\"user\", \"report writer\", \"remote access\", \"acr pull\"]\n    projects = {\n      primary = \"solaris\"\n      second  = \"aix\"\n      third   = \"rhel\"\n    }\n    enabled = true\n  }\n}\n\noutput \"user_details\" {\n  value = var.user_account.logon\n}\n\noutput \"main_project\" {\n  value = var.user_account.projects.primary\n}\n</code></pre>"},{"location":"devops/terraform/variables/#set-variables","title":"Set Variables","text":"<p>Sets are collections that can contain only unique values of the same datatype:</p> <pre><code># Although the value \"two\" is included twice in the default assignment\n# the value \"two\" will only occur once in the actual set variable\nvariable \"myset\" {\n  type    = set(string)\n  default = [\"one\", \"two\", \"three\", \"four\", \"two\"]\n}\n\noutput \"set_members\" {\n  value = var.myset\n}\n</code></pre> <p>Elements of a set are identified only by their value and don't have any index or key to select with, so it is only possible to perform operations across all elements of the set</p>"},{"location":"devops/terraform/variables/#tuple-variables","title":"Tuple Variables","text":"<p>Tuples are similar to lists but each value in the list can be of a different type:</p> <pre><code>variable \"user_info\" {\n  type    = tuple([string, bool, number])\n  default = [\"Finance\", true, 1]\n}\n\noutput \"tuple_values\" {\n  value = var.user_info\n}\n\noutput \"user_enabled\" {\n  value = var.user_info[1]\n}\n</code></pre>"},{"location":"devops/terraform/variables/#variable-validation","title":"Variable Validation","text":"<p>Validation blocks can be added to variable declaration to enforce any required rules and display error messages if rules are broken. Functions like <code>length()</code> and <code>regexall()</code> can be  useful for string validation. </p> <pre><code>\nvariable \"res_tags\" {\n...\n\n    validation {\n        condition = length(var.res_tags.[\"project\"]) &lt; 16 &amp;&amp; length(regexall(\"[^a-zA-Z0-9]\", var.res_tags.[\"project\"])) == 0\n        error_message = \"Project tags must be less than 16 characters and only contain numbers or letters\"\n    }\n...\n}\n</code></pre>"},{"location":"devops/terraform/variables/#protecting-sensitive-values","title":"Protecting Sensitive Values","text":"<p>Variables can be assigned a 'sensitive' attribute to flag whether the value should be displayed in the output from Terraform commands. If you create an output that includes the value of a  variable flagged as sensitive, then this will cause terraform commands to fail, unless you also flag the output as sensitive.</p> <p>However, the terraform state file is stored as plain text, and even sensitive values are stored as plain text so that terraform can read them to see if they have been changed. Therefore you  should ensure that your terraform state file is kept secure at all times. </p>"},{"location":"linux/awk/","title":"Awk","text":"<p>Taken from Tutorials Point See also Geeks for Geeks</p> <p>'awk' is a interpreted programming language designed for text processing. An awk program consists of commands that are executed sequentially on lines of text. Optional BEGIN and END blocks can be used for setup and tear-down. Use single-quotes to mark the start and end of the awk script.</p> <p>In its simplest form awk can be used to print the contents of a file:</p> <pre><code>awk '{print}' &lt;filename&gt;\n</code></pre> <p>The '-v' option can be used to assign variables before the awk commands are executed:</p> <pre><code>awk -v name=\"World!\" 'BEGIN{printf \"Hello, %s\\n\", name}'\n</code></pre> <p>'awk' treats input lines and output lines as space-separated fields. Columns from each line can be printed using their indexes (starting from 1):</p> <pre><code>awk '{print $3,$4}' &lt;filename\n</code></pre> <p>Or you can insert a tab separator between the output fields:</p> <pre><code>awk '{print $3 \"\\t\" $4}' &lt;filename&gt;\n</code></pre> <p>Filters can be added to select which lines to print:</p> <pre><code>awk '/&lt;pattern&gt;/ {print $3 \"\\t\" $4}' &lt;filename&gt;\n</code></pre> <p>In the absence of a body block, awks default action is to print the whole  record. $0 can be used to refer to the whole input record. These two  programs therefore produce the same output:</p> <pre><code>awk '/&lt;pattern&gt;/ {print $0}' &lt;filename&gt;\nawk '/&lt;pattern&gt;/' &lt;filename&gt;\n</code></pre> <p>Variables can be used without prior declaration:</p> <pre><code>awk '/&lt;pattern&gt;/{++counter} END{print \"Lines matched = \", counter}' &lt;filename&gt;\n</code></pre> <p>'length' is a built-in function to return the length of its operand. To print all lines longer than 82 characters use:</p> <pre><code>awk 'length($0) &gt; 82' &lt;filename&gt;    # using default action of {print}\n</code></pre> <p>Awk comes with a number of built-in variables:</p> Variable Meaning ARGC number of arguments provided at the command line ARGV an array of the command line arguments CONVFMT the conversion format for numbers ENVIRON hash of environment variables FILENAME the current filename FS current field separator (default is space) RS current record separator (default is newline) NF number of fields in current record NR number of the current record FNR number of current record in current file OFMT the output format for numbers OFS the output field separator (default space) ORS the output record separtor (default newline) RLENGTH the length of the string matched in <code>match</code> function RSTART the first position in the string matched in <code>match</code> function SUBSEP the separator character for array subscripts <pre><code>awk 'BEGIN {print \"Arguments = \", ARGC}' one two three four\n\nawk 'BEGIN {\n    for (i=0 ; i &lt; ARGC -1 ; i++) {\n        printf \"ARGV[%d] = %s\\n\", i, ARGV[i]\n    }\n}' one two three four\n\nawk 'BEGIN { print ENVIRON[\"USER\"] }'\n\nawk 'NF &lt; 10' &lt;filename&gt;                # print lines with less than 10 fields\nawk 'NF &lt; 10 {print NF}' &lt;filename&gt;     # print number of fields in lines with less than 10 fields\n\nawk 'NR &lt; 11' &lt;filename&gt;                # prints first 10 records\n\nawk 'BEGIN { if (match(\"Hello, World!\", \"or\")) {print RLENGTH}'\n</code></pre> <p>Additional variables are available with GNU AWK (gawk):</p> Variable Meaning ARGIND Index in ARGV of the current file being processed BINMODE used to set binmode for file I/O ERRNO error number if getline or close fails FIELDWIDTHS use a space-separated list of fieldwidths instead of a delimiter IGNORECASE used to make awk case-insensitive LINT used to dynamically set lint options PROCINFO used to access process information TEXTDOMAIN text domain of the AWK program"},{"location":"linux/bash_scripting/","title":"Bash Scripting Notes","text":"<p>Common Bash scripting techniques.</p>"},{"location":"linux/bash_scripting/#variable-names","title":"Variable names","text":"<p>Encapsulating variables in curly brackets to avoid ambiguity:</p> <pre><code>FirstThree=ABC\necho \"The first six are ${FirstThree}DEF\"\n</code></pre> <p>Encapsulating variable in double quotes preserves whitespace:</p> <pre><code>FirstThree='A  B  C'\necho $FirstThree    # prints 'A B C'\necho \"$FirstThree\"   # prints 'A  B  C'\n</code></pre> <p>Use $() or backticks to assign variables from the output of a command:</p> <pre><code>now=$(date +%Y-%m-%d)\necho $now\n</code></pre> <p>Assign an array using brackets around the list values. Use {} for disambiguation, and '@'  to access the whole array or an index value for a single element:</p> <pre><code>contents=($(ls))\necho ${contents[@]}\necho ${#contents[@]}  # number of elements in array\necho ${contents[0]}   # first element\necho ${contents[-1]}  # last element\n</code></pre> <p>You can also assign directly to an array element:</p> <pre><code>contents[3]=banana\n</code></pre> <p>Arguments are passed to scripts as space-separated values. '$0' holds the filename, '$1' is the first argument. '$#' holds the number of arguments passed to the script, and $@ holds a space-separated list of all arguments. </p>"},{"location":"linux/bash_scripting/#arithmetic-and-strings","title":"Arithmetic and Strings","text":"<p>Bash supports arithmetic operators using an arithmetic expression $(()):</p> <pre><code>INCOME=10000\nTAX=$(($INCOME / 100 * 25))\necho $TAX\n</code></pre> <p>Several string operators are available. It is worth noting that the 'index' operator returns the  numerical position of the search term, rather than the index-value of the search:</p> <pre><code>STRING=\"Nice to see you, to see you, nice\"\necho ${#STRING}                         # length of string\n\nexpr index \"$STRING\" \"s\"                # 9: position of first 's' IN $STRING\necho ${STRING:$((9-1)):3}               # see: 3 characters from index 8\necho ${STRING:9-1:3}                    # see: 3 characters from index 8\necho ${STRING:8}                        # string from index 8 to end\nexpr index \"$STRING\" \"you\"              # 7: position of first 'y','o' or 'u' IN $STRING\necho ${STRING[@]/you/me}                # replace first 'you' with 'me'\necho ${STRING[@]//you/me}               # replace all occurrences of 'you' with 'me'\necho ${STRING[@]// you/}                # delete all occurrences of ' you'\necho ${STRING[@]/#Nice/Good}            # replace characters at the start of the string\necho ${STRING[@]/%nice/OK}              # replace characters at the end of the string\necho ${STRING[@]/%nice/$(pwd)}          # replace characters at the end of the string\n</code></pre>"},{"location":"linux/bash_scripting/#conditionals","title":"Conditionals","text":"<p>The basic 'if' statement takes this form:</p> <pre><code>if [ &lt;condition&gt; ]; then\n    # statements to execute\nelif [ &lt;condition ]; then\n    # statements to execute\nelse\n    # statements to execute\nif\n</code></pre> <p>Conditionals can be a single test or combine multiple tests using '&amp;&amp;' or '||'. When using  multiple tests, surround the conditional in double brackets '[[]]'. </p> <p>Numeric comparisons are:</p> Operator Meaning -lt less than -gt greater than -le less than or equal to -ge greater than or equal to -eq equal to -ne not equal to <p>String comparisons are: </p> Operator Meaning = is the same as == is the same as != is different -z is empty <p>Use double quotes around strings in comparisons to avoid shell expansion.</p> <p>Case statements take the form: </p> <pre><code>case \"$variable\" in\n    \"$value1\" )\n        command\n    ;;\n    \"$value2\" | \"$value3\")\n        command\n    ;;\n    ...\n    * )\n        default command\n    ;;\nesac\n</code></pre>"},{"location":"linux/bash_scripting/#loops","title":"Loops","text":"<ul> <li> <p>'for' loops look like this: </p> <pre><code>for arg in [list]\ndo\n    command $arg\ndone\n</code></pre> </li> <li> <p>'while' loops look like this: </p> <pre><code>while [ condition ]\ndo\n    command\ndone\n</code></pre> </li> <li> <p>'until' loops look like this: </p> <pre><code>until [ condition ]\ndo\n    command\ndone\n</code></pre> </li> </ul> <p>Loop constructs support 'break' and 'continue' statements.  </p> <p>An example 'for' loop:</p> <pre><code>NUMBERS=(12 13 16 18 21 25 23 28 33 35)\n\nfor n in ${NUMBERS[@]}\ndo\n    if [ $n = 33 ]; then \n        break\n    elif [ $(($n % 2)) = 1 ]; then\n        echo $n\n    else\n        echo \"Not an odd number\"\n    fi\ndone\n</code></pre>"},{"location":"linux/bash_scripting/#functions","title":"Functions","text":"<p>Functions are declared are declared with the key word 'function':</p> <pre><code>function f1 {\n    # access parameters as $1, $2, etc\n}\n</code></pre> <p>Call the function by name, supplying any required arguments: </p> <pre><code>function f1 {\n    echo \"${1}, ${2}\"\n}\n\nSTMT=$(f1 \"Hello\" \"World!\")\n\necho $STMT\n</code></pre> <p>Use command substitution to collect the output from the function as a return value. Use the 'local'  keyword to scope your variables to the functions scope to avoid changing variables in the global scope:</p> <pre><code>function f1 {\n    GREET=\"Hello\"\n    local SCOPE=\"Earth\"\n    echo \"${GREET}, ${SCOPE}!\"\n}\n\nGREET=\"Goodbye\"\nSCOPE=\"World\"\n\nf1 $GREET $SCOPE           # changes $GREET globally\n\necho \"${GREET}, ${SCOPE}!\" # prints \"Hello, World!\"\n</code></pre> <p>When using command substitution around your function calls, the variables are scoped to the  statement, so variables in the function will not overwrite globals:</p> <pre><code>function f1 {\n    GREET=\"Hello\"\n    local SCOPE=\"Earth\"\n    echo \"${GREET}, ${SCOPE}!\"\n}\n\nGREET=\"Goodbye\"\nSCOPE=\"World\"\n\necho $(f1 $GREET $SCOPE)   # prints \"Hello, Earth!\" but does not change global $GREET\n\necho \"${GREET}, ${SCOPE}!\" # prints \"Goodbye, World!\"\n</code></pre>"},{"location":"linux/bash_scripting/#special-variables-and-signals","title":"Special Variables and Signals","text":"<p>A number of special variables are available to scripts and functions:</p> Variable Name Meaning $0 filename of current script $n nth argument to script or function $# number of arguments passed $@ all arguments passed $* all arguments passed $? exit status of last command $$ process id of current shell $! process number of last background job <p>You can use trap to intercept OS signals such as Ctrl+C (SIGINT) or Ctrl+D (SIGQUIT):</p> <pre><code>trap &lt;code to execute&gt; $SIG1 $SIG2 ...\n</code></pre> <p>Signals can be specified by name or number: use <code>kill -l</code> for a list of signals. The code  to execute can be inline or a function name. </p>"},{"location":"linux/bash_scripting/#file-tests","title":"File Tests","text":"Name Test -e file exists -d directory exists -r file is readable"},{"location":"linux/commands/","title":"Linux Commands","text":"<ul> <li> <p>Check your linux version</p> <pre><code>lsb_release -a\n</code></pre> </li> <li> <p>Check system startup time</p> <pre><code>systemd-analyze\n</code></pre> </li> <li> <p>Check  startup time by individual service</p> <pre><code>systemd-analyze blame\n</code></pre> </li> <li> <p>Disable an individual service</p> <pre><code>sudo systemctl disable NetworkManager-wait-online.service\n</code></pre> </li> <li> <p>Find process ids based on name patterns:</p> <pre><code>pgrep firefox\n</code></pre> </li> <li> <p>Find process ids based on username: </p> <pre><code>pgrep -u &lt;username&gt;\n</code></pre> </li> </ul>"},{"location":"linux/commands/#wifi-commands","title":"WiFi Commands","text":"<p>The <code>lshw</code> command can be used to quickly identify network hardware on your device:</p> <pre><code>sudo lshw -C network\n</code></pre> <p>The output will include both MAC address in the 'serial' field and IP address in the 'configuration'  field, along with other information such as the 'logical name' and the 'bus id'. <code>ifconfig</code>  will also provide the MAC and IP addresses for connect wireless adapters. Use <code>iwconfig</code>  to see the speed and link quality for your wireless adapter The <code>nmcli</code> command provides useful information from Network Manager. To see the saved  connections for your device use:</p> <pre><code>nmcli connection show\n</code></pre> <p>To see a list of Access Points and stats on their signal strength and capabilities, use:</p> <pre><code>nmcli dev wifi\n</code></pre> <p><code>wavemon</code> is an ncurses-based monitoring tool for WiFi</p>"},{"location":"linux/commands/#network-commands","title":"Network Commands","text":"<p>To see your current IP address instead of using ifconfig try ip:</p> <pre><code>ip address show\n</code></pre> <p>This will show:</p> <ol> <li>Interface name</li> <li>MAC address: link/ether</li> <li>IP address: inet</li> <li>State of the Interface: UP or DOWN</li> </ol> <p>You can also use <code>ip link show</code> to check the state of your interfaces.</p> <p>Use the route command to show the routing table on a computer:</p> <pre><code>route\n</code></pre> <p>The default route shows you which interface is used if no specific  route matches the address specified. </p> <p>You can also use ip route show to show the route to a specific network:</p> <pre><code>ip route show 10.0.0.0/8\n</code></pre> <p>You can get help on the ip command using man ip-address, man ip-link  or man ip-route. </p> <p>Use the arp command to check the ARP table (data link layer) addresses stored locally. This will show the mapping of physical addresses to ip  addresses for devices on the same LAN (e.g. your gateway or router):</p> <pre><code>arp -a \n</code></pre> <p>To check for ports on localhost use ss:</p> <pre><code>ss -tunlp4\n</code></pre> <p>Whilst telnet can be used to test connectivity to TCP ports, netcat also provides the ability to check connections to UDP ports:</p> <pre><code>nc 192.168.0.3 -u 80 -v\nnc 127.0.0.1 8000 -v\n</code></pre> <p>Netcat can also be used to quickly setup a server to listen on a  specified port. </p> <pre><code>nc -l 1234 &gt; filename.out\n</code></pre> <p>All the input sent to port 1234 will be copied to filename.out:</p> <pre><code>nc -N myhost.example.com 1234 &lt; filename.in\n</code></pre> <p>These two commands will allow you to transfer the contents if filename.in to filename.out on the listening server. </p> <p>The -N option closes the connection on termination.</p>"},{"location":"linux/commands/#sed","title":"sed","text":"<p>sed stands for Stream Editor, and can be used to replace text from  input to output:</p> <pre><code>echo 'Hello, World!' | sed 's/World/Universe/'\n</code></pre> <p>sed can also be used directly on a file:</p> <pre><code>sed 's/left/right/' &lt;filename&gt;\n</code></pre> <p>By default output is sent to STDOUT, but you can change the file  contents using the -i switch:</p> <pre><code>sed -i 's/left/right/' &lt;filename&gt;\n</code></pre> <p>Sed allows capturing groups and backreferences. To capture IP Address from the weblog.txt use:</p> <pre><code>sed -nE 's/.* h=(\\d+\\.\\d+\\.\\d+\\.\\d+).*$/\\1/p' weblog.txt\n</code></pre> <p>Sed works well in pipelines: to find unique values of ip_address in  above example use:</p> <pre><code>sed -nE 's/.* h=(\\d+\\.\\d+\\.\\d+\\.\\d+).*$/\\1/p' weblog.txt | sort -u\n</code></pre> <p>To count the occurrences for each unique IP Address use:</p> <pre><code>sed -nE 's/.* h=(\\d+\\.\\d+\\.\\d+\\.\\d+).*$/\\1/p' weblog.txt | sort | uniq -c\n</code></pre> <p>You can also pipe the list of IPs to lookup the hostnames:</p> <pre><code>sed -nE 's/.* h=(\\d+\\.\\d+\\.\\d+\\.\\d+).*$/\\1/p' weblog.txt | \\\nsort -u | awk '{ cmd = \"dig +short -x \" $1; system(cmd) }'\n</code></pre>"},{"location":"linux/commands/#port-tunnelling","title":"Port Tunnelling","text":"<p>The <code>ssh</code> command is typically used to connect to the SSH server on a remote  server using port 22. However, you can also connect to other ports on the  server, using <code>ssh</code> to tunnel the connection. For instance, if you have  a web application on a remote server, listening on port 8000, you can  create a tunnel to that port, and then connect to the tunnel on your local machine. To tunnel port 8000 from the remote server to the local server, use:</p> <pre><code>ssh -i &lt;path_to_private_key&gt; -L 8000:localhost:8000 ${username}@${hostname}\n</code></pre> <p>This command will allow you to open <code>http://localhost:8000</code> via a browser.  Although the connection is <code>http</code> traffic between your machine and the remote server is encrypted by the SSH tunnel. </p>"},{"location":"linux/performance/","title":"Performance","text":""},{"location":"linux/performance/#monitoring-system-performance-with-sar","title":"Monitoring System Performance with SAR","text":"<p>sar (System Activity Reporting) is part of the sysstat package. Configuration  for sysstat is held in /etc/sysstat/sysstat. To enable the service run <code>systemctl enable sysstat</code> and to start the service run <code>systemctl start sysstat</code>.</p> <p>The sar command can be used to report on activity counters collected in the system activity data files. These are usually stored in <code>/var/log/sa</code> or <code>/var/log/sysstat</code> directories. Files will  be named <code>saDD</code> or <code>sarDD</code>, where DD represents the day of the month. </p>"},{"location":"linux/performance/#basic-commands","title":"Basic Commands","text":"<p>To show current CPU activity at 5 second intervals, 10 times, run: <code>sar 5 10</code></p> <p>To show all cpu data from yesterday, try: <code>sar -1</code></p> <p>For memory usage: <code>sar -r -1</code></p> <p>Various flags exist to report different classes of activity:</p> <ol> <li><code>-B</code>: displays paging statistics. <ul> <li>the %vmeff shows the efficiency of VM paging: values approaching 100% are good; values less than 30% indicate problems with paging, although a value of 0 indicates that no paging is occurring.</li> </ul> </li> <li><code>-b</code>: I/O and transfer statistics (or disk activity).</li> <li><code>-d</code>: activity for each block device. Use with <code>-j PATH</code> to show device path names.</li> <li><code>-n DEV</code>: activity for network devices </li> <li><code>-q</code>: show statistics for queue length and load</li> <li><code>-r</code>: show memory statistics. <code>%memused</code> is the total of active memory and buffered/cached memory. <code>%memcommit</code> is the amount of memory required by current workload and includes pagefile allocation. <code>%memcommit</code> gives a better idea of how much physical memory is in use, and therefore how much is free for new processes, when the figure is below 100%. If the figure is higher than 100% then this suggests that physical memory plus pagefile memory is being used for the current workload. </li> <li><code>-u</code>: show CPU statistics</li> <li><code>-A</code>: display data for all classes of activity collected</li> </ol>"},{"location":"linux/performance/#free","title":"Free","text":"<p>The <code>free</code> command shows memory usage on the system, specifically Total, Used, Free, Cached and  Available memory. The <code>available</code> column indicates how much memory is availabe to new processes without having to perform paging: that is, it does not count memory that is buffered or cached. </p>"},{"location":"operations/soar/","title":"Security Orchestration, Automation and Response (SOAR)","text":"<p>Notes from the SANS Institute Whitepaper: 'Learn How SOAR Helps You Streamline Security While Improving Your Defenses Against Cyber Attacks', Cristopher Crowley, Temi Adebambo, August 2022</p> <p>Cybersecurity deals with systems that are behaving unexpectedly and deviating from intended or  authorised actions. SOAR tools are designed to help cyber security professionals develop  automations to handle these events. </p> <p>Cybersecurity staff need to:</p> <ul> <li>verify signals requiring a response</li> <li>collect data from multiple systems</li> <li>aggregate related data</li> <li>perform repeatable tasks and </li> <li>work through complicated sequences to make the correct decisions and take appropriate actions</li> </ul>"},{"location":"operations/soar/#workflow-development-with-soar","title":"Workflow Development with SOAR","text":"<p>SOAR is intended to help improve performance accuracy and precision to cybersecurity operations.  A SOAR should provide staff with repetitive and repeatable tasks (workflows) to respond to security  incidents. Workflows will consist of modules: interconnected sequences of actions. Workflows  are developed by identifying possible scenarios and responses (use cases), and  identifying what supporting information is required in each scenario. Workflows can be tested to identify any additional requirements. The workflow can then be added to the SOAR tool.  Workflows can be updated by working through additional use cases. </p> <p>The SOAR tool develops into a knowledge management system through the repetition and  workflow enhancement cycle. </p> <p>Development of alerting systems involves use case development or detection engineering. The workflow development pipeline will typically involve:</p> <ul> <li>select a scenario of interest and decompose to a story line</li> <li>this should result in one or more use cases</li> <li>identify data artifacts and detection opportunities</li> <li>data enrichment through correlation with related internal or external (threat intelligence) data</li> <li>implement detections in systems</li> <li>perform assessment, review, revision or retirement</li> </ul> <p>Use case development and detection engineering are fundamental in security operations because they maximise true positive alerts. </p> <p>DevSecOps is an important to instill confidence in the automation provided by SOAR tools, and  involves the implementation of rigourous testing. </p>"},{"location":"operations/soar/#definitions","title":"Definitions","text":"<ul> <li>Security<ul> <li>the restriction of a system to its intended use and</li> <li>the protection of the confidentiality, integrity and availability of that system</li> </ul> </li> <li>Orchestration<ul> <li>the automated configuration, coordination and management of computer systems and software</li> </ul> </li> <li>Automation<ul> <li>the technology by which a process or procedure is performed with minimal human assistance</li> </ul> </li> <li>Response<ul> <li>initial verification</li> <li>short-term mitigation</li> <li>fundamental root-cause analysis</li> <li>business continuity in the face of actual incidents</li> <li>long term enhancements to systems to be more resilient to issues</li> </ul> </li> </ul>"},{"location":"python/jupyter/","title":"Jupyter","text":"<p>The IPython shells in Jupyter Notebooks allow us to run  shell commands by prefixing them with an exclamation mark. Typically you can use this to run <code>pip install</code>  or <code>conda install</code> directly from a notebook. But you can also use this to capture shell output to a python variable: </p> <pre><code>files = !ls -lh data_dir\n</code></pre> <p>We can also use this to see the number of columns in a CSV file: </p> <pre><code>!awk -F',' '{print NF; exit}' data_dir/text_data.csv\n</code></pre>"},{"location":"python/venv/","title":"Create a Python Virtual Environment","text":"<p>To create a virtual environment run: </p> <pre><code>python3 -m venv &lt;environment_name&gt;\n</code></pre> <p>To activate the environment run: </p> <pre><code>source &lt;environment_name&gt;/bin/activate\n</code></pre> <p>Deactivate the environment with:</p> <pre><code>deactivate\n</code></pre> <p>To save installed packages to a requirements.txt file:</p> <pre><code>pip3 freeze &gt; requirements.txt\n</code></pre>"},{"location":"python/pandas/data_aggregation/","title":"Data Aggregation","text":""},{"location":"python/pandas/data_aggregation/#merging-dataframes","title":"Merging DataFrames","text":"<p>The pandas <code>concat</code> function works similar to the SQL <code>UNION ALL</code>:  by appending one dataset to another. The pandas <code>merge</code> method is equivalent to an SQL <code>JOIN</code>, and support inner, outer and left  and right joins.</p> <pre><code>class_lookup = pd.DataFrame(\n    [ ( 1, 'First'), (2, 'Second'), (3, 'Third') ],\n    columns = ['id', 'description']\n)\n\ninner_join = df.merge(class_lookup, left_on='Pclass', right_on='id')\n</code></pre> <p>The <code>left_on</code> and <code>right_on</code> parameters are necessary because the two dataframes use different names for the 'passenger class' field. This  join also means that our resulting <code>inner_join</code> DataFrame contains two  columns containing the same data: 'Pclass' and 'id'. We can rename the  column in one DataFrame to match the other, resulting in only one  column for the 'passenger class': </p> <pre><code>inner_join = df.merge(\n    class_lookup.rename(dict(id='Pclass'), axis=1),\n    on='Pclass'\n)\n</code></pre> <p>Left, Right and Outer Joins are made using the <code>how</code> parameter: </p> <pre><code>left_join = df.merge(\n    class_lookup.rename(dict(id='Pclass'), axis=1),\n    on='Pclass', how='left'\n)\n</code></pre> <p>Outer joins also allow an <code>indicator</code> parameter, that adds a column to indicate if the row resulted from a match on both tables, or from  no match on either the left or right side of the join: </p> <pre><code>outer_join = df.merge(\n    class_lookup.rename(dict(id='Pclass'), axis=1),\n    on='Pclass', how='outer', indicator=True\n)\n</code></pre> <p><code>merge</code> operations can also be done using the DataFrame indexes: </p> <pre><code>inner_join = df.merge(\n    class_df, left_index=True, right_index=True\n)\n</code></pre> <p>If both DataFrames have columns with the same names, the columns from the left DataFrame will be suffixed with <code>_x</code> and the  columns from the right DataFrame with <code>_y</code>. You can alter this  behaviour with the <code>suffixes</code> parameter, providing a tuple of  the required suffixes. </p> <p>If joining DataFrames using indexes, then you can use <code>join</code> instead of <code>merge</code>. The left table must use the index, but for the right table you can specify a column using the <code>on</code> parameter: </p> <pre><code>inner_join = df.join(\n    class_df, lsuffix='_l', rsuffix='_r'\n    on='passenger_class_id'\n)\n</code></pre> <p>Use set operations to test the effects of joins before they're made.  An Inner Join is represented as an intersection: </p> <pre><code>patient.set_index('patient_id', inplace=True)\npathology.set_index('patient_id', inplace=True)\n\npatient.index.intersection(pathology.index)\n</code></pre> <p>The difference shows what will be lost in the join, either where the join type is inner, left or right: </p> <pre><code># index values in patient but not in pathology\npatient.index.difference(pathology.index)\n\n# index values in pathology but not in patient\npathology.index.difference(patient.index)\n</code></pre>"},{"location":"python/pandas/data_aggregation/#aggregate-functions","title":"Aggregate Functions","text":"<p>Pandas provides access to aggregate methods that can be applied to one or more columns:</p> <pre><code>df[['CostInBillingCurrency', 'EffectivePrice', 'Quantity']].sum()\n</code></pre> <p>Additional aggregate functions include <code>mean</code>, <code>count</code>, <code>min</code>, <code>max</code> and <code>cumsum</code>. </p> <p>The <code>agg</code> method allows you to apply multiple aggregates to multiple  columns in a DataFrame:</p> <pre><code>df.agg({\n    'CostInBillingCurrency': ['sum', 'mean'],\n    'EffectivePrice': ['min', 'max', 'mean'],\n    'Quantity': ['sum', 'min', 'max'],\n})\n</code></pre> <p>The <code>groupby</code> method allows you to run aggregates after grouping rows of data: </p> <pre><code>df.groupby('ResourceGroup').sum()\n</code></pre> <p>However, before applying the aggregate function, it would be sensible to  select the columns that you wish to apply the function to, otherwise you'll be applying the function on all columns in the DataFrame: </p> <pre><code>df[['ResourceGroup', 'CostInBillingCurrency']].groupby('ResourceGroup').sum()\n</code></pre> <p>Alternatively, the <code>agg</code> method can be combined with <code>groupby</code> to select the columns and the specific aggregation needed: </p> <pre><code>\ndf[\n    ['MeterSubCategory', 'EffectivePrice', 'Quantity', 'CostInBillingCurrency']\n].groupby(\n    ['MeterSubCategory', 'EffectivePrice']\n).agg(['sum', 'max'])\n\ndf.groupby(['MeterSubCategory', 'EffectivePrice']\n).agg({\n    'Quantity': 'sum',\n    'CostInBillingCurrency': 'sum',\n})\n</code></pre> <p>Note that <code>agg</code> accepts an array if you want to supply the same aggregates to the  columns, or a dictionary if you want to specify which aggregations to apply to  each column. </p> <p>Using multiple aggregates results in a MultiIndex in the columns: </p> <pre><code>df_agg = df[\n    ['MeterSubCategory', 'EffectivePrice', 'Quantity', 'CostInBillingCurrency']\n].groupby(\n    ['MeterSubCategory', 'EffectivePrice']\n).agg(['sum', 'max'])\n\ndf_agg.columns\n\nMultiIndex([(             'Quantity', 'sum'),\n            (             'Quantity', 'max'),\n            ('CostInBillingCurrency', 'sum'),\n            ('CostInBillingCurrency', 'max')],\n           )\n</code></pre> <p>However, you can remove this hierarchical index, by processing the column tuples into an array of scalars: </p> <pre><code>df_agg.columns = ['_'.join(tuple_col) for tuple_col in df_agg.columns]\n</code></pre> <p>You can include a DateTimeIndex or PeriodIndex in the <code>groupby</code>,  and sort the output by values or index: </p> <pre><code>df.groupby([df.ResourceGroup, df.index.month]).agg({\n    'CostInBillingCurrency': 'sum'\n}).sort_values('CostInBillingCurrency')\n\ndf.groupby([df.ResourceGroup, df.index.month]).agg({\n    'CostInBillingCurrency': 'sum'\n}).sort_index()\n\n</code></pre> <p>You can select the largest or smallest values for a group of columns using <code>nlargest</code> or <code>nsmallest</code>: </p> <pre><code>df.groupby([df.ResourceGroup, df.index.month]).agg({\n    'CostInBillingCurrency': 'sum'\n}).nlargest(n=10, columns=['CostInBillingCurrency'])\n</code></pre> <p>The <code>groupby</code> method accepts <code>Grouper</code> objects: for a DataFrame with a DatetimeIndex,  you can set a quarterly grouper: </p> <pre><code>df['CostInBillingCurrency'].groupby(pd.Grouper(freq='QE')).sum()\n</code></pre> <p>By selecting the frequency in the Grouper, you can aggregate over time periods in a DatetimeIndex: </p> <pre><code>df.loc['2024-12-01': '2024-12-15', 'CostInBillingCurrency'].groupby(pd.Grouper(freq='D')).sum()\n</code></pre> <p>The <code>resample</code> method allows you to aggregrate data to a different granularity: </p> <pre><code>weekly_df = df.resample('1W').agg({\n    'CostInBillingCurrency': 'sum',\n})\n</code></pre>"},{"location":"python/pandas/data_frame_basics/","title":"DataFrame Basics","text":"<p>Data in Pandas is managed via Series and DataFrame objects. </p>"},{"location":"python/pandas/data_frame_basics/#series","title":"Series","text":"<p>A <code>pandas.Series</code> object is a data structure for an array of data of a single type, and can be thought of as a column in a table. Create a Series by  providing an array of values: </p> <pre><code>import numpy as np\nimport pandas as pd\n\nnp.random.seed(0) # set a seed so that the random numbers are always the same\npd.Series(np.random.rand(5), name='random')\n</code></pre> <p>In addition to the data we provide the Series object, pandas adds in Index object representing the row numbers. The Index object can be used to perform  element-wise operations on Series objects. For example, addition using two Series objects, will add the values for the elements with matching index values. Where the indexes do not match, the result will return a null value  (<code>NaN</code> or <code>None</code>) for those index values. </p>"},{"location":"python/pandas/data_frame_basics/#dataframes","title":"DataFrames","text":"<p>A <code>pandas.DataFrame</code> object is a 2-dimensional data structure used to store data in columns. </p> <p>To create a DataFrame manually, use a dictionary of lists:</p> <pre><code>df = pd.DataFrame(\n  {\n    'Forename': ['Jeff', 'James', 'Eric', 'Clark'],\n    'Surname': ['Jefferson', 'Jameson', 'Ericsson', 'Clarkson'],\n    'Date of Death': ['15/12/2021', '12/05/2002', '25/10/2023', '28/07/2012'],\n    'Place of Death': [np.random.choice(['Norfolk', 'Shropshire', 'Devon', 'Bedfordshire']) for _ in range(4)] ,\n    'Deceased': [np.random.choice(['Yes', 'No']) for _ in range(4) ],\n    'Age' : np.random.rand(4) * 100\n  }\n)\n</code></pre> <p>Or a list of dictionaries: </p> <pre><code>df2 = pd.DataFrame([\n    {'forename': 'Jeff', 'surname': 'Jefferson', 'date of death': '15/12/2021'},\n    {'forename': 'James', 'surname': 'Jameson', 'date of death': '01/10/2021'},\n    {'forename': 'Eric', 'surname': 'Ericsson', 'date of death': '11/05/2021'},\n    {'forename': 'Clark', 'surname': 'Clarkson', 'date of death': '25/02/2021'},\n])\n</code></pre> <p>Or a list of tuples:</p> <pre><code>df3 = pd.DataFrame([\n    ('Jeff', 'Jefferson', '15/12/2021'),\n    ('James', 'Jameson', '01/10/2021'),\n    ('Eric', 'Ericsson', '11/05/2021'),\n    ('Clark', 'Clarkson', '25/02/2021')],\n    columns=['forename', 'surname', 'date of death']\n)\n</code></pre> <p>Or a list of lists:</p> <pre><code>df5 = pd.DataFrame([\n        ['Jeff', 'Jefferson', '15/12/2021'],\n        ['James', 'Jameson', '01/10/2021'],\n        ['Eric', 'Ericsson', '11/05/2021'],\n        ['Clark', 'Clarkson', '25/02/2021']\n    ],\n    columns=['forename', 'surname', 'date of death']\n)\n</code></pre> <p>Or a <code>numpy</code> array:</p> <pre><code>df4 = pd.DataFrame(\n    np.array([\n        ['Jeff', 'Jefferson', '15/12/2021'],\n        ['James', 'Jameson', '01/10/2021'],\n        ['Eric', 'Ericsson', '11/05/2021'],\n        ['Clark', 'Clarkson', '25/02/2021']\n    ]),\n    columns=['forename', 'surname', 'date of death']\n)\n</code></pre>"},{"location":"python/pandas/data_frame_basics/#external-data","title":"External Data","text":"<p>DataFrames can be created from CSV files using the pandas <code>read_csv</code> function:</p> <pre><code>df = pd.read_csv(\"data/unclaimedestates.csv\", parse_dates=True)\n</code></pre> <p>The <code>read_csv</code> function provides additional arguments to allow you to  specifiy seperators, delimiters, etc. Use <code>help(pd.read_csv)</code> or  <code>pd.read_csv?</code> for the full function definition. </p> <p>There are functions for reading data in various formats, all beginning with a <code>read_</code> prefix: e.g. <code>read_json</code>, <code>read_excel</code>, <code>read_sql</code>. Try: </p> <pre><code>functions = [function for function in dir(pd) if function.startswith('read_')]\n</code></pre> <p>To read data from a database, you need to provide a both a query or table name and a connection object: </p> <pre><code>import pandas as pd\nfrom sqlalchemy import URL, create_engine\nimport psycopg2\n\npsql_connection_string = URL.create(\n    'postgresql',\n    username='user_name',\n    password='passwd',\n    host='server_name,\n    port=5432,\n    database='database_name'\n)\nconn = create_engine(psql_connection_string)\n\nquery = '''\nSELECT * FROM\n\"monitoring\".\"collect_data\"\nWHERE collected_date &gt; CURRENT_DATE - INTERVAL '60 days'\nORDER BY collected_date desc\n'''\n\nquery_df = pd.read_sql_query(query,con=conn)\n\ntable_df = pd.read_sql_table(\n    'collect_data',\n    schema='monitoring',\n    con=conn,\n    index_col='collected_date',\n)\n</code></pre> <p>DataFrames can also be written to external files  using the corresponding <code>to_</code> methods: e.g. <code>to_json</code>, <code>to_excel</code>, <code>to_sql</code>. With the <code>to_sql</code> function you can provide the table name, connection and what action to take if the table already  exists: </p> <pre><code>df.to_sql(name='users', con=connection, if_exists='replace')\n</code></pre> <p>Pandas <code>to_</code> functions will also allow to you include or exclude the DataFrame index in the output: </p> <pre><code>df.to_excel(\"recipes.xlsx\", sheet_name=\"flapjack\", index=False)\n</code></pre>"},{"location":"python/pandas/data_frame_basics/#examining-data","title":"Examining Data","text":"<p>DataFrame objects provide a number of attributes and functions that  you can use to examine the data frame. </p> Attribute Description shape number of rows and columns columns names of the columns dtypes data type for each column <p>All the output from the <code>shape</code>, <code>columns</code>, and <code>dtypes</code> attributes is  also provided through the <code>info</code> function, along with details on the index and memory usage: </p> <pre><code>df.info()\n</code></pre> <p>The <code>describe</code> method provides summary data (min, max, mean, count and percentiles)  for the numeric columns, but can also be used to get some summary data  (count, unique values, mode, frequency of mode) for non-numeric data:</p> <pre><code>df.describe(include=np.object)\n# or\ndf.describe(include='all')\n</code></pre> <p>By default <code>describe</code> provides the 0.25, 0.5, 0.75 percentiles but you can select  these with the <code>percentiles</code> parameter:</p> <pre><code>df.describe(percentiles=[0.05, 0.50, 0.95])\n</code></pre> <p><code>describe</code> only provides summary statistics for non-null values.</p> <p>Each summary statistic is also available via separate summary methods that can be applied to either the DataFrame or Series objects, e.g <code>min</code>, <code>max</code>,  <code>idxmin</code>, <code>idxmax</code>, <code>sum</code>, <code>mean</code>, <code>median</code>. For Series objects additional  summary methods exist: <code>unique</code>, <code>value_counts</code> and <code>mode</code>. </p>"},{"location":"python/pandas/data_frame_basics/#selecting-data","title":"Selecting Data","text":"<p>Display the dataframe by calling its name. This will display the first 5 rows  and the last 5 rows. To show the first n rows, use the <code>head</code> method, or <code>tail</code> for the last n rows, or <code>sample</code> for n random values:</p> <pre><code>df.sample(10)\n</code></pre> <p>Each column in a DataFrame is a Series. To select a column from a DataFrame,  use the column label between square brackets. For multiple columns supply a  list:</p> <pre><code>df[[\"Place of Death\", \"Date of Birth\"]\n</code></pre> <p>DataFrames support <code>slicing</code> operations, which can be combined with column selection: </p> <pre><code>df[[\"Place of Death\", \"Date of Birth\"]][5:15]\n# Order doesn't matter\ndf[5:15][[\"Place of Death\", \"Date of Birth\"]]\n</code></pre> <p>However, using slicing and selection to assign values to a DataFrame is not the recommended approach: instead you should use the <code>loc</code> or  <code>iloc</code> methods. <code>loc</code> and <code>iloc</code> accept two <code>labels</code>:  a row selector and a column selector. Either selector can be  expressed as a single value, a list or a slice. :</p> <pre><code>df.loc[2, 'Surname']\ndf.loc[[2,3], 'Surname']\ndf.loc[2:3, 'Surname']\n\ndf.loc[df.Surname == 'Johnson', ['Forename','Surname']]\ndf.loc[df.Surname == 'Johnson', 'Forename':'Surname']\n\ndf.loc[[df.Age.idxmin(), df.Age.idxmax()], :]\n</code></pre> <p>With <code>loc</code>, slicing operations are inclusive of the end index if provided. <code>iloc</code>  instead follows the standard python-slicing approach, and excludes the end value of a slice if provided. <code>iloc</code> expects integer values for both row selectors and  column selectors:</p> <pre><code># returns only the value at row 2, column 1 and 2\ndf.iloc[2:3, 1:3]\n\n# returns values at rows 2 and 3, column 1, 2 and 3\ndf.iloc[[2,3],[1,2,3]]\n</code></pre> <p><code>iloc</code> allows slicing for both row and column selectors. </p> <p>For selecting a single value from a DataFrame, the <code>at</code> and <code>iat</code> methods provide better performance: </p> <pre><code>wanted = df.at[2,'Surname']\nwanted = df.iat[2,1]\n</code></pre>"},{"location":"python/pandas/data_frame_basics/#filtering-data","title":"Filtering Data","text":"<p>You can filter the output using a condition inside the selection brackets:</p> <pre><code>df[df[\"Date of Birth\"] &lt; '01/01/1970']\n</code></pre> <p>The <code>isin()</code> method can be used to check for multiple conditions on  a column:</p> <pre><code>df[df[\"Surname\"].isin([\"Smith\", \"Jones\"])]\n</code></pre> <p>You can also use the 'or' operator to acheve the same result: </p> <pre><code>df[(df[\"Surname\"] == \"Jones\") | (df[\"Surname\"] == \"Smith\")]\n</code></pre> <p>The AND Operator <code>(&amp;)</code> can be used to specify multiple conditions that should all return <code>True</code>. The Negation Operator <code>(~)</code> can  be used to negate the return value of a condition: </p> <pre><code>df[~(df.Surname == 'Jameson')]\n</code></pre> <p>To filter out rows where a particular column has Null values, use the <code>notna()</code> method:</p> <pre><code>df[df[\"Executors\"].notna()]\n</code></pre> <p>We can also use strings and regular expressions: </p> <pre><code># contains 'son' anywhere in the string\ndf[df.Surname.str.contains('son')]\n\n## contains 'son' at the end of the string\ndf[df.Surname.str.contains(r'son$')]\n\n## contains 'Jeff' or 'James' at the beginning of the string\ndf[df.Surname.str.contains(r'^Jeff|James')]\n</code></pre> <p>To filter numeric values with a lower and upper range use <code>between</code>:</p> <pre><code>df[df.Age.between(10,32, inclusive='both')]\n</code></pre> <p><code>pop</code> can be used to remove a column and save this to  a Series. If the column contains Boolean values, then this can later be used to filter the DataFrame, even though the column no longer exists in  the DataFrame. This works because the Series has the same index as the  DataFrame:</p> <pre><code>is_retired = df.pop('Retired')\n\ndf['Retired'] # produces a key-error\n\nretired_df = df[is_retired].copy()\nnot_retired_df = df[~is_retired].copy()\n</code></pre> <p>This can be useful to filter a DataFrame, without storing the filter column in the DataFrame. The <code>copy</code> method is provided, to create new DataFrames that can be worked on independantly of the original. </p> <p>The filter Series does not have to contain Booleans, but can be used to create a Boolean result:</p> <pre><code>where_died = df.pop('Place of Death')\n\ndf[where_died == 'Norfolk']\n</code></pre> <p>These filtering techniques can also be used for row labels in both the <code>loc</code> or <code>iloc</code> operators.</p>"},{"location":"python/pandas/data_frame_basics/#dataframe-queries","title":"DataFrame Queries","text":"<p>Pandas provides the <code>query</code> method to filter data, instead of using a Boolean mask. The <code>query</code> method allows more complex filters to  be used with less typing: </p> <pre><code>df = pd.read_csv('titanic.csv')\n\ndf[(df.Survived == 1) &amp; (df.Sex == 'male')].equals(\n    df.query('Survived == 1 &amp; Sex == \"male\"'))\n</code></pre>"},{"location":"python/pandas/data_frame_basics/#concatenate-dataframes","title":"Concatenate DataFrames","text":"<p>The <code>concat()</code> method can be used to combine two tables with a similar structure. You can specify an axis of '0' to add the second DataFrame as new rows, or an  axis of '1' to add the DataFrame as new columns. </p> <p>To join the rows of two tables use:</p> <pre><code>disk_data = pd.concat([disk_data_server_1, disk_data_server_2], axis=0)\n</code></pre> <p>If either DataFrame contains columns not found in the other, these will be added as new columns in the new DataFrame, and the rows where these columns did not exist, will be filled with <code>NaN</code>. This behaviour can be change by specifying the join type as 'inner': which only keeps columns that are common to the input DataFrames. </p> <p>The 'ignore_index' option can be used to create a new index for the new  DataFrame. If you wish to keep the original indices from both input DataFrames,  you can optionally add an additional (hierarchical) row index to identify which  row came from which input DataFrame:</p> <pre><code>disk_data = pd.concat([disk_data_server_1, disk_data_server_2], axis=0, keys=[\"srv1\", \"srv2\"])\n</code></pre> <p>This produces a DataFrame with a multi-index. To access entries by index value,  supply a tuple or list of tuples: </p> <pre><code>disk_data.loc[[('srv2',0),('srv2',1)],:]\n</code></pre> <p>When using <code>concat</code> in a columnwise join (<code>axis=1</code>), Pandas uses the row indexes  to make the joins. For rows that don't share a common index, values are filled with <code>NaN</code>. </p>"},{"location":"python/pandas/data_visualisation/","title":"Visualising Data","text":""},{"location":"python/pandas/data_visualisation/#matplotlib","title":"Matplotlib","text":"<p>To create a graph of a DataFrame use the <code>plot()</code> method. Pandas creates a line plot by default for each of the Series in a DataFrame  that has numeric values. All the plots created by Pandas are <code>Matplotlib</code> objects. The <code>matplotlib.pyplot</code> library provides the <code>show()</code> method  to display the graph. First import <code>pyplot</code>: </p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n</code></pre> <p>For a DataFrame with a DatetimeIndex and a single numeric column,  you can call <code>plot</code> directly on the DataFrame, or pass the  axes to <code>pyplot</code>: </p> <pre><code>\ndaily_df = ws_df.groupby(pd.Grouper(freq='D')).agg({\n    'CostInBillingCurrency': 'sum',\n})\n\ndaily_df.plot()\n\n## Alternate method: \n# plt.plot(daily_df.index, daily_df.values)\n</code></pre> <p>Using the <code>plot</code> method on the DataFrame might provide better default  formatting of axes and labels. However, you can customise the parameters to <code>plt.plot</code>. For instance, instead of supplying the actual date,  you can provide the day:</p> <pre><code>plt.plot(daily_df.index.day, daily_df.values)\n</code></pre> <p>Use selection criteria on your DataFrame to choose which Series to plot.</p> <pre><code>plt.plot(df.index, df['java'])\n</code></pre> <p>You can use slicing to drop elements from the plot:</p> <pre><code>plt.plot(df.index[:-1], df['java'][:-1])\n</code></pre> <p>You can create a scatter plot by specifying a format string as the third  argument. By also specifying <code>data=</code> we can refer to the columns by name rather than as series objects: </p> <pre><code>plt.plot('EffectivePrice', 'UnitPrice', 'rx', data=df)\n</code></pre> <p>Format strings  are three-part formats to specify: </p> <ol> <li>Colour: [b = blue, k = black, r = red, g = green, m = magenta, c = cyan]</li> <li>Marker: ['.', ',', 'o', 'v', ...., 'x' ]</li> <li>Linestyle: ['-', '--', '-.', ':'] </li> </ol> <p>Histograms involve assigning values to 'bins' that can then be plotted by their frequency: </p> <pre><code>plt.hist(ws_df.UnitPrice, bins=10, color='skyblue', edgecolor='black')\n\n# Adding labels and title\nplt.xlabel('Costs')\nplt.ylabel('Frequency')\nplt.title('Cost Distributions')\n\nplt.savefig('reports/cost_distributions.png')\n</code></pre> <p>Note the use of <code>savefig</code> method to save the plot to a PNG file. </p>"},{"location":"python/pandas/data_visualisation/#plot-components","title":"Plot Components","text":"<p>The <code>Figure</code> object is the top-level component for <code>matplotlib</code>  visualisations. Use the <code>figure</code> method to create a figure: </p> <pre><code>fig = plt.figure()\n</code></pre> <p>The <code>subplots</code> method can be used to create a Figure with  Axes objects for subplots: </p> <pre><code>fig, axes = plt.subplots(nrows=2,ncols=2)\n</code></pre> <p>The above code creates a Figure with Axes for 4 plots laid-out in two rows and two columns. </p> <p>You can also create layouts by specifying a figure size and then  adding axes. Axes are specified as <code>[left, bottom, width, height]</code> as proportions of the figure size: </p> <pre><code>figure = plt.figure( figsize = ( 9, 9 ) )\n\n# Add axes [ 'left', 'bottom', 'width', 'height' ]\nbottom_left = figure.add_axes( [ 0, 0, 3/9, 5/9 ] )\ntop_left = figure.add_axes([ 0, 6/9, 3/9, 3/9 ] )\ntop_right = figure.add_axes( [ 4/9, 6/9, 5/9, 3/9 ] )\nbottom_right = figure.add_axes( [4/9, 0, 5/9, 5/9 ] )\n\nbottom_left.plot( [500, 600, 700], [22, 24, 26])\ntop_right.plot( [1,2,3,4], [2,4,6,8])\n</code></pre> <p><code>add_axes</code> can be used to add plots inside other plots: </p> <pre><code>fig = plt.figure(figsize=(9,9))\n\noutside = fig.add_axes([0.1, 0.1, 0.95, 0.95])\ninside = fig.add_axes([0.5, 0.5, 0.25, 0.25])\n</code></pre> <p>You can also plot mulitple columns on the same graph, and add some graph formating:</p> <pre><code>plt.figure(figsize=(16,10))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('No of Posts', fontsize=14)\nplt.ylim(0,35000)\n\nplt.plot(df.index, df['java'])\nplt.plot(df.index, df['python'])\n</code></pre> <p>To plot multiple columns use a <code>for</code> loop:</p> <pre><code>plt.figure(figsize=(16,10))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('No of Posts', fontsize=14)\nplt.ylim(0,35000)\n\nfor column in df.columns:\n    plt.plot(df.index, df[column], linewidth=3, label=df[column].name)\n\nplt.legend(fontsize=14)\n</code></pre> <p>For time series data you can specify a rolling mean to smooth out the data: instead of plotting the value of each data point, you can specify a window  to calculate an average value for each data point based on the values either side of the data point:</p> <pre><code>df['rolling_mem'] = df.mem_used_percent.rolling('3D').mean()\n</code></pre> <p>To calculate a rolling average, your time-series data must be monotonic:  that is the index must change in equal steps. Missing dates are not allowed. Check for monotonicity using either <code>df.index.is_monotonic_increasing</code> or <code>df.index.is_monotonic_decreasing</code>.</p> <p>Whereas a rolling window calculates an aggregate from the values within the  specified window, an <code>expanding</code> window, calculates the aggregate for all values up to the current value, that is a cumulative aggregate. </p> <pre><code>sub_df['CumulativeCost'] = sub_df['CostInBillingCurrency'].expanding().sum('CostInBillingCurrency')\n</code></pre> <p>The same assignment can be made using the <code>cumsum</code> function: </p> <pre><code>sub_df['CumulativeCost'] = sub_df['CostInBillingCurrency'].cumsum()\n</code></pre> <p>Plotting two columns with varying value-ranges can look untidy and make trend-spotting difficult. Instead you can specify two different y-axes for each plot to make comparison easier:</p> <pre><code>ax1 = plt.gca() # gets the current axis\nax2 = ax1.twinx() # create another axis that shares the same x-axis\n\nax1.plot(sets_by_year.index[:-2], sets_by_year.set_num[:-2], color='g')\nax2.plot(themes_by_year.index[:-2], themes_by_year.nr_themes[:-2], color='b')\n\nax1.set_xlabel('Year')\nax1.set_ylabel('Number of Sets', color='g')\nax2.set_ylabel('Number of Themes', color='b')\n</code></pre> <p>If your xticks overlap, you can specify a rotation to make them readable:</p> <pre><code>plt.xticks(fontsize=14, rotation=45)\n</code></pre> <p>You can also use locator functions for marking the x- and y- axes:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ndf_unemployment = pd.read_csv('UE Benefits Search vs UE Rate 2004-19.csv')\ndf_unemployment['MONTH'] = pd.to_datetime(df_unemployment['MONTH'])\n\nroll_df = df_unemployment[['UE_BENEFITS_WEB_SEARCH', 'UNRATE']].rolling(window=6).mean()\nroll_df['month'] = df_unemployment['MONTH']\n\n\nplt.figure(figsize=(16,10))\nplt.title('Rolling Web Searches vs Unemployment Rate', fontsize=20)\nplt.xticks(fontsize=14, rotation=45)\nplt.yticks(fontsize=14)\n\nax1 = plt.gca()\nax2 = ax1.twinx()\nax1.grid(color='gray', linestyle='--')\n\nax1.set_ylabel('Web Searches', fontsize=16, color='indianred')\nax2.set_ylabel('Unemployment Rate', fontsize=16, color='cadetblue')\n\nax1.set_xlim(roll_df.month[5:].min(), roll_df.month[5:].max())\n\nyears = mdates.YearLocator()\nmonths = mdates.MonthLocator()\nyears_fmt = mdates.DateFormatter('%Y')\nax1.xaxis.set_major_locator(years)\nax1.xaxis.set_major_formatter(years_fmt)\nax1.xaxis.set_minor_locator(months)\n\nax1.plot(roll_df.month[5:], roll_df.UE_BENEFITS_WEB_SEARCH[5:], color='indianred', linewidth=3, marker='o')\nax2.plot(roll_df.month[5:], df_unemployment.UNRATE[5:], color='cadetblue', linewidth=3)\n</code></pre> <p>Pandas provides a number of graph types including line, area, bar, pie and scatter:</p> <ul> <li><code>plt.plot</code> for a line chart</li> <li><code>plt.scatter</code> for a scatter plot</li> <li><code>plt.bar</code> for a bar chart</li> </ul>"},{"location":"python/pandas/data_visualisation/#plotly","title":"Plotly","text":"<p>Bar charts are a good way to visualise 'categorical' data. You can use  <code>value_counts()</code> to quickly create categorical data:</p> <pre><code>ratings = df.value_counts('Content_Rating')\n</code></pre> <p>Given a dataframe that contains the following data:</p> <pre><code>Content_Rating\nEveryone           6621\nTeen                912\nMature 17+          357\nEveryone 10+        305\nAdults only 18+       3\nUnrated               1\nName: count, dtype: int64\n</code></pre> <p>we can present this as a pie chart using plotly:</p> <pre><code>fig = px.pie(\n    labels=ratings.index,\n    values=ratings.values,\n    names=ratings.index,\n    title=\"Content Rating\"\n)\nfig.show()\n</code></pre> <p>We can change the location of the text on the pie chart using <code>update_traces()</code>:</p> <pre><code>fig = px.pie(\n    labels=ratings.index,\n    values=ratings.values,\n    title='Content Rating',\n    names=ratings.index,\n)\nfig.update_traces(\n    textposition='outside', \n    textinfo='percent+label'\n)\nfig.show()\n</code></pre> <p>To format the pie chart as a 'doughnut chart' add the 'hole' parameter:</p> <pre><code>fig = px.pie(\n    labels=ratings.index,\n    values=ratings.values,\n    title='Content Rating',\n    names=ratings.index,\n    hole=0.4\n)\nfig.update_traces(\n    textposition='inside',\n    textfont_size=15,\n    textinfo='percent'\n)\nfig.show()\n</code></pre> <p>Which produces:</p> <p></p>"},{"location":"python/pandas/data_wrangling/","title":"Data Cleansing","text":"<p>Before carrying out any data analysis tasks, the data should be prepared for  analysis. This process will involve a number of tasks including: </p> <ul> <li>Renaming columns</li> <li>Re-ordering columns and rows</li> <li>Data-type conversions</li> <li>Deduplication</li> <li>Handling missing or invalid data</li> <li>Filtering out unneccessary data</li> </ul> <p>To avoid altering our original data we can use <code>copy</code> to create a new copy of the DataFrame:</p> <pre><code>df_clean = df.copy()\n</code></pre>"},{"location":"python/pandas/data_wrangling/#columns","title":"Columns","text":"<p>The <code>rename</code> method can be used to rename columns in your DataFrame:</p> <pre><code>df_renamed = df.rename(\n    columns={\n        \"Date of Publication\": \"publication_date\",\n        \"Date of Death\": \"death_date\",\n        \"Date of Birth\": \"birth_date\",\n    },\n)\n</code></pre> <p>You can also use lambda functions to achieve the same result:</p> <pre><code>df_renamed = df.rename(lambda x: x.lower().replace(' ', '_'), axis=1)\n</code></pre> <p>Sort columns using <code>sort_index(axis=1)</code>. </p> <p>You can use pandas conversion functions to change the datatypes for  columns: </p> <pre><code>df_renamed.date_of_birth = pd.to_datetime(df_renamed.date_of_birth)\n</code></pre> <p>Use <code>format='mixed'</code> if your date strings are in different formats:</p> <pre><code>data = {\n    'First Name': ['John', 'Jack', 'Jane', 'Jill', 'Jake'],\n    'Family Name': ['Johnson', 'Jackson', 'Janeson', 'Jillson', 'Jakeson'],\n    'Date of Birth': ['2001-05-20', '1980-12-13 12:24:00', \n        '1999-12-31 03:30:00', '1964-06-20', '2020-05-12 17:32'],\n}\n\ndf = pd.DataFrame(data)\ndf_renamed = df.rename(lambda x: x.lower().replace(' ', '_'), axis=1)\ndf_renamed.date_of_birth = pd.to_datetime(\n    df_renamed.date_of_birth, format='mixed'\n)\n</code></pre> <p>Altering the data type of a column containing dates from 'object' to 'datetime' means we get fuller statistics via the <code>describe</code> method and can carry out operations associated to datetime objects:</p> <pre><code>df_renamed['time_of_birth'] = df_renamed.date_of_birth.dt.time\ndf_renamed['day_of_birth'] = df_renamed.date_of_birth.dt.date\n</code></pre> <p>To convert strings to numeric data, you can perform string formatting and then use the pandas <code>to_numeric</code> function: </p> <pre><code># remove commas\ndf_clean['Installs'] = df_clean['Installs'].astype(str).str.replace(',',\"\")\n# remove currency symbol\ndf_clean['Installs'] = df_clean['Installs'].astype(str).str.replace('\u00a3',\"\")\n# convert to numeric\ndf_clean['Installs'] = pd.to_numeric(df_clean['Installs'])\n</code></pre> <p><code>astype</code> can be used to perform some type conversions. Float64 values can be converted to Int64 values (<code>astype(int))</code>. No rounding is performed, everything after the decimal point is lost: </p> <pre><code>data = {\n    'forename': ['John', 'Jack', 'Jane', 'Jill', 'Jake'],\n    'surname': ['Johnson', 'Jackson', 'Janeson', 'Jillson', 'Jakeson'],\n    'date_of_birth': ['2001-05-20', '1980-12-13 12:24:00', '1999-12-31 03:30:00', '1964-06-20', '2020-05-12 17:32'],\n    'sats_score': np.random.random(5) * 100,\n    'marital_status': ['divorced', 'beheaded', 'died', 'divorced', 'beheaded']\n}\n\ndf1 = pd.DataFrame(data)\ndf1['sats_score_truncated'] = df1.sats_score.astype('int')\n</code></pre> <p>Rather than changing the precision of your Float64 objects, you can change the display format of Float64 values, to show only the first 2 decimal places: </p> <pre><code>pd.set_option('display.float_format', lambda x: '%.2f' % x)\n# Alternate method\npd.options.display.float_format = \"{:,.2f}\".format\n\n</code></pre> <p>While this will change the formatting for pandas objects such as DataFrames or Series, <code>print</code> statements and calculations will still display with scientific-precision. To alter this behaviour use <code>round</code> or a format: </p> <pre><code>df.CostInBillingCurrency.sum().round(2)\n# Alternate method\n'{:,.2f}'.format(df.CostInBillingCurrency.sum())\n</code></pre> <p>The marital status column can be converted from an 'Object' type to a 'Category' type using <code>astype</code>: </p> <pre><code>df1.marital_status = df1.marital_status.astype('category')\n</code></pre> <p>Use the <code>drop()</code> method to remove unwanted rows or columns. Set <code>inplace=True</code> to alter the current DataFrame or use assignment to capture the resulting  DataFrame. </p> <p><code>drop</code> defaults to removing rows (<code>axis=0</code>), but we can also drop columns either by specifying <code>axis=1</code> or by providing a list to the <code>columns</code> parameter: </p> <pre><code># creates new dataframe and leaves the original unchanged\nnameless_df = df.drop(columns=['Forename', 'Surname'])\n\n# affects the original dataframe\ndf.drop(['Forename', 'Surname'], axis=1, inplace=True)\n</code></pre> <p><code>dropna</code> can be used to drop columns by specifying the axis, and a  threshold of rows that contain null values: </p> <pre><code>df_clean = df.dropna(\n    axis='columns',\n    thresh=3 # keep if 3 or more non-null values\n)\n\ndf_clean = df.dropna(\n    axis=1,\n    thresh=df.shape[0]*.75 # keep if 75% or more non-null values\n)\n</code></pre> <p>We can also use <code>del df['column_name']</code> to remove a specific column from  a DataFrame. </p>"},{"location":"python/pandas/data_wrangling/#indexes","title":"Indexes","text":"<p>If no index is specified, pandas will add an index when creating your DataFrame. This will be a <code>RangeIndex</code>, containing integers from 0 to n-1, where n is the number of rows in your dataset. </p> <p>You can specify an index using a specific column from your data, or generating one manually: </p> <pre><code>data = {\n    'forename': ['John', 'Jack', 'Jane', 'Jill', 'Jake'],\n    'surname': ['Johnson', 'Jackson', 'Janeson', 'Jillson', 'Jakeson'],\n    'date_of_birth': ['2001-05-20', '1980-12-13 12:24:00', '1999-12-31 03:30:00', '1964-06-20', '2020-05-12 17:32'],\n}\n\ndf1 = pd.DataFrame(data, index=pd.date_range(start='2025-06-01', periods=5, freq='D'))\n\ndf2 = pd.DataFrame(data)\ndf2.date_of_birth = pd.to_datetime(df2.date_of_birth, format='mixed')\ndf2.index = df2.date_of_birth\n</code></pre> <p>Note that the number of elements in the <code>date_range</code> should match the number of rows in the DataFrame. Both <code>df1</code> and <code>df2</code> now have an index of type `DatetimeIndex'. </p> <p>You can localise your DatetimeIndex (make them timezone-aware) and convert them to UTC: </p> <pre><code># df2 = df2.tz_localize('GMT+0')\ndf2 = df2.tz_localize('CET')\ndf2 = df2.tz_convert('UTC')\n</code></pre> <p>A DatetimeIndex or a PeriodIndex can be used to slice a DataFrame by dates, but this only works if your Datetime Index has been sorted first: </p> <pre><code>df_dated.set_index('date_of_birth', inplace=True)\ndf_dated.sort_index(inplace=True)\n\ndf_dated['1964':'2001']\n\ndf_dated['2001-05':'2020']\ndf_dated['200105':'2020']\n\ndf_dated['1980': '2025-01-01']\n\n</code></pre> <p>A DatetimeIndex can be converted to PeriodIndex, where the index values are converted to the specified period (e.g. Week, Month, Year):</p> <pre><code>df2 = df2.to_period('M')\n</code></pre> <p>Use <code>sort_index</code> to sort data by index value. Both <code>sort_index</code> and <code>sort_values</code> return a new DataFrame, so you have to use assignment to keep the sorted data, or use <code>inplace=True</code>. </p> <p><code>reset_index</code> will copy the current index on a DataFrame to a new  column (preserving it's current name), and create a new RangeIndex  for the DataFrame. </p> <p><code>set_index</code> can be used to drop the current index, and replace it with one or more columns from the DataFrame. Rows from a MultiIndex are accessed using tuples: <code>(outer_index, inner_index)</code>. If you set a MultiIndex,  you can undo this later with <code>unstack</code>. The <code>unstack</code> method will move  the innermost index column to the DataFrame columns. </p> <p>Note that <code>df.set_index('column_name', inplace=True)</code> works differently to <code>df.index = df.column_name</code>: <code>df.index</code> will copy the column from the  data and use this as the index, whereas <code>set_index</code> will move the column from the data and use this as the index.</p> <p><code>reindex</code> can be used to align the DataFrame to a new index. Where the DataFrame is missing values for the new index, you can pick a method to fill-in the row values: </p> <ul> <li>ffill - forward fill from nearest previous value</li> <li>bfill - back fill from nearest next value</li> <li>nearest - fill from the nearest value</li> </ul>"},{"location":"python/pandas/data_wrangling/#derived-data","title":"Derived Data","text":"<p>You can add new columns to a DataFrame using assignments:</p> <pre><code>df[\"Forename Lower Case\"] = df[\"Forename\"].str.lower()\n\ndf['Retired'] = df.Age &gt; 67\n</code></pre> <p>Use the <code>map</code> function to apply a dictionary lookup to add a column:</p> <pre><code>\nworkspace_names = {\n    'rg1': 'project one',\n    'rg2': 'project_two',\n    ...\n    'rgN': 'project_N',\n}\n\ndf['ProjectName'] = df['ResourceGroup'].map(workspace_names)\n</code></pre> <p>The <code>assign</code> method allows us to add multiple columns at once. However, <code>assign</code> does not change the original DataFrame, but returns a new one.  If you want to change the original, just assign the return value back to  the original DataFrame: </p> <pre><code>df = df.assign(\n    is_retired=df.Age &gt;= 67,\n    is_working_age=(df.Age &gt;= 18) &amp; (df.Age &lt; 67),\n    is_child=df.Age &lt; 18\n    income=lambda x: np.where(x.salary.isnull(), x.pension, x.salary),\n)\n</code></pre> <p>The same <code>assign</code> statement can be written with <code>lambda</code> functions: </p> <pre><code>df = df.assign(\n    is_retired=lambda x: x.Age &gt;= 67,\n    is_working_age=lambda x: x.Age.between(18,67,inclusive='left'),\n    is_child=lambda x: x.Age &lt; 18\n)\n</code></pre> <p>Using lambda functions in your assign statements offers the benefit of  being able to reference other columns that are being assigned in the  same statement. </p>"},{"location":"python/pandas/data_wrangling/#rows","title":"Rows","text":"<p>To drop rows we need to provide a list of indices: <code>df.drop([1,2], inplace=True)</code>.</p> <p>Use <code>isna</code> to find rows with null values in a specified column(s):</p> <pre><code>df_null_rating_or_age = df[df.Rating.isna() | df.Age.isna()]\n</code></pre> <p>If used on the DataFrame, <code>isna</code> will return a DataFrame with the values  replaced with TRUE or FALSE to indicate null or non-null values. </p> <p>If you use <code>dropna</code> on the DataFrame, this will return a DataFrame containing only rows  where none of the values are null: in other words, if any value in the row is  null, then it will be dropped. This behaviour can be changed using the <code>how</code>  parameter. The default behaviour is <code>any</code>: but you can set the parameter to <code>all</code>,  so that only rows where all values are null are dropped. Combining this with the <code>subset</code> parameter allows you to drop only rows where all the values in the  specified columns are null:</p> <pre><code>## drop rows where the subset columns are all null\ndf_clean = df.dropna(how=all, subset=['home', 'var', 'lib'])\n\n## drop rows where there is a null value in any of the subset columns\ndf_clean = df.dropna(how=any, subset=['home', 'var', 'lib'])\n</code></pre> <p><code>fillna</code> can be used replace <code>NaN</code> values: </p> <pre><code>df = df.assign(\n    salary=lambda x: x.salary.fillna('0.00'),\n    free_space=lambda x: x.free_space.ffill(),\n    inplace=True,\n)\n\ndf_clean.loc[:, 'pension'].fillna('0.00', inplace=True)\n</code></pre> <p>Use <code>inplace=True</code> if you don't want to create a new dataframe.</p> <p>You can also use <code>apply</code> to apply the same fill strategy to all columns in the  DataFrame or a list of columns: </p> <pre><code>df_clean = df.apply(lambda x: x.ffill(inplace=True))\n</code></pre> <p>Note that apply does not have a <code>inplace</code> option, so you have to reassign the DataFrame or DataFrame slice: </p> <pre><code>df.loc[:, ['name', 'toy']] = df.loc[:, ['name', 'toy']].apply(\n            lambda x: x.str.replace('Bat', 'Cat')\n)\n</code></pre> <p>Apply lets you run vectorised operations on rows or columns, which is the  preferred method in Pandas. Although methods like <code>iterrows</code> and <code>iteritems</code>  exist, these should be used only as a last resort as if is less efficient to  loop through items one-by-one in Pandas. There is an <code>applymap</code> method to run  a function that has not been vectorised, or <code>np.vectorize</code> to vectorise your own functions.</p> <p><code>duplicated</code> returns all duplicated rows, apart from the first occurrence of the row.  The <code>subset</code> parameter can be used to specify which columns to use for the comparison. Use <code>drop_duplicates</code> to remove the duplicated rows (keeping the first occurrence):</p> <pre><code>df_clean = df.drop_duplicates(subset = ['App', 'Type', 'Price'])\n</code></pre> <p>You can use filtering to drop unwanted values:</p> <pre><code>df_clean = df_clean[df_clean.Installs &gt; 300]\n</code></pre>"},{"location":"python/pandas/data_wrangling/#reshaping-dataframes","title":"Reshaping DataFrames","text":"<p>The <code>pivot()</code> method can be used to reshape data from long-format to  wide-format:</p> <pre><code>import pandas as pd\n\ndata = [\n    ('2025-01-06', 'home', 97),\n    ('2025-01-06', 'opt', 36),\n    ('2025-01-06', 'var', 83),\n    ('2025-01-07', 'home', 96),\n    ('2025-01-07', 'opt', 36),\n    ('2025-01-07', 'var', 83),\n    ('2025-01-08', 'home', 95),\n    ('2025-01-08', 'opt', 45),\n    ('2025-01-08', 'var', 63),\n]\n\ndf = pd.DataFrame(data, columns=['date', 'mount_point', 'free_space'])\ndf.date = pd.to_datetime(df.date)\n\npivot_df = df.pivot(index='date', columns='mount_point', values='free_space')\n\n</code></pre> <p>If you pass in a list to the values parameter, you will have a hierarchical index in the columns, which you will access using <code>[&lt;value_column_name&gt;][&lt;column_name&gt;]</code>: </p> <p>The reverse of <code>pivot()</code> is <code>melt()</code>, which will turn data in wide-format  to long-format:</p> <pre><code>wide_data = [\n    ('2025-01-06', 97, 36, 83),\n    ('2025-01-07', 96, 36, 83),\n    ('2025-01-08', 95, 45, 63)\n]\ndf2 = pd.DataFrame(wide_data, columns=['date', 'home', 'opt', 'var'])\n\nmelt_df = pivot_df.melt(\n    id_vars='date',\n    value_vars=['home', 'opt', 'var'],\n    var_name='mount_point'\n    value_name='free_space',\n)\n</code></pre> <p>The <code>merge</code> method works similar to an SQL JOIN statement, producing a DataFrame  that results from combining DataFrames using a common Series:</p> <pre><code>patient_result_data = pd.merge([\"patient\", \"pathology\"], how=\"left\", on=\"patient_id\")\n</code></pre> <p>If the two tables to merge have the same data in different column names, use the  <code>left_on</code> and <code>right_on</code> parameters:</p> <pre><code>patient_result_data = pd.merge([\"patient\", \"pathology\"], \n    how=\"left\", left_on=\"patient_id\", right_on=\"hospital_number\")\n</code></pre>"},{"location":"python/pandas/data_wrangling/#working-with-datetime-values","title":"Working with Datetime Values","text":"<p>Use the <code>to_datetime()</code> method to work with datetime data in your columns.: Use the <code>dayfirst</code> parameter for dates in \"%d/%m/%Y\" format. </p> <pre><code>df = pd.read_csv(\"ukgovunclaimedestates.csv\", parse_dates=True)\n\ndf[\"Date of Birth\"] = pd.to_datetime(df[\"Date of Birth\"], dayfirst=True)\ndf[\"Date of Death\"] = pd.to_datetime(df[\"Date of Death\"], dayfirst=True)\n\nprint(df.groupby(df[\"Date of Birth\"].dt.year)[\"Date of Birth\"].count())\n\ndf[\"Age\"] = df[\"Date of Death\"].dt.year - df[\"Date of Birth\"].dt.year\n</code></pre> <p>Converting columns to DateTime objects, provides access to utility methods to extract 'year', 'month', 'day', 'weekday' and perform calculations via  the <code>dt()</code> methods. If you set the index (using the <code>set_index()</code> method) to a DateTime object, then you can use the same methods on the index. </p> <pre><code>df[\"date_of_birth\"] = pd.to_datetime(df[\"Date of Birth\"], dayfirst=True)\ndf.set_index(\"date_of_birth\", inplace=True)\n\nprint(df.index.year)\n</code></pre>"},{"location":"python/pandas/data_wrangling/#working-with-string-values","title":"Working with String Values","text":"<p>Text data is stored in 'Object' format in a DataFrame. Using the <code>str()</code> accessor on  text data, allows access various string methods,  including <code>lower</code>, <code>split</code>, <code>replace</code></p> <p><code>split</code> returns a number of elements from a single element: use the the <code>get</code> method to select which element you want:</p> <pre><code>df[\"Surname\"] = df[\"Name\"].str.split(\", \").str.get(0)\ndf[\"Forename\"] = df[\"Name\"].str.split(\", \").str.get(1)\n</code></pre> <p><code>replace</code> uses a dictionary to replace values:</p> <pre><code>df[\"Gender\"] = df[\"Sex\"].replace({\"male\": \"m\", \"female\": \"f\"})\n</code></pre>"},{"location":"python/pandas/data_wrangling/#row-comprehensions","title":"Row Comprehensions","text":"<p>The <code>to_dict()</code> method of a DataFrame will produce a dictionary mapping the column names to  an array of the column values. If you want a dictionary mapping row values to other row values you can use the <code>iterrows()</code> method. </p> <p>The <code>iterrows()</code> method returns a list of tuples containing the index and row values for  each row. This can be used in a comprehension to create a new dictionary:</p> <pre><code>df = pd.read_csv(\"ukgovunclaimedestates.csv\", parse_dates=True)\n\nperson_dict = {row[\"BV Reference\"]:row[\"Surname\"] for (index, row) in df.iterrows()}\n</code></pre>"},{"location":"python/pandas/pandas/","title":"Notes on Pandas","text":"<p>Notes from the Pandas  Getting Started  guide</p>"},{"location":"python/pandas/pandas/#series","title":"Series","text":"<p>A <code>pandas.Series</code> object is a data structure for an array of data of a single type, and can be thought of as a column in a table. Create a Series by  providing an array of values: </p> <pre><code>import numpy as np\nimport pandas as pd\n\nnp.random.seed(0) # set a seed so that the random numbers are always the same\npd.Series(np.random.rand(5), name='random')\n</code></pre> <p>In addition to the data we provide the Series object, pandas adds in Index object representing the row numbers. The Index object can be used to perform  element-wise operations on Series objects. For example, addition using two Series objects, will add the values for the elements with matching index values. Where the indexes do not match, the result will return a null value  (<code>NaN</code> or <code>None</code>) for those index values. </p>"},{"location":"python/pandas/pandas/#dataframes","title":"DataFrames","text":"<p>A DataFrame is a 2-dimensional data structure used to store data in columns. </p> <p>To create a DataFrame manually, use a dictionary of lists:</p> <pre><code>df = pd.DataFrame(\n  {\n    'Forename': ['Jeff', 'James', 'Eric', 'Clark'],\n    'Surname': ['Jefferson', 'Jameson', 'Ericsson', 'Clarkson'],\n    'Date of Death': ['15/12/2021', '12/05/2002', '25/10/2023', '28/07/2012'],\n    'Place of Death': [np.random.choice(['Norfolk', 'Shropshire', 'Devon', 'Bedfordshire']) for _ in range(4)] ,\n    'Deceased': [np.random.choice(['Yes', 'No']) for _ in range(4) ],\n    'Age' : np.random.rand(4) * 100\n  }\n)\n</code></pre> <p>Or a list of dictionaries: </p> <pre><code>df2 = pd.DataFrame([\n    {'forename': 'Jeff', 'surname': 'Jefferson', 'date of death': '15/12/2021'},\n    {'forename': 'James', 'surname': 'Jameson', 'date of death': '01/10/2021'},\n    {'forename': 'Eric', 'surname': 'Ericsson', 'date of death': '11/05/2021'},\n    {'forename': 'Clark', 'surname': 'Clarkson', 'date of death': '25/02/2021'},\n])\n</code></pre> <p>Or a list of tuples:</p> <pre><code>df3 = pd.DataFrame([\n    ('Jeff', 'Jefferson', '15/12/2021'),\n    ('James', 'Jameson', '01/10/2021'),\n    ('Eric', 'Ericsson', '11/05/2021'),\n    ('Clark', 'Clarkson', '25/02/2021')],\n    columns=['forename', 'surname', 'date of death']\n)\n</code></pre> <p>Or a list of lists:</p> <pre><code>df5 = pd.DataFrame([\n        ['Jeff', 'Jefferson', '15/12/2021'],\n        ['James', 'Jameson', '01/10/2021'],\n        ['Eric', 'Ericsson', '11/05/2021'],\n        ['Clark', 'Clarkson', '25/02/2021']\n    ],\n    columns=['forename', 'surname', 'date of death']\n)\n</code></pre> <p>Or a <code>numpy</code> array:</p> <pre><code>df4 = pd.DataFrame(\n    np.array([\n        ['Jeff', 'Jefferson', '15/12/2021'],\n        ['James', 'Jameson', '01/10/2021'],\n        ['Eric', 'Ericsson', '11/05/2021'],\n        ['Clark', 'Clarkson', '25/02/2021']\n    ]),\n    columns=['forename', 'surname', 'date of death']\n)\n</code></pre>"},{"location":"python/pandas/pandas/#external-data","title":"External Data","text":"<p>DataFrames can be created from CSV files using the pandas <code>read_csv</code> function:</p> <pre><code>df = pd.read_csv(\"data/unclaimedestates.csv\", parse_dates=True)\n</code></pre> <p>The <code>read_csv</code> function provides additional arguments to allow you to  specifiy seperators, delimiters, etc. Use <code>help(pd.read_csv)</code> or  <code>pd.read_csv?</code> for the full function definition. </p> <p>There are functions for reading data in various formats, all beginning with a <code>read_</code> prefix: e.g. <code>read_json</code>, <code>read_excel</code>, <code>read_sql</code>. Try: </p> <pre><code>functions = [function for function in dir(pd) if function.startswith('read_')]\n</code></pre> <p>To read data from a database, you need to provide a both a query or table name and a connection object: </p> <pre><code>import pandas as pd\nfrom sqlalchemy import URL, create_engine\nimport psycopg2\n\npsql_connection_string = URL.create(\n    'postgresql',\n    username='user_name',\n    password='passwd',\n    host='server_name,\n    port=5432,\n    database='database_name'\n)\nconn = create_engine(psql_connection_string)\n\nquery = '''\nSELECT * FROM\n\"schema_name\".\"table_name\"\nWHERE collected_date &gt; CURRENT_DATE - INTERVAL '60 days'\nORDER BY collected_date desc\n'''\n\ndf = pd.read_sql_query(query,con=conn)\n</code></pre> <p>DataFrames can also be written to external files  using the corresponding <code>to_</code> methods: e.g. <code>to_json</code>, <code>to_excel</code>, <code>to_sql</code>. With the <code>to_sql</code> function you can provide the table name, connection and what action to take if the table already  exists: </p> <pre><code>df.to_sql(name='users', con=connection, if_exists='replace')\n</code></pre> <p>Pandas <code>to_</code> functions will also allow to you include or exclude the DataFrame index in the output: </p> <pre><code>df.to_excel(\"recipes.xlsx\", sheet_name=\"flapjack\", index=False)\n</code></pre>"},{"location":"python/pandas/pandas/#examining-data","title":"Examining Data","text":"<p>DataFrame objects provide a number of attributes and functions that  you can use to examine the data frame. </p> Attribute Description shape number of rows and columns columns names of the columns dtypes data type for each column <p>All the output from the <code>shape</code>, <code>columns</code>, and <code>dtypes</code> attributes is  also provided through the <code>info</code> function, along with details on the index and memory usage: </p> <pre><code>df.info()\n</code></pre> <p>The <code>describe</code> method provides summary data (min, max, mean, count and percentiles)  for the numeric columns, but can also be used to get some summary data  (count, unique values, mode, frequency of mode) for non-numeric data:</p> <pre><code>df.describe(include=np.object)\n# or\ndf.describe(include='all')\n</code></pre> <p>By default <code>describe</code> provides the 0.25, 0.5, 0.75 percentiles but you can select  these with the <code>percentiles</code> parameter:</p> <pre><code>df.describe(percentiles=[0.05, 0.50, 0.95])\n</code></pre> <p><code>describe</code> only provides summary statistics for non-null values.</p> <p>Each summary statistic is also available via separate summary methods that can be applied to either the DataFrame or Series objects, e.g <code>min</code>, <code>max</code>,  <code>idxmin</code>, <code>idxmax</code>, <code>sum</code>, <code>mean</code>, <code>median</code>. For Series objects additional  summary methods exist: <code>unique</code>, <code>value_counts</code> and <code>mode</code>. </p>"},{"location":"python/pandas/pandas/#selecting-data","title":"Selecting Data","text":"<p>Display the dataframe by calling its name. This will display the first 5 rows  and the last 5 rows. To show the first n rows, use the <code>head</code> method, or <code>tail</code> for the last n rows, or <code>sample</code> for n random values:</p> <pre><code>df.sample(10)\n</code></pre> <p>Each column in a DataFrame is a Series. To select a column from a DataFrame,  use the column label between square brackets. For multiple columns supply a  list:</p> <pre><code>df[[\"Place of Death\", \"Date of Birth\"]\n</code></pre> <p>DataFrames support <code>slicing</code> operations, which can be combined with column selection: </p> <pre><code>df[[\"Place of Death\", \"Date of Birth\"]][5:15]\n# Order doesn't matter\ndf[5:15][[\"Place of Death\", \"Date of Birth\"]]\n</code></pre> <p>For date indexes, slicing can also be done using dateparts or ranges: </p> <pre><code>df[2024]\ndf[2024-05]\ndf[2024-01:2024-04]\n</code></pre> <p>However, using slicing and selection to assign values to a DataFrame is not the recommended approach: instead you should use the <code>loc</code> or  <code>iloc</code> methods. <code>loc</code> and <code>iloc</code> accept two <code>labels</code>:  a row selector and a column selector. Either selector can be  expressed as a single value, a list or a slice. :</p> <pre><code>df.loc[2, 'Surname']\ndf.loc[[2,3], 'Surname']\ndf.loc[2:3, 'Surname']\n\ndf.loc[df.Surname == 'Johnson', ['Forename','Surname']]\ndf.loc[df.Surname == 'Johnson', 'Forename':'Surname']\n\ndf.loc[[df.Age.idxmin(), df.Age.idxmax()], :]\n</code></pre> <p>With <code>loc</code>, slicing operations are inclusive of the end index if provided. <code>iloc</code>  instead follows the standard python-slicing approach, and excludes the end value of a slice if provided. <code>iloc</code> expects integer values for both row selectors and  column selectors:</p> <pre><code># returns only the value at row 2, column 1 and 2\ndf.iloc[2:3, 1:3]\n\n# returns values at rows 2 and 3, column 1, 2 and 3\ndf.iloc[[2,3],[1,2,3]]\n</code></pre> <p><code>iloc</code> allows slicing for both row and column selectors. </p> <p>For selecting a single value from a DataFrame, the <code>at</code> and <code>iat</code> methods provide better performance: </p> <pre><code>wanted = df.at[2,'Surname']\nwanted = df.iat[2,1]\n</code></pre>"},{"location":"python/pandas/pandas/#filtering-data","title":"Filtering Data","text":"<p>You can filter the output using a condition inside the selection brackets:</p> <pre><code>df[df[\"Date of Birth\"] &lt; '01/01/1970']\n</code></pre> <p>The <code>isin()</code> method can be used to check for multiple conditions on  a column:</p> <pre><code>df[df[\"Surname\"].isin([\"Smith\", \"Jones\"])]\n</code></pre> <p>You can also use the 'or' operator to acheve the same result: </p> <pre><code>df[(df[\"Surname\"] == \"Jones\") | (df[\"Surname\"] == \"Smith\")]\n</code></pre> <p>The AND Operator <code>(&amp;)</code> can be used to specify multiple conditions that should all return <code>True</code>. The Negation Operator <code>(~)</code> can  be used to negate the return value of a condition: </p> <pre><code>df[~(df.Surname == 'Jameson')]\n</code></pre> <p>To filter out rows where a particular column has Null values, use the <code>notna()</code> method:</p> <pre><code>df[df[\"Executors\"].notna()]\n</code></pre> <p>We can also use strings and regular expressions: </p> <pre><code># contains 'son' anywhere in the string\ndf[df.Surname.str.contains('son')]\n\n## contains 'son' at the end of the string\ndf[df.Surname.str.contains(r'son$')]\n\n## contains 'Jeff' or 'James' at the beginning of the string\ndf[df.Surname.str.contains(r'^Jeff|James')]\n</code></pre> <p>To filter numeric values with a lower and upper range use <code>between</code>:</p> <pre><code>df[df.Age.between(10,32, inclusive='both')]\n</code></pre> <p>These filtering techniques can also be used for row labels in both the <code>loc</code> or <code>iloc</code> operators.</p>"},{"location":"python/pandas/pandas/#derived-data","title":"Derived Data","text":"<p>To avoid altering our original data we can use <code>copy</code> to create a new copy of the DataFrame:</p> <pre><code>df_clean = df.copy()\n</code></pre> <p>You can add new columns to a DataFrame using assignments:</p> <pre><code>df[\"Forename Lower Case\"] = df[\"Forename\"].str.lower()\n\ndf['Retired'] = df.Age &gt; 67\n</code></pre> <p>The <code>assign</code> method allows us to add multiple columns at once. However, <code>assign</code> does not change the original DataFrame, but returns a new one.  If you want to change the original, just assign the return value back to  the original DataFrame: </p> <pre><code>df = df.assign(\n    is_retired=df.Age &gt;= 67,\n    is_working_age=(df.Age &gt;= 18) &amp; (df.Age &lt; 67),\n    is_child=df.Age &lt; 18\n)\n</code></pre> <p>The same <code>assign</code> statement can be written with <code>lambda</code> functions: </p> <pre><code>df = df.assign(\n    is_retired=lambda x: x.Age &gt;= 67,\n    is_working_age=lambda x: x.Age.between(18,67,inclusive='left'),\n    is_child=lambda x: x.Age &lt; 18\n)\n</code></pre> <p>The <code>rename()</code> method can be used to rename columns:</p> <pre><code>df_renamed = df.rename(\n    columns={\n        \"Date of Publication\": \"publication_date\",\n        \"Date of Death\": \"death_date\",\n        \"Date of Birth\": \"birth_date\",\n    }\n)\n</code></pre>"},{"location":"python/pandas/pandas/#data-joins","title":"Data Joins","text":"<p>The <code>concat()</code> method can be used to combine two tables with a similar structure. You can specify an axis of '0' to add the second DataFrame as new rows, or an  axis of '1' to add the DataFrame as new columns. </p> <p>To join the rows of two tables use:</p> <pre><code>disk_data = pd.concat([disk_data_server_1, disk_data_server_2], axis=0)\n</code></pre> <p>If either DataFrame contains columns not found in the other, these will be added as new columns in the new DataFrame, and the rows where these columns did not exist, will be filled with <code>NaN</code>. This behaviour can be change by specifying the join type as 'inner': which only keeps columns that are common to the input DataFrames. </p> <p>The 'ignore_index' option can be used to create a new index for the new  DataFrame. If you wish to keep the original indices from both input DataFrames,  you can optionally add an additional (hierarchical) row index to identify which  row came from which input DataFrame:</p> <pre><code>disk_data = pd.concat([disk_data_server_1, disk_data_server_2], axis=0, keys=[\"srv1\", \"srv2\"])\n</code></pre> <p>This produces a DataFrame with a multi-index. To access entries by index value,  supply a tuple or list of tuples: </p> <pre><code>disk_data.loc[[('srv2',0),('srv2',1)],:]\n</code></pre> <p>When using <code>concat</code> in a columnwise join (<code>axis=1</code>), Pandas uses the row indexes  to make the joins. For rows that don't share a common index, values are filled with <code>NaN</code>. </p>"},{"location":"python/pandas/pandas/#data-cleansing","title":"Data Cleansing","text":"<p>Use the <code>drop()</code> method to remove unwanted rows or columns. Set <code>inplace=True</code> to alter the current DataFrame or use assignment to capture the resulting  DataFrame. </p> <p><code>drop</code> defaults to removing rows (<code>axis=0</code>), but we can also drop columns either by specifying <code>axis=1</code> or by providing a list to the <code>columns</code> parameter: </p> <pre><code># creates new dataframe and leaves the original unchanged\nnameless_df = df.drop(columns=['Forename', 'Surname'])\n\n# affects the original dataframe\ndf.drop(['Forename', 'Surname'], axis=1, inplace=True)\n</code></pre> <p>We can also use <code>del df['column_name']</code> to remove a specific column from  a DataFrame. <code>pop</code> can also be used to remove a column and save this to  a Series. If the column contains Boolean values, then this can later be used to filter the DataFrame, even though the column no longer exists in  the DataFrame. This works because the Series has the same index as the  DataFrame:</p> <pre><code>is_retired = df.pop('Retired')\n\ndf['Retired'] # produces a key-error\n\nretired_df = df[is_retired].copy()\nnot_retired_df = df[~is_retired].copy()\n</code></pre> <p>This can be useful to filter a DataFrame, without storing the filter column in the DataFrame. The <code>copy</code> method is provided, to create new DataFrames that can be worked on independantly of the original. </p> <p>The filter Series does not have to contain Booleans, but can be used to create a Boolean result:</p> <pre><code>where_died = df.pop('Place of Death')\n\ndf[where_died == 'Norfolk']\n</code></pre> <p>To drop rows we need to provide a list of indices: <code>df.drop([1,2], inplace=True)</code>.</p> <p>Use <code>isna()</code> to find rows with 'NaN' vaules and <code>dropna()</code> to remove rows  with 'NaN' values in the selected columns:</p> <pre><code>df_clean = df.dropna(subset=['Rating'])\n</code></pre> <p>Use <code>inplace=True</code> if you don't want to create a new dataframe.</p> <p>Use <code>duplicated</code> to find duplicate rows and <code>drop_duplicates</code> to remove the duplicated rows (keeping the first occurrence):</p> <pre><code>df_clean = df.drop_duplicates(subset = ['App', 'Type', 'Price'])\n</code></pre> <p>To convert strings to numeric data, you can perform string formatting and then use the pandas <code>to_numeric</code> function: </p> <pre><code># remove commas\ndf_clean['Installs'] = df_clean['Installs'].astype(str).str.replace(',',\"\")\n# remove currency symbol\ndf_clean['Installs'] = df_clean['Installs'].astype(str).str.replace('\u00a3',\"\")\n# convert to numeric\ndf_clean['Installs'] = pd.to_numeric(df_clean['Installs'])\n</code></pre> <p>You can use filtering to drop unwanted values:</p> <pre><code>df_clean = df_clean[df_clean.Installs &gt; 300]\n</code></pre>"},{"location":"python/pandas/pandas/#aggregate-functions","title":"Aggregate Functions","text":"<p>Pandas provides access to aggregate methods that can be applied to one or more columns:</p> <pre><code>print(df[[\"Date of Death\", \"Date of Publication\"]].max())\nprint(df[[\"Date of Death\", \"Date of Publication\"]].min())\n</code></pre> <p>The <code>agg()</code> method allows you to apply multiple aggregates to a DataFrame:</p> <pre><code>df_aggregations = df.agg(\n    {\n        \"Date of Death\": [\"min\", \"max\", \"count\", ],\n        \"Date of Birth\": [\"count\"],\n        \"Date of Publication\": [\"max\"]\n    }\n)\n</code></pre> <p>Use <code>group_by()</code> to group aggregates:</p> <pre><code>religions = df[[\"Nationality\", \"Religion\"]].groupby(\"Religion\").count()\n</code></pre> <p>Data can be sorted using the <code>sort_values()</code> or <code>sort_index()</code> methods:</p> <pre><code>print(df.sort_values(by=[\"Date of Birth\", \"Place of Birth\"], ascending=True)[[\"Date of Birth\", \"Place of Birth\"]])\n</code></pre> <p>The <code>pivot()</code> method can be used to reshape data, specifying columns and values. </p> <pre><code>df.pivot(index='DATE', columns='LANGUAGE', values='POSTS')\n</code></pre> <p>The <code>pivot_table()</code> method supports aggregation. The reverse of <code>pivot()</code> is <code>melt()</code>.</p> <p>The <code>merge</code> method works similar to an SQL JOIN statement, producing a DataFrame  that results from combining DataFrames using a common Series:</p> <pre><code>patient_result_data = pd.merge([\"patient\", \"pathology\"], how=\"left\", on=\"patient_id\")\n</code></pre> <p>If the two tables to merge have the same data in different column names, use the  <code>left_on</code> and <code>right_on</code> parameters:</p> <pre><code>patient_result_data = pd.merge([\"patient\", \"pathology\"], \n    how=\"left\", left_on=\"patient_id\", right_on=\"hospital_number\")\n</code></pre>"},{"location":"python/pandas/pandas/#working-with-datetime-values","title":"Working with Datetime Values","text":"<p>Use the <code>to_datetime()</code> method to work with datetime data in your columns.: Use the <code>dayfirst</code> parameter for dates in \"%d/%m/%Y\" format. </p> <pre><code>df = pd.read_csv(\"ukgovunclaimedestates.csv\", parse_dates=True)\n\ndf[\"Date of Birth\"] = pd.to_datetime(df[\"Date of Birth\"], dayfirst=True)\ndf[\"Date of Death\"] = pd.to_datetime(df[\"Date of Death\"], dayfirst=True)\n\nprint(df.groupby(df[\"Date of Birth\"].dt.year)[\"Date of Birth\"].count())\n\ndf[\"Age\"] = df[\"Date of Death\"].dt.year - df[\"Date of Birth\"].dt.year\n</code></pre> <p>Converting columns to DateTime objects, provides access to utility methods to extract 'year', 'month', 'day', 'weekday' and perform calculations via  the <code>dt()</code> methods. If you set the index (using the <code>set_index()</code> method) to a DateTime object, then you can use the same methods on the index. </p> <pre><code>df[\"date_of_birth\"] = pd.to_datetime(df[\"Date of Birth\"], dayfirst=True)\ndf.set_index(\"date_of_birth\", inplace=True)\n\nprint(df.index.year)\n</code></pre>"},{"location":"python/pandas/pandas/#working-with-string-values","title":"Working with String Values","text":"<p>Using the <code>str()</code> accessor on text data, allows us to access various  string methods, including <code>lower()</code>, <code>split()</code>, <code>replace()</code></p> <p><code>split()</code> returns a number of elements from a single element: use the the <code>get()</code> method to select which element you want:</p> <pre><code>df[\"Surname\"] = df[\"Name\"].str.split(\", \").str.get(0)\ndf[\"Forename\"] = df[\"Name\"].str.split(\", \").str.get(1)\n</code></pre> <p><code>replace</code> uses a dictionary to replace values:</p> <pre><code>df[\"Gender\"] = df[\"Sex\"].replace({\"male\": \"m\", \"female\": \"f\"})\n</code></pre>"},{"location":"python/pandas/pandas/#matplotlib","title":"Matplotlib","text":"<p>To create a graph of a DataFrame use the <code>plot()</code> method. Pandas creates a line plot by default for each of the Series in a DataFrame  that has numeric values. All the plots created by Pandas are <code>Matplotlib</code> objects. The <code>matplotlib.pyplot</code> library provides the <code>show()</code> method  to display the graph: </p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"ukgovunclaimedestates.csv\", parse_dates=True)\n\nreligions = df[\"Religion\"].groupby(df[\"Religion\"]).count()\n\nreligions.plot()\n\nplt.show()\n</code></pre> <p>Use selection criteria on your DataFrame to choose which Series to plot. In JupyterLab, you can specify the x and y axes for your plot:</p> <pre><code>plt.plot(df.index, df['java'])\n</code></pre> <p>You can use slicing to drop elements from the plot:</p> <pre><code>plt.plot(df.index[:-1], df['java'][:-1])\n</code></pre> <p>You can also plot mulitple columns on the same graph, and add some graph formating:</p> <pre><code>plt.figure(figsize=(16,10))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('No of Posts', fontsize=14)\nplt.ylim(0,35000)\n\nplt.plot(df.index, df['java'])\nplt.plot(df.index, df['python'])\n</code></pre> <p>To plot multiple columns use a <code>for</code> loop:</p> <pre><code>plt.figure(figsize=(16,10))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('No of Posts', fontsize=14)\nplt.ylim(0,35000)\n\nfor column in df.columns:\n    plt.plot(df.index, df[column], linewidth=3, label=df[column].name)\n\nplt.legend(fontsize=14)\n</code></pre> <p>For time series data you can specify a rolling mean to smooth out the data: instead of plotting the value of each data point, you can specify a window  to calculate an average value for each data point based on the values either side of the data point:</p> <pre><code>rolling_df = df.rolling(window=6).mean()\n</code></pre> <p>Plotting two columns with varying value-ranges can look untidy and make trend-spotting difficult. Instead you can specify two different y-axes for each plot to make comparison easier:</p> <pre><code>ax1 = plt.gca() # gets the current axis\nax2 = ax1.twinx() # create another axis that shares the same x-axis\n\nax1.plot(sets_by_year.index[:-2], sets_by_year.set_num[:-2], color='g')\nax2.plot(themes_by_year.index[:-2], themes_by_year.nr_themes[:-2], color='b')\n\nax1.set_xlabel('Year')\nax1.set_ylabel('Number of Sets', color='g')\nax2.set_ylabel('Number of Themes', color='b')\n</code></pre> <p>If your xticks overlap, you can specify a rotation to make them readable:</p> <pre><code>plt.xticks(fontsize=14, rotation=45)\n</code></pre> <p>You can also use locator functions for marking the x- and y- axes:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ndf_unemployment = pd.read_csv('UE Benefits Search vs UE Rate 2004-19.csv')\ndf_unemployment['MONTH'] = pd.to_datetime(df_unemployment['MONTH'])\n\nroll_df = df_unemployment[['UE_BENEFITS_WEB_SEARCH', 'UNRATE']].rolling(window=6).mean()\nroll_df['month'] = df_unemployment['MONTH']\n\n\nplt.figure(figsize=(16,10))\nplt.title('Rolling Web Searches vs Unemployment Rate', fontsize=20)\nplt.xticks(fontsize=14, rotation=45)\nplt.yticks(fontsize=14)\n\nax1 = plt.gca()\nax2 = ax1.twinx()\nax1.grid(color='gray', linestyle='--')\n\nax1.set_ylabel('Web Searches', fontsize=16, color='indianred')\nax2.set_ylabel('Unemployment Rate', fontsize=16, color='cadetblue')\n\nax1.set_xlim(roll_df.month[5:].min(), roll_df.month[5:].max())\n\nyears = mdates.YearLocator()\nmonths = mdates.MonthLocator()\nyears_fmt = mdates.DateFormatter('%Y')\nax1.xaxis.set_major_locator(years)\nax1.xaxis.set_major_formatter(years_fmt)\nax1.xaxis.set_minor_locator(months)\n\nax1.plot(roll_df.month[5:], roll_df.UE_BENEFITS_WEB_SEARCH[5:], color='indianred', linewidth=3, marker='o')\nax2.plot(roll_df.month[5:], df_unemployment.UNRATE[5:], color='cadetblue', linewidth=3)\n</code></pre> <p>Pandas provides a number of graph types including line, area, bar, pie and scatter:</p> <ul> <li><code>plt.plot</code> for a line chart</li> <li><code>plt.scatter</code> for a scatter plot</li> <li><code>plt.bar</code> for a bar chart</li> </ul>"},{"location":"python/pandas/pandas/#plotly","title":"Plotly","text":"<p>Bar charts are a good way to visualise 'categorical' data. You can use  <code>value_counts()</code> to quickly create categorical data:</p> <pre><code>ratings = df.value_counts('Content_Rating')\n</code></pre> <p>Given a dataframe that contains the following data:</p> <pre><code>Content_Rating\nEveryone           6621\nTeen                912\nMature 17+          357\nEveryone 10+        305\nAdults only 18+       3\nUnrated               1\nName: count, dtype: int64\n</code></pre> <p>we can present this as a pie chart using plotly:</p> <pre><code>fig = px.pie(\n    labels=ratings.index,\n    values=ratings.values,\n    names=ratings.index,\n    title=\"Content Rating\"\n)\nfig.show()\n</code></pre> <p>We can change the location of the text on the pie chart using <code>update_traces()</code>:</p> <pre><code>fig = px.pie(\n    labels=ratings.index,\n    values=ratings.values,\n    title='Content Rating',\n    names=ratings.index,\n)\nfig.update_traces(\n    textposition='outside', \n    textinfo='percent+label'\n)\nfig.show()\n</code></pre> <p>To format the pie chart as a 'doughnut chart' add the 'hole' parameter:</p> <pre><code>fig = px.pie(\n    labels=ratings.index,\n    values=ratings.values,\n    title='Content Rating',\n    names=ratings.index,\n    hole=0.4\n)\nfig.update_traces(\n    textposition='inside',\n    textfont_size=15,\n    textinfo='percent'\n)\nfig.show()\n</code></pre> <p>Which produces:</p> <p></p>"},{"location":"python/pandas/pandas/#row-comprehensions","title":"Row Comprehensions","text":"<p>The <code>to_dict()</code> method of a DataFrame will produce a dictionary mapping the column names to  an array of the column values. If you want a dictionary mapping row values to other row values you can use the <code>iterrows()</code> method. </p> <p>The <code>iterrows()</code> method returns a list of tuples containing the index and row values for  each row. This can be used in a comprehension to create a new dictionary:</p> <pre><code>df = pd.read_csv(\"ukgovunclaimedestates.csv\", parse_dates=True)\n\nperson_dict = {row[\"BV Reference\"]:row[\"Surname\"] for (index, row) in df.iterrows()}\n</code></pre>"},{"location":"python/pandas/statistics_101/","title":"Statistics","text":""},{"location":"python/pandas/statistics_101/#z-score","title":"Z-Score","text":"<p>A z-score is a statistical measure that quantifies how many standard deviations a data point is from the mean. The formula for calculating a z-score is: </p> <pre><code>z = (x - \u03bc) / \u03c3\n</code></pre> <p>Or z equals x - mean / standard deviation. </p> <p>To add a z-score to a dataframe try: </p> <pre><code>score_df = df.assign(\n    z_score=lambda x: x.Age.sub(x.Age.mean()).div(x.Age.std()),\n    z_score_absolute=lambda x: x.z_score.abs(),\n)\n</code></pre>"},{"location":"python/pandas/statistics_101/#rate-of-change","title":"Rate of Change","text":"<p>In a DataFrame with a Datetime Index, you can calculate the change in a column  from one day to the next using <code>diff</code> or <code>pct_change</code> and also use <code>rank</code> to  rank the size of the change: </p> <pre><code>change_df = df.assign(\n    actual_change=lambda x: x.docker_volume_usage_percent.diff().fillna(0).abs(),\n    percent_change=lambda x: x.docker_volume_usage_percent.pct_change().fillna(0).abs(),\n    ranked_change=lambda x: x.actual_change.rank()\n)\n\nchange_df.sort_values('ranked_change', ascending=False).head()\n</code></pre>"},{"location":"python/pandas/statistics_101/#binning-data","title":"Binning Data","text":"<p>When dealing with discrete measurements, it can be useful to categorise the data. For instance, if you have a series of disk usage measurements,  ranging from 73% to 99% you could categorise the measurements into 3 bins  spread equally between the min and max values and label them  'normal', 'high', 'critical': </p> <pre><code>\ndf['volume_usage_level'] = pd.cut(\n    df.docker_volume_usage_percent, \n    bins=3, labels=['normal', 'high', 'critical']\n)\n</code></pre> <p>Now you can search the DataFrame by <code>volume_usage_level</code> or run <code>df.volume_usage_level.value_counts()</code> to see how often the volume is at 'normal', 'high' or 'critical' levels.</p> <p><code>qcut</code> can categorise your data into bins containing  equal number of measurements.</p>"}]}